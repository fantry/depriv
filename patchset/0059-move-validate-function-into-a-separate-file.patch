From 4e2af31bd6851e1322d2a43ef653d5d0b4f1d488 Mon Sep 17 00:00:00 2001
From: Xin Li <lxin@vmware.com>
Date: Thu, 18 Jun 2020 18:01:32 -0700
Subject: [PATCH 059/140] move validate function into a separate file

---
 arch/x86/kvm/Makefile               |   2 +-
 arch/x86/kvm/vmx/depriv.c           | 478 +---------------------------
 arch/x86/kvm/vmx/depriv_validator.c | 467 +++++++++++++++++++++++++++
 3 files changed, 484 insertions(+), 463 deletions(-)
 create mode 100644 arch/x86/kvm/vmx/depriv_validator.c

diff --git a/arch/x86/kvm/Makefile b/arch/x86/kvm/Makefile
index b922354d5639..01d6528e53ef 100644
--- a/arch/x86/kvm/Makefile
+++ b/arch/x86/kvm/Makefile
@@ -24,7 +24,7 @@ kvm-$(CONFIG_KVM_XEN)	+= xen.o
 kvm-intel-y		+= vmx/vmx.o vmx/vmenter.o vmx/pmu_intel.o vmx/vmcs12.o \
 			   vmx/evmcs.o vmx/nested.o vmx/posted_intr.o
 kvm-intel-$(CONFIG_X86_SGX_KVM)	+= vmx/sgx.o
-kvm-intel-$(CONFIG_KVM_INTEL_DEPRIV)	+= vmx/depriv.o vmx/depriv_entry.o vmx/depriv_handler.o
+kvm-intel-$(CONFIG_KVM_INTEL_DEPRIV)	+= vmx/depriv.o vmx/depriv_entry.o vmx/depriv_handler.o vmx/depriv_validator.o
 
 kvm-amd-y		+= svm/svm.o svm/vmenter.o svm/pmu.o svm/nested.o svm/avic.o svm/sev.o
 
diff --git a/arch/x86/kvm/vmx/depriv.c b/arch/x86/kvm/vmx/depriv.c
index 57e2cf164c32..1d34a1088baf 100644
--- a/arch/x86/kvm/vmx/depriv.c
+++ b/arch/x86/kvm/vmx/depriv.c
@@ -13,7 +13,6 @@
 #include <asm/debugreg.h>
 #include <asm/insn.h>
 
-#include "cpuid.h"
 #include "vmx.h"
 
 MODULE_AUTHOR("Xin Li");
@@ -28,8 +27,14 @@ module_param(test_handle_invalid_guest_state, bool, S_IRUGO);
 static bool __read_mostly call_extra_exit_handlers = 1;
 module_param(call_extra_exit_handlers, bool, S_IRUGO);
 
-static unsigned int __read_mostly depriv_print_mod = 10000;
-module_param(depriv_print_mod, uint, 0444);
+static unsigned int __read_mostly log_mod = 10000;
+module_param(log_mod, uint, 0444);
+
+static unsigned int __read_mostly exception_bitmap = 0;
+module_param(exception_bitmap, uint, 0444);
+
+static bool __read_mostly intercept_msr = 0;
+module_param(intercept_msr, bool, S_IRUGO);
 
 /*
  * host state memory buffer page order
@@ -65,459 +70,6 @@ static struct semaphore depriv_repriv_sema;
 static DEFINE_PER_CPU(struct vmcs *, depriv_vmcs);
 static DEFINE_PER_CPU(void *, depriv_cpu_state);
 
-#define check(_c) do {							\
-	if (!(_c))							\
-		pr_err("depriv: invalid guest state (%d): %s\n",	\
-		       __LINE__, #_c);					\
-} while (0)
-
-#define CHECK_VMX_CTLS(ctls, val)					\
-	check((~(ctls) & ((u32)(val))) == 0 &&				\
-	      ((ctls) & ~((u32)((val) >> 32))) == 0)
-
-#define CHECK_HOST_SEG(seg)						\
-	check((vmcs_read16(HOST_##seg##_SELECTOR) & 0x7) == 0)
-
-static inline bool is_canonical_address(u64 la, u64 cr4)
-{
-	return get_canonical(la, cr4 & X86_CR4_LA57 ? 57 : 48) == la;
-}
-
-#define CHECK_IS_ADDR_CANONICAL(a, gh)					\
-	check(is_canonical_address(a, vmcs_readl(gh##_CR4)))
-
-#define CHECK_IS_HOST_ADDR_CANONICAL(a)					\
-	CHECK_IS_ADDR_CANONICAL(a, HOST)
-
-#define CHECK_IS_GUEST_ADDR_CANONICAL(a)				\
-	CHECK_IS_ADDR_CANONICAL(a, GUEST)
-
-#define CHECK_IS_VMCS_FIELD_ADDR_CANONICAL(vmcs_field, gh)		\
-	CHECK_IS_ADDR_CANONICAL(vmcs_readl(vmcs_field), gh)
-
-#define CHECK_IS_HOST_VMCS_FIELD_ADDR_CANONICAL(addr)			\
-	CHECK_IS_VMCS_FIELD_ADDR_CANONICAL(HOST_##addr, HOST)
-
-#define CHECK_IS_GUEST_VMCS_FIELD_ADDR_CANONICAL(addr)			\
-	CHECK_IS_VMCS_FIELD_ADDR_CANONICAL(GUEST_##addr, GUEST)
-
-#define CHECK_IS_HOST_TABLE_BASE_CANONICAL(tab)				\
-	CHECK_IS_HOST_VMCS_FIELD_ADDR_CANONICAL(tab##_BASE)
-
-#define CHECK_IS_GUEST_TABLE_BASE_CANONICAL(tab)			\
-	CHECK_IS_GUEST_VMCS_FIELD_ADDR_CANONICAL(tab##_BASE)
-
-#define PAGE_M (PAGE_SIZE - 1)
-
-static u32 vmx_segment_sel(u8 seg)
-{
-	switch (seg) {
-	case VCPU_SREG_ES:
-		return GUEST_ES_SELECTOR;
-	case VCPU_SREG_CS:
-		return GUEST_CS_SELECTOR;
-	case VCPU_SREG_SS:
-		return GUEST_SS_SELECTOR;
-	case VCPU_SREG_DS:
-		return GUEST_DS_SELECTOR;
-	case VCPU_SREG_FS:
-		return GUEST_FS_SELECTOR;
-	case VCPU_SREG_GS:
-		return GUEST_GS_SELECTOR;
-	case VCPU_SREG_TR:
-		return GUEST_TR_SELECTOR;
-	case VCPU_SREG_LDTR:
-		return GUEST_LDTR_SELECTOR;
-	default:
-		return GUEST_ES_SELECTOR;
-	}
-}
-
-static void vmx_check_guest_segment(u8 seg, bool vm86_active,
-				    bool long_mode_active, bool unrestricted)
-{
-	u32 sel = vmx_segment_sel(seg);
-	u16 selector = vmcs_read16(sel);
-	unsigned long base = vmcs_readl(sel + GUEST_ES_BASE - GUEST_ES_SELECTOR);
-	u32 limit = vmcs_read32(sel + GUEST_ES_LIMIT - GUEST_ES_SELECTOR);
-	u32 ar = vmcs_read32(sel + GUEST_ES_AR_BYTES - GUEST_ES_SELECTOR);
-	u8 rpl = selector & SEGMENT_RPL_MASK;
-	u8 ti = selector & SEGMENT_TI_MASK;
-	u8 type = ar & VMX_AR_TYPE_MASK;
-	u8 dpl = VMX_AR_DPL(ar);
-
-	if (ar & VMX_AR_UNUSABLE_MASK)
-		return;
-
-	pr_debug("depriv: seg%d sel=0x%04x, attr=0x%05x, limit=0x%08x, base=0x%016lx\n",
-		 seg, selector, ar, limit, base);
-
-	check(ar & VMX_AR_P_MASK);
-
-	if (seg == VCPU_SREG_SS)
-		check(vm86_active || unrestricted ||
-		      rpl == (vmcs_read16(GUEST_CS_SELECTOR) & SEGMENT_RPL_MASK));
-	if (seg < VCPU_SREG_TR && vm86_active)
-		check(base == (unsigned long)(selector << 4));
-	if (seg == VCPU_SREG_TR || seg == VCPU_SREG_FS || seg == VCPU_SREG_GS)
-		CHECK_IS_GUEST_ADDR_CANONICAL(base);
-	else if (seg == VCPU_SREG_LDTR)
-		check(is_canonical_address(base, vmcs_readl(GUEST_CR4)));
-	else if (seg == VCPU_SREG_CS)
-		check(!((u32)(base >> 32)));
-	else
-		check(!((u32)(base >> 32)));
-
-	if (seg < VCPU_SREG_TR && vm86_active)
-		check(limit == 0xffff);
-
-	if (seg == VCPU_SREG_LDTR) {
-		check(ti == 0);
-		check(type == VMX_AR_TYPE_READABLE_MASK);
-		check(!(ar & VMX_AR_S_MASK));
-		check((ar & VMX_AR_RESERVD_MASK) == 0);
-		check((limit & PAGE_M) == PAGE_M || !(ar & VMX_AR_G_MASK));
-		check(limit >> 20 == 0 || ar & VMX_AR_G_MASK);
-		return;
-	}
-
-	if (seg == VCPU_SREG_TR) {
-		check(ti == 0);
-		check((!long_mode_active &&
-		       type == (VMX_AR_TYPE_ACCESSES_MASK | VMX_AR_TYPE_READABLE_MASK)) ||
-		      type == (VMX_AR_TYPE_ACCESSES_MASK | VMX_AR_TYPE_READABLE_MASK |
-			       VMX_AR_TYPE_CODE_MASK));
-		check(!(ar & VMX_AR_S_MASK));
-		check((ar & VMX_AR_RESERVD_MASK) == 0);
-		check((limit & PAGE_M) == PAGE_M || !(ar & VMX_AR_G_MASK));
-		check(limit >> 20 == 0 || ar & VMX_AR_G_MASK);
-		return;
-	}
-
-	if (vm86_active) {
-		check(ar == (VMX_AR_TYPE_ACCESSES_MASK |
-			     VMX_AR_TYPE_READABLE_MASK |
-			     VMX_AR_S_MASK |
-			     (3 << VMX_AR_DPL_SHIFT) |
-			     VMX_AR_P_MASK));
-		return;
-	}
-
-	// Type
-	if (seg == VCPU_SREG_CS)
-		check((unrestricted &&
-		       type == (VMX_AR_TYPE_ACCESSES_MASK | VMX_AR_TYPE_READABLE_MASK)) ||
-		      (type & VMX_AR_TYPE_ACCESSES_MASK && type & VMX_AR_TYPE_CODE_MASK));
-	else if (seg == VCPU_SREG_SS)
-		check(type == (VMX_AR_TYPE_ACCESSES_MASK | VMX_AR_TYPE_READABLE_MASK) ||
-		      type == (VMX_AR_TYPE_ACCESSES_MASK | VMX_AR_TYPE_READABLE_MASK |
-			       VMX_AR_TYPE_WRITEABLE_MASK));
-	else
-		check(type & VMX_AR_TYPE_ACCESSES_MASK &&
-		      (!(type & VMX_AR_TYPE_CODE_MASK) || type & VMX_AR_TYPE_READABLE_MASK));
-	// S
-	check(ar & VMX_AR_S_MASK);
-	// DPL
-	if (seg == VCPU_SREG_CS)
-		if (type == (VMX_AR_TYPE_ACCESSES_MASK | VMX_AR_TYPE_READABLE_MASK))
-			check(dpl == 0 && unrestricted);
-		else if (!(type & VMX_AR_TYPE_WRITEABLE_MASK))
-			check(dpl == VMX_AR_DPL(vmcs_read32(GUEST_SS_AR_BYTES)));
-		else
-			check(dpl <= VMX_AR_DPL(vmcs_read32(GUEST_SS_AR_BYTES)));
-	else if (seg == VCPU_SREG_SS)
-		if (!(vmcs_readl(GUEST_CR0) & X86_CR0_PE) ||
-		    (vmcs_read32(GUEST_CS_AR_BYTES) & VMX_AR_TYPE_MASK) ==
-		    (VMX_AR_TYPE_ACCESSES_MASK | VMX_AR_TYPE_READABLE_MASK))
-			check(dpl == 0);
-		else
-			check(dpl == rpl);
-	else if (!unrestricted && type <= VMX_AR_TYPE_BUSY_64_TSS)
-		check(dpl >= rpl);
-	// reserved bits
-	check((ar & VMX_AR_RESERVD_MASK) == 0);
-	// D/B
-	if (seg == VCPU_SREG_CS)
-		check(!long_mode_active || !(ar & VMX_AR_L_MASK) || !(ar & VMX_AR_DB_MASK));
-	// G
-	check((limit & PAGE_M) == PAGE_M || !(ar & VMX_AR_G_MASK));
-	check(limit >> 20 == 0 || ar & VMX_AR_G_MASK);
-}
-
-static void vmx_check_guest_state(void)
-{
-	u32 vmentry_ctl = vmcs_read32(VM_ENTRY_CONTROLS);
-	u32 vmexit_ctl = vmcs_read32(VM_EXIT_CONTROLS);
-	u32 cpu_based_exec_ctrl = vmcs_read32(CPU_BASED_VM_EXEC_CONTROL);
-	u32 pin_based_exec_ctrl = vmcs_read32(PIN_BASED_VM_EXEC_CONTROL);
-	u32 secondary_exec_control = cpu_has_secondary_exec_ctrls() ?
-		vmcs_read32(SECONDARY_VM_EXEC_CONTROL) : 0;
-	u64 rflags = vmcs_readl(GUEST_RFLAGS);
-	bool vm86_active = rflags & X86_EFLAGS_VM;
-	bool long_mode_active = vmentry_ctl & VM_ENTRY_IA32E_MODE;
-	u32 vmentry_intr_info = vmcs_read32(VM_ENTRY_INTR_INFO_FIELD);
-	bool unrestricted = cpu_has_secondary_exec_ctrls() ?
-		secondary_exec_control & SECONDARY_EXEC_UNRESTRICTED_GUEST :
-		false;
-
-	u64 cr0_must_be_zeros = ~read_msr(MSR_IA32_VMX_CR0_FIXED1);
-	u64 host_cr0_must_be_ones = read_msr(MSR_IA32_VMX_CR0_FIXED0);
-	u64 guest_cr0_must_be_ones = host_cr0_must_be_ones &
-		~(unrestricted ? X86_CR0_PG | X86_CR0_PE : 0);
-	u64 cr4_must_be_zeros = ~read_msr(MSR_IA32_VMX_CR4_FIXED1);
-	u64 cr4_must_be_ones = read_msr(MSR_IA32_VMX_CR4_FIXED0);
-
-	u64 pin_based_ctrls = read_msr(MSR_IA32_VMX_TRUE_PINBASED_CTLS);
-	u64 proc_based_ctrls = read_msr(MSR_IA32_VMX_TRUE_PROCBASED_CTLS);
-
-	u64 debug_ctrl = vmcs_read64(GUEST_IA32_DEBUGCTL);
-	u32 activity_state = vmcs_read32(GUEST_ACTIVITY_STATE);
-	u32 interruptibility = vmcs_read32(GUEST_INTERRUPTIBILITY_INFO);
-	u64 pending_dbg_exceptions = vmcs_readl(GUEST_PENDING_DBG_EXCEPTIONS);
-
-	u8 seg;
-
-	// 26.2.1.1: VM-Execution Control Fields
-	CHECK_VMX_CTLS(pin_based_exec_ctrl, pin_based_ctrls);
-	CHECK_VMX_CTLS(cpu_based_exec_ctrl, proc_based_ctrls);
-
-	if (cpu_based_exec_ctrl & CPU_BASED_ACTIVATE_SECONDARY_CONTROLS) {
-		u64 secondary_ctrls = read_msr(MSR_IA32_VMX_PROCBASED_CTLS2);
-		CHECK_VMX_CTLS(secondary_exec_control, secondary_ctrls);
-	}
-
-	check(vmcs_read32(CR3_TARGET_COUNT) == 0);
-
-	check(!(cpu_based_exec_ctrl & CPU_BASED_USE_IO_BITMAPS));
-	check(!(cpu_based_exec_ctrl & CPU_BASED_TPR_SHADOW));
-	check(pin_based_exec_ctrl & PIN_BASED_NMI_EXITING ||
-	      !(pin_based_exec_ctrl & PIN_BASED_VIRTUAL_NMIS));
-	check(pin_based_exec_ctrl & PIN_BASED_VIRTUAL_NMIS ||
-	      !(cpu_based_exec_ctrl & CPU_BASED_NMI_WINDOW_EXITING));
-	check(!(pin_based_exec_ctrl & PIN_BASED_POSTED_INTR));
-
-	// 26.2.1.2: VM-Exit Control Fields
-	CHECK_VMX_CTLS(vmexit_ctl, read_msr(MSR_IA32_VMX_TRUE_EXIT_CTLS));
-	check(pin_based_exec_ctrl & PIN_BASED_VMX_PREEMPTION_TIMER ||
-	      !(vmexit_ctl & VM_EXIT_SAVE_VMX_PREEMPTION_TIMER));
-
-	// 26.2.1.3: VM-Entry Control Fields
-	CHECK_VMX_CTLS(vmentry_ctl, read_msr(MSR_IA32_VMX_TRUE_ENTRY_CTLS));
-	if (vmentry_intr_info & INTR_INFO_VALID_MASK) {
-		u32 type = vmentry_intr_info & INTR_INFO_INTR_TYPE_MASK;
-		u8 vector = vmentry_intr_info & INTR_INFO_VECTOR_MASK;
-		u32 insn_len = vmcs_read32(VM_ENTRY_INSTRUCTION_LEN);
-		bool has_error_code = vmentry_intr_info & INTR_INFO_DELIVER_CODE_MASK;
-
-		check(type != INTR_TYPE_RESERVED);
-		check((u32)(proc_based_ctrls >> 32) & CPU_BASED_MONITOR_TRAP_FLAG ||
-		      type != INTR_TYPE_OTHER_EVENT);
-		check(type != INTR_TYPE_NMI_INTR || vector == BP_VECTOR);
-		check(type != INTR_TYPE_HARD_EXCEPTION || vector <= 31);
-		check(type != INTR_TYPE_OTHER_EVENT || vector == DE_VECTOR);
-
-		check(has_error_code == (vmcs_readl(GUEST_CR0) & X86_CR0_PE &&
-					 (type == INTR_TYPE_HARD_EXCEPTION &&
-					  (vector == DF_VECTOR ||
-					   vector == TS_VECTOR ||
-					   vector == NP_VECTOR ||
-					   vector == SS_VECTOR ||
-					   vector == GP_VECTOR ||
-					   vector == PF_VECTOR ||
-					   vector == AC_VECTOR))));
-
-		check(!(vmentry_intr_info &
-			(INTR_INFO_RESVD_BITS_MASK | INTR_INFO_UNBLOCK_NMI)));
-
-		check(!has_error_code ||
-		      (vmcs_read32(VM_ENTRY_EXCEPTION_ERROR_CODE) & 0xffff8000) == 0);
-		check(!(type == INTR_TYPE_SOFT_INTR ||
-			type == INTR_TYPE_PRIV_SW_EXCEPTION ||
-			type == INTR_TYPE_SOFT_EXCEPTION) ||
-		      insn_len < MAX_INSN_SIZE);
-	}
-
-	check(!(vmentry_ctl & VM_ENTRY_SMM));
-	check(!(vmentry_ctl & VM_ENTRY_DEACT_DUAL_MONITOR));
-
-	// 26.2.2: Checks on Host Control Registers and MSRs
-	check((~vmcs_readl(HOST_CR0) & cr0_must_be_zeros) == cr0_must_be_zeros);
-	check((vmcs_readl(HOST_CR0) & host_cr0_must_be_ones) == host_cr0_must_be_ones);
-
-	check((~vmcs_readl(HOST_CR4) & cr4_must_be_zeros) == cr4_must_be_zeros);
-	check((vmcs_readl(HOST_CR4) & cr4_must_be_ones) == cr4_must_be_ones);
-
-	CHECK_IS_HOST_VMCS_FIELD_ADDR_CANONICAL(IA32_SYSENTER_ESP);
-	CHECK_IS_HOST_VMCS_FIELD_ADDR_CANONICAL(IA32_SYSENTER_EIP);
-
-	if (vmexit_ctl & VM_EXIT_LOAD_IA32_PAT) {
-		u64 pat = vmcs_read64(HOST_IA32_PAT);
-		unsigned i;
-		for (i = 0; i < 8; i++) {
-			u8 byte = pat & 0xff;
-			check(byte < 8 && byte != 2 && byte != 3);
-			pat >>= 8;
-		}
-	}
-
-	if (vmexit_ctl & VM_EXIT_LOAD_IA32_EFER) {
-		u64 efer = vmcs_read64(HOST_IA32_EFER);
-		check((efer & 0xffffffffffff0200ull) == 0);
-		if (vmexit_ctl & VM_EXIT_HOST_ADDR_SPACE_SIZE)
-			check((efer & (EFER_LMA | EFER_LME)) ==
-			      (EFER_LMA | EFER_LME));
-		else
-			check((efer & (EFER_LMA | EFER_LME)) == 0);
-	}
-
-	// 26.2.3: Checks on Host Segment and Descriptor-Table Registers
-	CHECK_HOST_SEG(ES);
-	CHECK_HOST_SEG(CS);
-	CHECK_HOST_SEG(SS);
-	CHECK_HOST_SEG(DS);
-	CHECK_HOST_SEG(FS);
-	CHECK_HOST_SEG(GS);
-	CHECK_HOST_SEG(TR);
-
-	check(vmcs_read16(HOST_CS_SELECTOR) != 0);
-	check(vmcs_read16(HOST_TR_SELECTOR) != 0);
-
-	CHECK_IS_HOST_TABLE_BASE_CANONICAL(FS);
-	CHECK_IS_HOST_TABLE_BASE_CANONICAL(GS);
-	CHECK_IS_HOST_TABLE_BASE_CANONICAL(GDTR);
-	CHECK_IS_HOST_TABLE_BASE_CANONICAL(IDTR);
-	CHECK_IS_HOST_TABLE_BASE_CANONICAL(TR);
-
-	// 26.2.4: Checks Related to Address-Space Size
-	check(vmexit_ctl & VM_EXIT_HOST_ADDR_SPACE_SIZE);
-
-	check(vmcs_readl(HOST_CR4) & X86_CR4_PAE);
-	CHECK_IS_HOST_VMCS_FIELD_ADDR_CANONICAL(RIP);
-
-	// 26.3.1.1: Checks on Guest Control Registers, Debug Registers, and MSRs
-	check((~vmcs_readl(GUEST_CR0) & cr0_must_be_zeros) ==
-	      cr0_must_be_zeros);
-	check((vmcs_readl(GUEST_CR0) & guest_cr0_must_be_ones) ==
-	      guest_cr0_must_be_ones);
-
-	check((~vmcs_readl(GUEST_CR4) & cr4_must_be_zeros) == cr4_must_be_zeros);
-	check((vmcs_readl(GUEST_CR4) & cr4_must_be_ones) == cr4_must_be_ones);
-
-	if (vmentry_ctl & VM_ENTRY_LOAD_DEBUG_CONTROLS) {
-		u64 debug_ctrl_reserved = 0xffffffffffff003cull;
-		check((debug_ctrl & debug_ctrl_reserved) == 0);
-	}
-
-	check(!long_mode_active || vmcs_readl(GUEST_CR4) & X86_CR4_PAE);
-
-	if (vmentry_ctl & VM_ENTRY_LOAD_DEBUG_CONTROLS)
-		check(!((u32)(vmcs_readl(GUEST_DR7) >> 32)));
-
-	CHECK_IS_GUEST_VMCS_FIELD_ADDR_CANONICAL(SYSENTER_ESP);
-	CHECK_IS_GUEST_VMCS_FIELD_ADDR_CANONICAL(SYSENTER_EIP);
-
-	if (vmentry_ctl & VM_ENTRY_LOAD_IA32_PAT) {
-		u64 pat = vmcs_read64(GUEST_IA32_PAT);
-		unsigned i;
-		for (i = 0; i < 8; i++) {
-			u8 byte = pat & 0xff;
-			check(byte < 8 && byte != 2 && byte != 3);
-			pat >>= 8;
-		}
-	}
-
-	if (vmentry_ctl & VM_ENTRY_LOAD_IA32_EFER) {
-		u64 efer = vmcs_read64(GUEST_IA32_EFER);
-		check((efer & 0xffffffffffff0200ull) == 0);
-		if ((vmcs_readl(GUEST_CR0) & X86_CR0_PG) != 0) {
-			if (long_mode_active)
-				check((efer & (EFER_LMA | EFER_LME)) ==
-				      (EFER_LMA | EFER_LME));
-			else
-				check((efer & (EFER_LMA | EFER_LME)) == 0);
-		}
-	}
-
-	if (vmentry_ctl & VM_ENTRY_LOAD_BNDCFGS) {
-		u64 bndcfgs = vmcs_read64(GUEST_BNDCFGS);
-		CHECK_IS_GUEST_ADDR_CANONICAL(bndcfgs & 0xfffffffffffff000ull);
-		check((bndcfgs & 0x00000ffc) == 0);
-	}
-
-	// 26.3.1.2: Checks on Guest Segment Registers
-	for (seg = VCPU_SREG_ES; seg <= VCPU_SREG_LDTR; seg++)
-		vmx_check_guest_segment(seg, vm86_active, long_mode_active, unrestricted);
-
-	// 26.3.1.3: Checks on Guest Descriptor-Table Registers
-	CHECK_IS_GUEST_TABLE_BASE_CANONICAL(GDTR);
-	CHECK_IS_GUEST_TABLE_BASE_CANONICAL(IDTR);
-
-	// 26.3.1.4: Checks on Guest RIP and RFLAGS
-	check((long_mode_active && vmcs_read32(GUEST_CS_AR_BYTES) & VMX_AR_L_MASK) ||
-	      (u32)(vmcs_readl(GUEST_RIP) >> 32) == 0);
-	CHECK_IS_GUEST_VMCS_FIELD_ADDR_CANONICAL(RIP);
-
-	check((rflags & ((-1ull << 22) | (1 << 15) | (1 << 5) | (1 << 3))) == 0);
-	check((rflags & X86_EFLAGS_FIXED) != 0);
-	check(!long_mode_active || !vm86_active);
-	check(((vmentry_intr_info & (INTR_INFO_VALID_MASK |
-				     INTR_INFO_INTR_TYPE_MASK)) !=
-	       (INTR_INFO_VALID_MASK | INTR_TYPE_EXT_INTR)) ||
-	      (rflags & X86_EFLAGS_IF));
-
-	// 26.3.1.5: Checks on Guest Non-Register State
-	check(activity_state <= GUEST_ACTIVITY_WAIT_SIPI);
-	check(activity_state != GUEST_ACTIVITY_HLT ||
-	      VMX_AR_DPL(vmcs_read32(GUEST_SS_AR_BYTES)) == 0);
-	check(!(interruptibility & (GUEST_INTR_STATE_STI | GUEST_INTR_STATE_MOV_SS)) ||
-	      activity_state == GUEST_ACTIVITY_ACTIVE);
-	if (vmentry_intr_info & INTR_INFO_VALID_MASK) {
-		u32 type = vmentry_intr_info & INTR_INFO_INTR_TYPE_MASK;
-		u8 vector = vmentry_intr_info & INTR_INFO_VECTOR_MASK;
-
-		check(activity_state != GUEST_ACTIVITY_HLT ||
-		      (type == INTR_TYPE_EXT_INTR ||
-		       type == INTR_TYPE_NMI_INTR ||
-		       (type == INTR_TYPE_HARD_EXCEPTION &&
-			(vector == DB_VECTOR || vector == MC_VECTOR)) ||
-		       (type == INTR_TYPE_OTHER_EVENT && vector == DB_VECTOR)));
-		check(activity_state != GUEST_ACTIVITY_SHUTDOWN ||
-		      (type == INTR_TYPE_NMI_INTR ||
-		       (type == INTR_TYPE_HARD_EXCEPTION && vector == MC_VECTOR)));
-		check(activity_state != GUEST_ACTIVITY_WAIT_SIPI);
-	}
-
-	check((interruptibility & 0xFFFFFFE0) == 0);
-	check((interruptibility & (GUEST_INTR_STATE_STI | GUEST_INTR_STATE_MOV_SS)) !=
-	      (GUEST_INTR_STATE_STI | GUEST_INTR_STATE_MOV_SS));
-	check(rflags & X86_EFLAGS_IF || !(interruptibility & GUEST_INTR_STATE_STI));
-	check(((vmentry_intr_info & (INTR_INFO_VALID_MASK | INTR_INFO_INTR_TYPE_MASK)) !=
-	       (INTR_INFO_VALID_MASK | INTR_TYPE_EXT_INTR)) ||
-	      (interruptibility & (GUEST_INTR_STATE_STI | GUEST_INTR_STATE_MOV_SS)) == 0);
-	check(((vmentry_intr_info & (INTR_INFO_VALID_MASK | INTR_INFO_INTR_TYPE_MASK)) !=
-	       (INTR_INFO_VALID_MASK | INTR_TYPE_NMI_INTR)) ||
-	      (interruptibility & GUEST_INTR_STATE_MOV_SS) == 0);
-	check((interruptibility & GUEST_INTR_STATE_SMI) == 0);
-	// Some processors require the following check; others do not.
-	check(((vmentry_intr_info & (INTR_INFO_VALID_MASK | INTR_INFO_INTR_TYPE_MASK)) !=
-	       (INTR_INFO_VALID_MASK | INTR_TYPE_NMI_INTR)) ||
-	      (interruptibility & GUEST_INTR_STATE_STI) == 0);
-	check(((vmentry_intr_info & (INTR_INFO_VALID_MASK | INTR_INFO_INTR_TYPE_MASK)) !=
-	       (INTR_INFO_VALID_MASK | INTR_TYPE_NMI_INTR)) ||
-	      (pin_based_exec_ctrl & PIN_BASED_VIRTUAL_NMIS) == 0 ||
-	      (interruptibility & GUEST_INTR_STATE_NMI) == 0);
-
-	check((u32)(pending_dbg_exceptions >> 32) == 0);
-	check((pending_dbg_exceptions & 0xfffeaff0) == 0);
-	if ((interruptibility & (GUEST_INTR_STATE_STI | GUEST_INTR_STATE_MOV_SS)) != 0 ||
-	    activity_state == GUEST_ACTIVITY_HLT) {
-		check(!((rflags & X86_EFLAGS_TF) && !(debug_ctrl & 0x2)) ||
-		      (pending_dbg_exceptions & 0x00004000));
-		check(!(!(rflags & X86_EFLAGS_TF) || (debug_ctrl & 0x2)) ||
-		      !(pending_dbg_exceptions & 0x00004000));
-	}
-}
-
 static __init int adjust_vmx_controls(u32 ctl_min, u32 ctl_opt,
 				      u32 msr, u32 *result)
 {
@@ -647,7 +199,7 @@ static void vmx_depriv_cpu_controls(void)
 	vmcs_write32(VM_ENTRY_CONTROLS,
 		     depriv_vmcs_conf.vmentry_ctrl);
 
-	vmcs_write32(EXCEPTION_BITMAP, 0);
+	vmcs_write32(EXCEPTION_BITMAP, exception_bitmap);
 	vmcs_write32(PAGE_FAULT_ERROR_CODE_MASK, 0);
 	vmcs_write32(PAGE_FAULT_ERROR_CODE_MATCH, 0);
 	vmcs_write32(CR3_TARGET_COUNT, 0);
@@ -986,6 +538,8 @@ void vmx_depriv_vmcall(void);
 	DEPRIV_IRET_STACK_FS_BASE	= vmcs_readl(GUEST_FS_BASE);		\
 } while (0)
 
+extern void vmx_validate_guest_state(void);
+
 static void vmx_depriv_cpu(void *info)
 {
 	int cpu = smp_processor_id();
@@ -1033,7 +587,7 @@ static void vmx_depriv_cpu(void *info)
 
 	// the 2nd page of host state
 	msr_bitmap = host_cpu_state + DEPRIV_CPU_STATE_VMCS_MSR_BITMAP;
-	memset(msr_bitmap, 0, PAGE_SIZE);
+	memset(msr_bitmap, intercept_msr ? 1 : 0, PAGE_SIZE);
 	vmcs_write64(MSR_BITMAP, __pa(msr_bitmap));
 
 	vmx_depriv_cpu_state();
@@ -1068,7 +622,7 @@ static void vmx_depriv_cpu(void *info)
 	if (test_handle_invalid_guest_state)
 		vmcs_write32(GUEST_TR_AR_BYTES, 0x009b);
 
-	vmx_check_guest_state();
+	vmx_validate_guest_state();
 
 	/*
 	 * Should we save/restore general purpose registers around vmx_depriv?
@@ -1308,17 +862,17 @@ bool vmx_depriv_vmexit_handler(unsigned long *regs)
 		if (rip == (unsigned long)vmx_depriv_rip)
 			regs[VCPU_REGS_RAX] = 2;
 
-		vmx_check_guest_state();
+		vmx_validate_guest_state();
 		DEPRIV_CONTINUE_IN_ROOT_MODE(0);
 	}
 
-	if (!(*cnt % depriv_print_mod))
+	if (!(*cnt % log_mod))
 		pr_info("depriv: cpu%d (%ld) exit reason: %d rip: %#lx rsp: %#lx\n",
 			cpu, *cnt, reason, rip, rsp);
 
 	switch (reason) {
 	case EXIT_REASON_CPUID: {
-		if (!(*cnt % depriv_print_mod))
+		if (!(*cnt % log_mod))
 			pr_info("depriv: cpu%d (%ld) cpuid[%#x]\n", cpu, *cnt, (u32)regs[VCPU_REGS_RAX]);
 
 		native_cpuid((unsigned int *)&regs[VCPU_REGS_RAX],
diff --git a/arch/x86/kvm/vmx/depriv_validator.c b/arch/x86/kvm/vmx/depriv_validator.c
new file mode 100644
index 000000000000..e0d176f22195
--- /dev/null
+++ b/arch/x86/kvm/vmx/depriv_validator.c
@@ -0,0 +1,467 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Deprivilege is to run Linux kernel in VMX non-root mode
+ *
+ * Authors:
+ * 	Xin Li <fantry@gmail.com>
+ */
+
+#include <linux/kvm_host.h>
+
+#include <asm/insn.h>
+
+#include "cpuid.h"
+#include "vmx.h"
+
+#define check(_c) do {							\
+	if (!(_c))							\
+		pr_err("depriv: invalid guest state (%d): %s\n",	\
+		       __LINE__, #_c);					\
+} while (0)
+
+#define CHECK_VMX_CTLS(ctls, val)					\
+	check((~(ctls) & ((u32)(val))) == 0 &&				\
+	      ((ctls) & ~((u32)((val) >> 32))) == 0)
+
+#define CHECK_HOST_SEG(seg)						\
+	check((vmcs_read16(HOST_##seg##_SELECTOR) & 0x7) == 0)
+
+static inline bool is_canonical_address(u64 la, u64 cr4)
+{
+	return get_canonical(la, cr4 & X86_CR4_LA57 ? 57 : 48) == la;
+}
+
+#define CHECK_IS_ADDR_CANONICAL(a, gh)					\
+	check(is_canonical_address(a, vmcs_readl(gh##_CR4)))
+
+#define CHECK_IS_HOST_ADDR_CANONICAL(a)					\
+	CHECK_IS_ADDR_CANONICAL(a, HOST)
+
+#define CHECK_IS_GUEST_ADDR_CANONICAL(a)				\
+	CHECK_IS_ADDR_CANONICAL(a, GUEST)
+
+#define CHECK_IS_VMCS_FIELD_ADDR_CANONICAL(vmcs_field, gh)		\
+	CHECK_IS_ADDR_CANONICAL(vmcs_readl(vmcs_field), gh)
+
+#define CHECK_IS_HOST_VMCS_FIELD_ADDR_CANONICAL(addr)			\
+	CHECK_IS_VMCS_FIELD_ADDR_CANONICAL(HOST_##addr, HOST)
+
+#define CHECK_IS_GUEST_VMCS_FIELD_ADDR_CANONICAL(addr)			\
+	CHECK_IS_VMCS_FIELD_ADDR_CANONICAL(GUEST_##addr, GUEST)
+
+#define CHECK_IS_HOST_TABLE_BASE_CANONICAL(tab)				\
+	CHECK_IS_HOST_VMCS_FIELD_ADDR_CANONICAL(tab##_BASE)
+
+#define CHECK_IS_GUEST_TABLE_BASE_CANONICAL(tab)			\
+	CHECK_IS_GUEST_VMCS_FIELD_ADDR_CANONICAL(tab##_BASE)
+
+#define PAGE_M (PAGE_SIZE - 1)
+
+static u32 vmx_segment_sel(u8 seg)
+{
+	switch (seg) {
+	case VCPU_SREG_ES:
+		return GUEST_ES_SELECTOR;
+	case VCPU_SREG_CS:
+		return GUEST_CS_SELECTOR;
+	case VCPU_SREG_SS:
+		return GUEST_SS_SELECTOR;
+	case VCPU_SREG_DS:
+		return GUEST_DS_SELECTOR;
+	case VCPU_SREG_FS:
+		return GUEST_FS_SELECTOR;
+	case VCPU_SREG_GS:
+		return GUEST_GS_SELECTOR;
+	case VCPU_SREG_TR:
+		return GUEST_TR_SELECTOR;
+	case VCPU_SREG_LDTR:
+		return GUEST_LDTR_SELECTOR;
+	default:
+		return GUEST_ES_SELECTOR;
+	}
+}
+
+static void vmx_validate_guest_selector(u8 seg, bool vm86_active,
+					bool long_mode_active, bool unrestricted)
+{
+	u32 sel = vmx_segment_sel(seg);
+	u16 selector = vmcs_read16(sel);
+	unsigned long base = vmcs_readl(sel + GUEST_ES_BASE - GUEST_ES_SELECTOR);
+	u32 limit = vmcs_read32(sel + GUEST_ES_LIMIT - GUEST_ES_SELECTOR);
+	u32 ar = vmcs_read32(sel + GUEST_ES_AR_BYTES - GUEST_ES_SELECTOR);
+	u8 rpl = selector & SEGMENT_RPL_MASK;
+	u8 ti = selector & SEGMENT_TI_MASK;
+	u8 type = ar & VMX_AR_TYPE_MASK;
+	u8 dpl = VMX_AR_DPL(ar);
+
+	if (ar & VMX_AR_UNUSABLE_MASK)
+		return;
+
+	pr_debug("depriv: seg%d sel=0x%04x, attr=0x%05x, limit=0x%08x, base=0x%016lx\n",
+		 seg, selector, ar, limit, base);
+
+	check(ar & VMX_AR_P_MASK);
+
+	if (seg == VCPU_SREG_SS)
+		check(vm86_active || unrestricted ||
+		      rpl == (vmcs_read16(GUEST_CS_SELECTOR) & SEGMENT_RPL_MASK));
+	if (seg < VCPU_SREG_TR && vm86_active)
+		check(base == (unsigned long)(selector << 4));
+	if (seg == VCPU_SREG_TR || seg == VCPU_SREG_FS || seg == VCPU_SREG_GS)
+		CHECK_IS_GUEST_ADDR_CANONICAL(base);
+	else if (seg == VCPU_SREG_LDTR)
+		check(is_canonical_address(base, vmcs_readl(GUEST_CR4)));
+	else if (seg == VCPU_SREG_CS)
+		check(!((u32)(base >> 32)));
+	else
+		check(!((u32)(base >> 32)));
+
+	if (seg < VCPU_SREG_TR && vm86_active)
+		check(limit == 0xffff);
+
+	if (seg == VCPU_SREG_LDTR) {
+		check(ti == 0);
+		check(type == VMX_AR_TYPE_READABLE_MASK);
+		check(!(ar & VMX_AR_S_MASK));
+		check((ar & VMX_AR_RESERVD_MASK) == 0);
+		check((limit & PAGE_M) == PAGE_M || !(ar & VMX_AR_G_MASK));
+		check(limit >> 20 == 0 || ar & VMX_AR_G_MASK);
+		return;
+	}
+
+	if (seg == VCPU_SREG_TR) {
+		check(ti == 0);
+		check((!long_mode_active &&
+		       type == (VMX_AR_TYPE_ACCESSES_MASK | VMX_AR_TYPE_READABLE_MASK)) ||
+		      type == (VMX_AR_TYPE_ACCESSES_MASK | VMX_AR_TYPE_READABLE_MASK |
+			       VMX_AR_TYPE_CODE_MASK));
+		check(!(ar & VMX_AR_S_MASK));
+		check((ar & VMX_AR_RESERVD_MASK) == 0);
+		check((limit & PAGE_M) == PAGE_M || !(ar & VMX_AR_G_MASK));
+		check(limit >> 20 == 0 || ar & VMX_AR_G_MASK);
+		return;
+	}
+
+	if (vm86_active) {
+		check(ar == (VMX_AR_TYPE_ACCESSES_MASK |
+			     VMX_AR_TYPE_READABLE_MASK |
+			     VMX_AR_S_MASK |
+			     (3 << VMX_AR_DPL_SHIFT) |
+			     VMX_AR_P_MASK));
+		return;
+	}
+
+	// Type
+	if (seg == VCPU_SREG_CS)
+		check((unrestricted &&
+		       type == (VMX_AR_TYPE_ACCESSES_MASK | VMX_AR_TYPE_READABLE_MASK)) ||
+		      (type & VMX_AR_TYPE_ACCESSES_MASK && type & VMX_AR_TYPE_CODE_MASK));
+	else if (seg == VCPU_SREG_SS)
+		check(type == (VMX_AR_TYPE_ACCESSES_MASK | VMX_AR_TYPE_READABLE_MASK) ||
+		      type == (VMX_AR_TYPE_ACCESSES_MASK | VMX_AR_TYPE_READABLE_MASK |
+			       VMX_AR_TYPE_WRITEABLE_MASK));
+	else
+		check(type & VMX_AR_TYPE_ACCESSES_MASK &&
+		      (!(type & VMX_AR_TYPE_CODE_MASK) || type & VMX_AR_TYPE_READABLE_MASK));
+	// S
+	check(ar & VMX_AR_S_MASK);
+	// DPL
+	if (seg == VCPU_SREG_CS)
+		if (type == (VMX_AR_TYPE_ACCESSES_MASK | VMX_AR_TYPE_READABLE_MASK))
+			check(dpl == 0 && unrestricted);
+		else if (!(type & VMX_AR_TYPE_WRITEABLE_MASK))
+			check(dpl == VMX_AR_DPL(vmcs_read32(GUEST_SS_AR_BYTES)));
+		else
+			check(dpl <= VMX_AR_DPL(vmcs_read32(GUEST_SS_AR_BYTES)));
+	else if (seg == VCPU_SREG_SS)
+		if (!(vmcs_readl(GUEST_CR0) & X86_CR0_PE) ||
+		    (vmcs_read32(GUEST_CS_AR_BYTES) & VMX_AR_TYPE_MASK) ==
+		    (VMX_AR_TYPE_ACCESSES_MASK | VMX_AR_TYPE_READABLE_MASK))
+			check(dpl == 0);
+		else
+			check(dpl == rpl);
+	else if (!unrestricted && type <= VMX_AR_TYPE_BUSY_64_TSS)
+		check(dpl >= rpl);
+	// reserved bits
+	check((ar & VMX_AR_RESERVD_MASK) == 0);
+	// D/B
+	if (seg == VCPU_SREG_CS)
+		check(!long_mode_active || !(ar & VMX_AR_L_MASK) || !(ar & VMX_AR_DB_MASK));
+	// G
+	check((limit & PAGE_M) == PAGE_M || !(ar & VMX_AR_G_MASK));
+	check(limit >> 20 == 0 || ar & VMX_AR_G_MASK);
+}
+
+void vmx_validate_guest_state(void)
+{
+	u32 vmentry_ctl = vmcs_read32(VM_ENTRY_CONTROLS);
+	u32 vmexit_ctl = vmcs_read32(VM_EXIT_CONTROLS);
+	u32 cpu_based_exec_ctrl = vmcs_read32(CPU_BASED_VM_EXEC_CONTROL);
+	u32 pin_based_exec_ctrl = vmcs_read32(PIN_BASED_VM_EXEC_CONTROL);
+	u32 secondary_exec_control = cpu_has_secondary_exec_ctrls() ?
+		vmcs_read32(SECONDARY_VM_EXEC_CONTROL) : 0;
+	u64 rflags = vmcs_readl(GUEST_RFLAGS);
+	bool vm86_active = rflags & X86_EFLAGS_VM;
+	bool long_mode_active = vmentry_ctl & VM_ENTRY_IA32E_MODE;
+	u32 vmentry_intr_info = vmcs_read32(VM_ENTRY_INTR_INFO_FIELD);
+	bool unrestricted = cpu_has_secondary_exec_ctrls() ?
+		secondary_exec_control & SECONDARY_EXEC_UNRESTRICTED_GUEST :
+		false;
+
+	u64 cr0_must_be_zeros = ~read_msr(MSR_IA32_VMX_CR0_FIXED1);
+	u64 host_cr0_must_be_ones = read_msr(MSR_IA32_VMX_CR0_FIXED0);
+	u64 guest_cr0_must_be_ones = host_cr0_must_be_ones &
+		~(unrestricted ? X86_CR0_PG | X86_CR0_PE : 0);
+	u64 cr4_must_be_zeros = ~read_msr(MSR_IA32_VMX_CR4_FIXED1);
+	u64 cr4_must_be_ones = read_msr(MSR_IA32_VMX_CR4_FIXED0);
+
+	u64 pin_based_ctrls = read_msr(MSR_IA32_VMX_TRUE_PINBASED_CTLS);
+	u64 proc_based_ctrls = read_msr(MSR_IA32_VMX_TRUE_PROCBASED_CTLS);
+
+	u64 debug_ctrl = vmcs_read64(GUEST_IA32_DEBUGCTL);
+	u32 activity_state = vmcs_read32(GUEST_ACTIVITY_STATE);
+	u32 interruptibility = vmcs_read32(GUEST_INTERRUPTIBILITY_INFO);
+	u64 pending_dbg_exceptions = vmcs_readl(GUEST_PENDING_DBG_EXCEPTIONS);
+
+	u8 seg;
+
+	// 26.2.1.1: VM-Execution Control Fields
+	CHECK_VMX_CTLS(pin_based_exec_ctrl, pin_based_ctrls);
+	CHECK_VMX_CTLS(cpu_based_exec_ctrl, proc_based_ctrls);
+
+	if (cpu_based_exec_ctrl & CPU_BASED_ACTIVATE_SECONDARY_CONTROLS) {
+		u64 secondary_ctrls = read_msr(MSR_IA32_VMX_PROCBASED_CTLS2);
+		CHECK_VMX_CTLS(secondary_exec_control, secondary_ctrls);
+	}
+
+	check(vmcs_read32(CR3_TARGET_COUNT) == 0);
+
+	check(!(cpu_based_exec_ctrl & CPU_BASED_USE_IO_BITMAPS));
+	check(!(cpu_based_exec_ctrl & CPU_BASED_TPR_SHADOW));
+	check(pin_based_exec_ctrl & PIN_BASED_NMI_EXITING ||
+	      !(pin_based_exec_ctrl & PIN_BASED_VIRTUAL_NMIS));
+	check(pin_based_exec_ctrl & PIN_BASED_VIRTUAL_NMIS ||
+	      !(cpu_based_exec_ctrl & CPU_BASED_NMI_WINDOW_EXITING));
+	check(!(pin_based_exec_ctrl & PIN_BASED_POSTED_INTR));
+
+	// 26.2.1.2: VM-Exit Control Fields
+	CHECK_VMX_CTLS(vmexit_ctl, read_msr(MSR_IA32_VMX_TRUE_EXIT_CTLS));
+	check(pin_based_exec_ctrl & PIN_BASED_VMX_PREEMPTION_TIMER ||
+	      !(vmexit_ctl & VM_EXIT_SAVE_VMX_PREEMPTION_TIMER));
+
+	// 26.2.1.3: VM-Entry Control Fields
+	CHECK_VMX_CTLS(vmentry_ctl, read_msr(MSR_IA32_VMX_TRUE_ENTRY_CTLS));
+	if (vmentry_intr_info & INTR_INFO_VALID_MASK) {
+		u32 type = vmentry_intr_info & INTR_INFO_INTR_TYPE_MASK;
+		u8 vector = vmentry_intr_info & INTR_INFO_VECTOR_MASK;
+		u32 insn_len = vmcs_read32(VM_ENTRY_INSTRUCTION_LEN);
+		bool has_error_code = vmentry_intr_info & INTR_INFO_DELIVER_CODE_MASK;
+
+		check(type != INTR_TYPE_RESERVED);
+		check((u32)(proc_based_ctrls >> 32) & CPU_BASED_MONITOR_TRAP_FLAG ||
+		      type != INTR_TYPE_OTHER_EVENT);
+		check(type != INTR_TYPE_NMI_INTR || vector == BP_VECTOR);
+		check(type != INTR_TYPE_HARD_EXCEPTION || vector <= 31);
+		check(type != INTR_TYPE_OTHER_EVENT || vector == DE_VECTOR);
+
+		check(has_error_code == (vmcs_readl(GUEST_CR0) & X86_CR0_PE &&
+					 (type == INTR_TYPE_HARD_EXCEPTION &&
+					  (vector == DF_VECTOR ||
+					   vector == TS_VECTOR ||
+					   vector == NP_VECTOR ||
+					   vector == SS_VECTOR ||
+					   vector == GP_VECTOR ||
+					   vector == PF_VECTOR ||
+					   vector == AC_VECTOR))));
+
+		check(!(vmentry_intr_info &
+			(INTR_INFO_RESVD_BITS_MASK | INTR_INFO_UNBLOCK_NMI)));
+
+		check(!has_error_code ||
+		      (vmcs_read32(VM_ENTRY_EXCEPTION_ERROR_CODE) & 0xffff8000) == 0);
+		check(!(type == INTR_TYPE_SOFT_INTR ||
+			type == INTR_TYPE_PRIV_SW_EXCEPTION ||
+			type == INTR_TYPE_SOFT_EXCEPTION) ||
+		      insn_len < MAX_INSN_SIZE);
+	}
+
+	check(!(vmentry_ctl & VM_ENTRY_SMM));
+	check(!(vmentry_ctl & VM_ENTRY_DEACT_DUAL_MONITOR));
+
+	// 26.2.2: Checks on Host Control Registers and MSRs
+	check((~vmcs_readl(HOST_CR0) & cr0_must_be_zeros) == cr0_must_be_zeros);
+	check((vmcs_readl(HOST_CR0) & host_cr0_must_be_ones) == host_cr0_must_be_ones);
+
+	check((~vmcs_readl(HOST_CR4) & cr4_must_be_zeros) == cr4_must_be_zeros);
+	check((vmcs_readl(HOST_CR4) & cr4_must_be_ones) == cr4_must_be_ones);
+
+	CHECK_IS_HOST_VMCS_FIELD_ADDR_CANONICAL(IA32_SYSENTER_ESP);
+	CHECK_IS_HOST_VMCS_FIELD_ADDR_CANONICAL(IA32_SYSENTER_EIP);
+
+	if (vmexit_ctl & VM_EXIT_LOAD_IA32_PAT) {
+		u64 pat = vmcs_read64(HOST_IA32_PAT);
+		unsigned i;
+		for (i = 0; i < 8; i++) {
+			u8 byte = pat & 0xff;
+			check(byte < 8 && byte != 2 && byte != 3);
+			pat >>= 8;
+		}
+	}
+
+	if (vmexit_ctl & VM_EXIT_LOAD_IA32_EFER) {
+		u64 efer = vmcs_read64(HOST_IA32_EFER);
+		check((efer & 0xffffffffffff0200ull) == 0);
+		if (vmexit_ctl & VM_EXIT_HOST_ADDR_SPACE_SIZE)
+			check((efer & (EFER_LMA | EFER_LME)) ==
+			      (EFER_LMA | EFER_LME));
+		else
+			check((efer & (EFER_LMA | EFER_LME)) == 0);
+	}
+
+	// 26.2.3: Checks on Host Segment and Descriptor-Table Registers
+	CHECK_HOST_SEG(ES);
+	CHECK_HOST_SEG(CS);
+	CHECK_HOST_SEG(SS);
+	CHECK_HOST_SEG(DS);
+	CHECK_HOST_SEG(FS);
+	CHECK_HOST_SEG(GS);
+	CHECK_HOST_SEG(TR);
+
+	check(vmcs_read16(HOST_CS_SELECTOR) != 0);
+	check(vmcs_read16(HOST_TR_SELECTOR) != 0);
+
+	CHECK_IS_HOST_TABLE_BASE_CANONICAL(FS);
+	CHECK_IS_HOST_TABLE_BASE_CANONICAL(GS);
+	CHECK_IS_HOST_TABLE_BASE_CANONICAL(GDTR);
+	CHECK_IS_HOST_TABLE_BASE_CANONICAL(IDTR);
+	CHECK_IS_HOST_TABLE_BASE_CANONICAL(TR);
+
+	// 26.2.4: Checks Related to Address-Space Size
+	check(vmexit_ctl & VM_EXIT_HOST_ADDR_SPACE_SIZE);
+
+	check(vmcs_readl(HOST_CR4) & X86_CR4_PAE);
+	CHECK_IS_HOST_VMCS_FIELD_ADDR_CANONICAL(RIP);
+
+	// 26.3.1.1: Checks on Guest Control Registers, Debug Registers, and MSRs
+	check((~vmcs_readl(GUEST_CR0) & cr0_must_be_zeros) ==
+	      cr0_must_be_zeros);
+	check((vmcs_readl(GUEST_CR0) & guest_cr0_must_be_ones) ==
+	      guest_cr0_must_be_ones);
+
+	check((~vmcs_readl(GUEST_CR4) & cr4_must_be_zeros) == cr4_must_be_zeros);
+	check((vmcs_readl(GUEST_CR4) & cr4_must_be_ones) == cr4_must_be_ones);
+
+	if (vmentry_ctl & VM_ENTRY_LOAD_DEBUG_CONTROLS) {
+		u64 debug_ctrl_reserved = 0xffffffffffff003cull;
+		check((debug_ctrl & debug_ctrl_reserved) == 0);
+	}
+
+	check(!long_mode_active || vmcs_readl(GUEST_CR4) & X86_CR4_PAE);
+
+	if (vmentry_ctl & VM_ENTRY_LOAD_DEBUG_CONTROLS)
+		check(!((u32)(vmcs_readl(GUEST_DR7) >> 32)));
+
+	CHECK_IS_GUEST_VMCS_FIELD_ADDR_CANONICAL(SYSENTER_ESP);
+	CHECK_IS_GUEST_VMCS_FIELD_ADDR_CANONICAL(SYSENTER_EIP);
+
+	if (vmentry_ctl & VM_ENTRY_LOAD_IA32_PAT) {
+		u64 pat = vmcs_read64(GUEST_IA32_PAT);
+		unsigned i;
+		for (i = 0; i < 8; i++) {
+			u8 byte = pat & 0xff;
+			check(byte < 8 && byte != 2 && byte != 3);
+			pat >>= 8;
+		}
+	}
+
+	if (vmentry_ctl & VM_ENTRY_LOAD_IA32_EFER) {
+		u64 efer = vmcs_read64(GUEST_IA32_EFER);
+		check((efer & 0xffffffffffff0200ull) == 0);
+		if ((vmcs_readl(GUEST_CR0) & X86_CR0_PG) != 0) {
+			if (long_mode_active)
+				check((efer & (EFER_LMA | EFER_LME)) ==
+				      (EFER_LMA | EFER_LME));
+			else
+				check((efer & (EFER_LMA | EFER_LME)) == 0);
+		}
+	}
+
+	if (vmentry_ctl & VM_ENTRY_LOAD_BNDCFGS) {
+		u64 bndcfgs = vmcs_read64(GUEST_BNDCFGS);
+		CHECK_IS_GUEST_ADDR_CANONICAL(bndcfgs & 0xfffffffffffff000ull);
+		check((bndcfgs & 0x00000ffc) == 0);
+	}
+
+	// 26.3.1.2: Checks on Guest Segment Registers
+	for (seg = VCPU_SREG_ES; seg <= VCPU_SREG_LDTR; seg++)
+		vmx_validate_guest_selector(seg, vm86_active, long_mode_active, unrestricted);
+
+	// 26.3.1.3: Checks on Guest Descriptor-Table Registers
+	CHECK_IS_GUEST_TABLE_BASE_CANONICAL(GDTR);
+	CHECK_IS_GUEST_TABLE_BASE_CANONICAL(IDTR);
+
+	// 26.3.1.4: Checks on Guest RIP and RFLAGS
+	check((long_mode_active && vmcs_read32(GUEST_CS_AR_BYTES) & VMX_AR_L_MASK) ||
+	      (u32)(vmcs_readl(GUEST_RIP) >> 32) == 0);
+	CHECK_IS_GUEST_VMCS_FIELD_ADDR_CANONICAL(RIP);
+
+	check((rflags & ((-1ull << 22) | (1 << 15) | (1 << 5) | (1 << 3))) == 0);
+	check((rflags & X86_EFLAGS_FIXED) != 0);
+	check(!long_mode_active || !vm86_active);
+	check(((vmentry_intr_info & (INTR_INFO_VALID_MASK |
+				     INTR_INFO_INTR_TYPE_MASK)) !=
+	       (INTR_INFO_VALID_MASK | INTR_TYPE_EXT_INTR)) ||
+	      (rflags & X86_EFLAGS_IF));
+
+	// 26.3.1.5: Checks on Guest Non-Register State
+	check(activity_state <= GUEST_ACTIVITY_WAIT_SIPI);
+	check(activity_state != GUEST_ACTIVITY_HLT ||
+	      VMX_AR_DPL(vmcs_read32(GUEST_SS_AR_BYTES)) == 0);
+	check(!(interruptibility & (GUEST_INTR_STATE_STI | GUEST_INTR_STATE_MOV_SS)) ||
+	      activity_state == GUEST_ACTIVITY_ACTIVE);
+	if (vmentry_intr_info & INTR_INFO_VALID_MASK) {
+		u32 type = vmentry_intr_info & INTR_INFO_INTR_TYPE_MASK;
+		u8 vector = vmentry_intr_info & INTR_INFO_VECTOR_MASK;
+
+		check(activity_state != GUEST_ACTIVITY_HLT ||
+		      (type == INTR_TYPE_EXT_INTR ||
+		       type == INTR_TYPE_NMI_INTR ||
+		       (type == INTR_TYPE_HARD_EXCEPTION &&
+			(vector == DB_VECTOR || vector == MC_VECTOR)) ||
+		       (type == INTR_TYPE_OTHER_EVENT && vector == DB_VECTOR)));
+		check(activity_state != GUEST_ACTIVITY_SHUTDOWN ||
+		      (type == INTR_TYPE_NMI_INTR ||
+		       (type == INTR_TYPE_HARD_EXCEPTION && vector == MC_VECTOR)));
+		check(activity_state != GUEST_ACTIVITY_WAIT_SIPI);
+	}
+
+	check((interruptibility & 0xFFFFFFE0) == 0);
+	check((interruptibility & (GUEST_INTR_STATE_STI | GUEST_INTR_STATE_MOV_SS)) !=
+	      (GUEST_INTR_STATE_STI | GUEST_INTR_STATE_MOV_SS));
+	check(rflags & X86_EFLAGS_IF || !(interruptibility & GUEST_INTR_STATE_STI));
+	check(((vmentry_intr_info & (INTR_INFO_VALID_MASK | INTR_INFO_INTR_TYPE_MASK)) !=
+	       (INTR_INFO_VALID_MASK | INTR_TYPE_EXT_INTR)) ||
+	      (interruptibility & (GUEST_INTR_STATE_STI | GUEST_INTR_STATE_MOV_SS)) == 0);
+	check(((vmentry_intr_info & (INTR_INFO_VALID_MASK | INTR_INFO_INTR_TYPE_MASK)) !=
+	       (INTR_INFO_VALID_MASK | INTR_TYPE_NMI_INTR)) ||
+	      (interruptibility & GUEST_INTR_STATE_MOV_SS) == 0);
+	check((interruptibility & GUEST_INTR_STATE_SMI) == 0);
+	// Some processors require the following check; others do not.
+	check(((vmentry_intr_info & (INTR_INFO_VALID_MASK | INTR_INFO_INTR_TYPE_MASK)) !=
+	       (INTR_INFO_VALID_MASK | INTR_TYPE_NMI_INTR)) ||
+	      (interruptibility & GUEST_INTR_STATE_STI) == 0);
+	check(((vmentry_intr_info & (INTR_INFO_VALID_MASK | INTR_INFO_INTR_TYPE_MASK)) !=
+	       (INTR_INFO_VALID_MASK | INTR_TYPE_NMI_INTR)) ||
+	      (pin_based_exec_ctrl & PIN_BASED_VIRTUAL_NMIS) == 0 ||
+	      (interruptibility & GUEST_INTR_STATE_NMI) == 0);
+
+	check((u32)(pending_dbg_exceptions >> 32) == 0);
+	check((pending_dbg_exceptions & 0xfffeaff0) == 0);
+	if ((interruptibility & (GUEST_INTR_STATE_STI | GUEST_INTR_STATE_MOV_SS)) != 0 ||
+	    activity_state == GUEST_ACTIVITY_HLT) {
+		check(!((rflags & X86_EFLAGS_TF) && !(debug_ctrl & 0x2)) ||
+		      (pending_dbg_exceptions & 0x00004000));
+		check(!(!(rflags & X86_EFLAGS_TF) || (debug_ctrl & 0x2)) ||
+		      !(pending_dbg_exceptions & 0x00004000));
+	}
+}
-- 
2.34.1

