diff --git a/arch/x86/Kbuild b/arch/x86/Kbuild
index f384cb1a4f7a..44e7bea7cee0 100644
--- a/arch/x86/Kbuild
+++ b/arch/x86/Kbuild
@@ -3,6 +3,8 @@ obj-y += entry/
 
 obj-$(CONFIG_PERF_EVENTS) += events/
 
+obj-$(CONFIG_DEPRIV) += depriv/
+
 obj-$(CONFIG_KVM) += kvm/
 
 # Xen paravirtualization support
diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig
index 407533c835fe..6fe6b58305c8 100644
--- a/arch/x86/Kconfig
+++ b/arch/x86/Kconfig
@@ -221,6 +221,7 @@ config X86
 	select HAVE_FUNCTION_ERROR_INJECTION
 	select HAVE_KRETPROBES
 	select HAVE_KVM
+	select HAVE_DEPRIV			if X86_64
 	select HAVE_LIVEPATCH			if X86_64
 	select HAVE_MIXED_BREAKPOINTS_REGS
 	select HAVE_MOD_ARCH_SPECIFIC
@@ -2884,4 +2885,6 @@ config HAVE_ATOMIC_IOMAP
 
 source "arch/x86/kvm/Kconfig"
 
+source "arch/x86/depriv/Kconfig"
+
 source "arch/x86/Kconfig.assembler"
diff --git a/arch/x86/depriv/Kconfig b/arch/x86/depriv/Kconfig
new file mode 100644
index 000000000000..03d453aa3dc6
--- /dev/null
+++ b/arch/x86/depriv/Kconfig
@@ -0,0 +1,33 @@
+# SPDX-License-Identifier: GPL-2.0
+#
+# Depriv configuration
+#
+
+source "virt/depriv/Kconfig"
+
+if VIRTUALIZATION
+
+config DEPRIV
+	tristate "Deprivilege Linux using hardware virtualization extensions"
+	depends on HAVE_DEPRIV
+	help
+	  Deprivilege Linux kernel using hardware virtualization extensions.
+
+	  If unsure, say N.
+
+config DEPRIV_WERROR
+	bool "Compile depriv with -Werror"
+	# KASAN may cause the build to fail due to larger frames
+	default y if X86_64 && !KASAN
+	# We use the dependency on !COMPILE_TEST to not be enabled
+	# blindly in allmodconfig or allyesconfig configurations
+	depends on (X86_64 && !KASAN) || !COMPILE_TEST
+	depends on EXPERT
+
+config DEPRIV_INTEL
+	tristate "Deprivilege for Intel"
+	depends on DEPRIV
+	help
+	  Run Linux kernel in VMX non-root mode when possible.
+
+endif # VIRTUALIZATION
diff --git a/arch/x86/depriv/Makefile b/arch/x86/depriv/Makefile
new file mode 100644
index 000000000000..45e9e2fe1d30
--- /dev/null
+++ b/arch/x86/depriv/Makefile
@@ -0,0 +1,17 @@
+# SPDX-License-Identifier: GPL-2.0
+
+ccflags-y	+= -Iarch/x86/depriv -Iarch/x86
+ccflags-$(CONFIG_DEPRIV_WERROR)	+= -Werror
+
+ifeq ($(CONFIG_FRAME_POINTER),y)
+OBJECT_FILES_NON_STANDARD_entry.o := y
+endif
+
+DEPRIV := ../../../virt/depriv
+
+#depriv-y	+= $(DERPIV)/depriv_main.o
+
+depriv-intel-y	+= vmx/vmx.o vmx/depriv.o vmx/entry.o vmx/handler.o vmx/validator.o
+
+#obj-$(CONFIG_DEPRIV)		+= depriv.o
+obj-$(CONFIG_DEPRIV_INTEL)	+= depriv-intel.o
diff --git a/arch/x86/depriv/vmx/depriv.c b/arch/x86/depriv/vmx/depriv.c
new file mode 100644
index 000000000000..2e63a15a821e
--- /dev/null
+++ b/arch/x86/depriv/vmx/depriv.c
@@ -0,0 +1,702 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Deprivilege is to run Linux kernel in VMX non-root mode
+ *
+ * Authors:
+ * 	Xin Li <fantry@gmail.com>
+ */
+
+#include <linux/delay.h>
+#include <linux/module.h>
+#include <linux/pid_namespace.h>
+#include <linux/slab.h>
+#include <linux/reboot.h>
+
+#include <asm/tlbflush.h>
+#include <asm/x86_vcpu_regs.h>
+
+#include "vmx.h"
+
+MODULE_AUTHOR("Xin Li");
+MODULE_LICENSE("GPL");
+
+#define VMX_VMCALL_FIRST	(0)
+#define VMX_VMCALL_REPRIV	VMX_VMCALL_FIRST
+#define VMX_VMCALL_CLEAR_VMCS	(VMX_VMCALL_FIRST + 1)
+#define VMX_VMCALL_LAST		VMX_VMCALL_CLEAR_VMCS
+
+static bool __read_mostly test_early_invalid_state = 1;
+module_param(test_early_invalid_state, bool, S_IRUGO);
+
+static bool __read_mostly test_handle_invalid_host_state = 0;
+module_param(test_handle_invalid_host_state, bool, S_IRUGO);
+
+static bool __read_mostly test_handle_invalid_guest_state = 0;
+module_param(test_handle_invalid_guest_state, bool, S_IRUGO);
+
+static bool __read_mostly call_extra_exit_handlers = 1;
+module_param(call_extra_exit_handlers, bool, S_IRUGO);
+
+static unsigned int __read_mostly log_mod = 100000;
+module_param(log_mod, uint, 0444);
+
+static bool __read_mostly intercept_msr = 0;
+module_param(intercept_msr, bool, S_IRUGO);
+
+/*
+ * host state buffer page order is 2, meaning 4 pages will be allocated:
+ *	page 0: trampoline stack
+ *	page 1: VMCS MSR bitmap
+ *	page 2: root mode PGD page
+ *	page 3: VMCS for VMXON
+ */
+#define DEPRIV_CPU_STATE_PAGE_ORDER		2
+#define DEPRIV_CPU_STATE_BUFFER_SIZE		(PAGE_SIZE << DEPRIV_CPU_STATE_PAGE_ORDER)
+#define DEPRIV_CPU_STATE_VMXON_VMCS		(DEPRIV_CPU_STATE_BUFFER_SIZE - PAGE_SIZE)
+#define DEPRIV_CPU_STATE_ROOT_PGD		(DEPRIV_CPU_STATE_VMXON_VMCS - PAGE_SIZE)
+#define DEPRIV_CPU_STATE_VMCS_MSR_BITMAP	(DEPRIV_CPU_STATE_ROOT_PGD - PAGE_SIZE)
+
+/*
+ * needed to iret to root mode kernel or user space when the VM exit happened
+ */
+#define DEPRIV_HOST_STACK_RESERVED_BYTES	(16 * 8)
+
+#define DEPRIV_HOST_STACK_VM_EXIT_COUNT		(0 * 8)
+#define DEPRIV_HOST_STACK_IRET_STACK		(1 * 8)
+
+struct cpumask cpu_depriv_mode_mask;
+
+static struct cpumask cpu_vmx_operation_mask;
+static bool volatile depriv_exiting = false;
+static DEFINE_PER_CPU(struct vmcs *, current_vmcs);
+
+static inline void __hardware_disable(void)
+{
+	int cpu = raw_smp_processor_id();
+	struct vmcs *vmcs;
+
+	pr_debug("depriv: disabling VMX on cpu%d\n", cpu);
+
+	if (!cpumask_test_cpu(cpu, &cpu_vmx_operation_mask))
+		return;
+
+	vmcs = init_pid_ns.depriv_context->cpu_context[cpu];
+	if (vmcs) {
+		vmcs_clear(vmcs);
+		free_vmcs(vmcs);
+		init_pid_ns.depriv_context->cpu_context[cpu] = NULL;
+	}
+
+	per_cpu(current_vmcs, cpu) = NULL;
+
+	__cpu_vmxoff();
+	cpumask_clear_cpu(cpu, &cpu_vmx_operation_mask);
+
+	pr_debug("depriv: VMX disabled on cpu%d\n", cpu);
+}
+
+static void vmx_repriv_cpu_release_resources(void *unused)
+{
+	int cpu = raw_smp_processor_id();
+	void *host_cpu_state = per_cpu(depriv_cpu_state, cpu);
+
+	BUG_ON(!arch_irqs_disabled());
+
+	/*
+	 * must be executed in root mode.
+	 */
+	if (cpumask_test_cpu(cpu, &cpu_depriv_mode_mask))
+		pr_err("depriv: cpu%d still in depriv mode\n", cpu);
+
+	__hardware_disable();
+
+	if (!host_cpu_state)
+		return;
+
+	per_cpu(depriv_cpu_state, cpu) = NULL;
+	memset(host_cpu_state, 0, DEPRIV_CPU_STATE_BUFFER_SIZE);
+	free_pages((unsigned long)host_cpu_state, DEPRIV_CPU_STATE_PAGE_ORDER);
+	pr_debug("depriv: repriv cpu%d released cpu state buffer\n", cpu);
+}
+
+int asm_vmx_depriv(bool launch);
+void vmx_depriv_rip(void);
+void vmx_validate_vmcs(void);
+
+/*
+ * WARNING: must be called with interrupt disabled!
+ */
+static bool __vmx_depriv(bool launch)
+{
+	int cpu = raw_smp_processor_id();
+	unsigned long rsp;
+	int depriv_result;
+
+	asm volatile("mov %%rsp,%0" : "=m"(rsp));
+	// reserve extra 8 bytes for RIP pushed to stack when calling asm_vmx_depriv
+	vmcs_writel(GUEST_RSP, rsp - 8);
+	vmcs_writel(GUEST_RFLAGS, arch_local_save_flags());
+	vmcs_writel(GUEST_RIP, (unsigned long)vmx_depriv_rip);
+
+	/*
+	 * Should we save/restore general purpose registers around asm_vmx_depriv?
+	 */
+	depriv_result = asm_vmx_depriv(launch);
+	switch (depriv_result) {
+	case 0: // switched to non-root mode
+		cpumask_set_cpu(cpu, &cpu_depriv_mode_mask);
+		return true;
+	// still in root mode
+	case 1:
+		pr_err("depriv: cpu%d launch failed with invalid host state\n", cpu);
+		break;
+	case 2:
+		pr_err("depriv: cpu%d launch failed with invalid guest state\n", cpu);
+		break;
+	case 3:
+		pr_err("depriv: cpu%d resume failed\n", cpu);
+		break;
+	default:
+		pr_err("depriv: cpu%d unknown deprivilege error %d\n", cpu, depriv_result);
+		BUG();
+	}
+
+	vmx_validate_vmcs();
+	return false;
+}
+
+static inline int __hardware_enable(struct vmcs *vmcs)
+{
+	int cpu = raw_smp_processor_id();
+	int r;
+
+	pr_debug("depriv: enabling VMX on cpu%d\n", cpu);
+
+	if (cpumask_test_cpu(cpu, &cpu_vmx_operation_mask)) {
+		pr_err("depriv: VMX operation already enabled on cpu%d\n", cpu);
+		return -EBUSY;
+	}
+
+	vmcs->hdr.revision_id = depriv_vmcs_config.revision_id;
+	r = __cpu_vmxon(__pa(vmcs));
+	if (r)
+		return r;
+
+	pr_debug("depriv: VMX enabled on cpu%d\n", cpu);
+	ept_sync_global();
+	cpumask_set_cpu(cpu, &cpu_vmx_operation_mask);
+	return 0;
+}
+
+static inline bool init_vmcs(struct pid_namespace *ns)
+{
+	int cpu = raw_smp_processor_id();
+	struct vmcs *vmcs;
+	void *host_cpu_state;
+	unsigned long host_rsp;
+
+	vmcs = alloc_vmcs();
+	if (!vmcs) {
+		pr_err("depriv: cpu%d unable to allocate VMCS for namespace %p\n", cpu, ns);
+		return false;
+	}
+
+	vmcs_clear(vmcs);
+	vmcs_load(vmcs);
+	indirect_branch_prediction_barrier();
+	per_cpu(current_vmcs, cpu) = vmcs;
+	ns->depriv_context->cpu_context[cpu] = vmcs;
+	vmcs_write64(VMCS_LINK_POINTER, ~0ull);
+	vmcs_write64(EPT_POINTER, (__pa(vmcs) + PAGE_SIZE) | VMX_EPTP_MT_WB | VMX_EPTP_PWL_4);
+	if (cpu_has_vmx_spp())
+		vmcs_write64(SPPT_POINTER, __pa(vmcs) + PAGE_SIZE * 3);
+	pr_debug("depriv: cpu%d eptp %#llx for namespace %p\n", cpu, vmcs_read64(EPT_POINTER), ns);
+
+	host_cpu_state = per_cpu(depriv_cpu_state, cpu);
+	vmcs_writel(HOST_CR3, __pa(host_cpu_state + DEPRIV_CPU_STATE_ROOT_PGD));
+	vmcs_write64(MSR_BITMAP, __pa(host_cpu_state + DEPRIV_CPU_STATE_VMCS_MSR_BITMAP));
+
+	vmx_depriv_cpu_state();
+
+	// page 0 of host state: reserve extra DEPRIV_HOST_STACK_RESERVED_BYTES bytes
+	// for reprivileging host
+	host_rsp = (unsigned long)host_cpu_state
+		+ DEPRIV_CPU_STATE_VMCS_MSR_BITMAP - DEPRIV_HOST_STACK_RESERVED_BYTES;
+	vmcs_writel(HOST_RSP, host_rsp);
+
+	*(unsigned long *)(host_rsp + DEPRIV_HOST_STACK_IRET_STACK) =
+		depriv_iret_trampoline_stack(cpu);
+
+	return true;
+}
+
+static void vmx_depriv_cpu(void *info)
+{
+	int cpu = raw_smp_processor_id(), r;
+	unsigned long cr3_pa = (unsigned long)info;
+	struct page *page = NULL;
+	void *host_cpu_state;
+
+	BUG_ON(!arch_irqs_disabled());
+
+	per_cpu(current_vmcs, cpu) = NULL;
+
+	// memory for root mode VM exit handler
+	page = __alloc_pages_node(cpu_to_node(cpu), GFP_ATOMIC, DEPRIV_CPU_STATE_PAGE_ORDER);
+	if (!page) {
+		pr_err("depriv: unable to allocate host state buffer for cpu%d\n", cpu);
+		goto error;
+	}
+
+	host_cpu_state = page_address(page);
+	memset(host_cpu_state, 0, DEPRIV_CPU_STATE_BUFFER_SIZE);
+	per_cpu(depriv_cpu_state, cpu) = host_cpu_state;
+
+	// page 3 of host state
+	r = __hardware_enable((struct vmcs *)(host_cpu_state + DEPRIV_CPU_STATE_VMXON_VMCS));
+	if (r) {
+		pr_err("depriv: vmxon error %d, unable to enable VMX on cpu%d\n", r, cpu);
+		goto error;
+	}
+
+	// page 2 of host state
+	memcpy(host_cpu_state + DEPRIV_CPU_STATE_ROOT_PGD, __va(cr3_pa), PAGE_SIZE);
+
+	// page 1 of host state
+	if (intercept_msr)
+		memset(host_cpu_state + DEPRIV_CPU_STATE_VMCS_MSR_BITMAP, 0xffffffff, PAGE_SIZE);
+
+	BUG_ON(init_pid_ns.depriv_context->cpu_context[cpu] != NULL);
+	if (!init_vmcs(&init_pid_ns))
+		goto error;
+
+	if (test_early_invalid_state && test_handle_invalid_host_state)
+		vmcs_write32(CR3_TARGET_COUNT, DEPRIV_INVALID_HOST_CR3_TARGET_COUNT);
+
+	if (test_early_invalid_state && test_handle_invalid_guest_state) {
+		u32 ar = vmcs_read32(GUEST_TR_AR_BYTES);
+		vmcs_write32(GUEST_TR_AR_BYTES, ar | VMX_AR_S_MASK);
+	}
+
+	/* switching to non-root mode */
+	if (__vmx_depriv(true))
+		return;
+
+error:
+	pr_err("depriv: cpu%d failed to deprivilege\n", cpu);
+}
+
+static inline void vmx_vmcall(long call_no, void *info)
+{
+	int cpu = raw_smp_processor_id();
+	void *host_cpu_state = per_cpu(depriv_cpu_state, cpu);
+
+	if (call_no < VMX_VMCALL_FIRST || call_no > VMX_VMCALL_LAST) {
+		pr_err("depriv: cpu%d invalid vmcall no %ld\n", cpu, call_no);
+		return;
+	}
+
+	asm_volatile_goto("1: vmcall\n\t"
+			  _ASM_EXTABLE(1b, %l[fault])
+			  : : "D" (call_no), "S" (info) : : fault);
+
+	pr_info("depriv: cpu%d vmcalled\n", cpu);
+	if (host_cpu_state) {
+		long host_rsp = (unsigned long)host_cpu_state
+			+ DEPRIV_CPU_STATE_VMCS_MSR_BITMAP - DEPRIV_HOST_STACK_RESERVED_BYTES;
+		unsigned long *cnt = (unsigned long *)(host_rsp + DEPRIV_HOST_STACK_VM_EXIT_COUNT);
+		pr_info("depriv: cpu%d (%ld) vmcalled\n", cpu, *cnt);
+	}
+
+	return;
+
+fault:
+	pr_err("depriv: cpu%d vmcall faulted, already in root mode\n", cpu);
+
+	if (call_no == VMX_VMCALL_CLEAR_VMCS) {
+		vmcs_clear(info);
+		return;
+	}
+}
+
+/*
+ * WARNING: must be called with interrupt disabled!
+ */
+static inline void __vmx_repriv(void)
+{
+	int cpu = raw_smp_processor_id();
+
+	if (!cpumask_test_cpu(cpu, &cpu_vmx_operation_mask)) {
+		pr_err("depriv: cpu%d NOT in vmx operation\n", cpu);
+		return;
+	}
+
+	if (cpumask_test_cpu(cpu, &cpu_depriv_mode_mask))
+		vmx_vmcall(VMX_VMCALL_REPRIV, NULL);
+	else
+		pr_debug("depriv: cpu%d already in root mode\n", cpu);
+}
+
+static void vmx_repriv_cpu(void *info)
+{
+	int cpu = raw_smp_processor_id();
+	void *host_cpu_state = per_cpu(depriv_cpu_state, cpu);
+
+	BUG_ON(!arch_irqs_disabled());
+
+	if (!host_cpu_state) {
+		pr_info("depriv: cpu%d already reprivileged\n", cpu);
+		return;
+	}
+
+	__vmx_repriv();
+
+	// switched to root mode
+	pr_debug("depriv: cpu%d reprivileged\n", cpu);
+}
+
+#define DEPRIV_CONTINUE_IN_NON_ROOT_MODE(ins_len) do {				\
+	vmcs_writel(GUEST_RIP, rip + ins_len);					\
+	cpumask_set_cpu(cpu, &cpu_depriv_mode_mask);				\
+	return true;								\
+} while (0)
+
+#define DEPRIV_SWITCH_TO_ROOT_MODE do {						\
+	return false;								\
+} while (0)
+
+void dump_guest_insn(unsigned long rip, int insn_len, char *insn);
+int vmx_depriv_handle(u32 reason, unsigned long *regs, unsigned long cnt);
+void dump_va_page_table_entry(unsigned long va);
+
+bool vmx_depriv_vmexit_handler(unsigned long *regs)
+{
+	unsigned long host_rsp = vmcs_readl(HOST_RSP);
+	unsigned long *cnt = (unsigned long *)(host_rsp + DEPRIV_HOST_STACK_VM_EXIT_COUNT);
+	unsigned long counter;
+	unsigned long rip = vmcs_readl(GUEST_RIP);
+	unsigned long rsp = vmcs_readl(GUEST_RSP);
+	int cpu = raw_smp_processor_id();
+	u32 reason = vmcs_read32(VM_EXIT_REASON), insn_len = 0;
+	char insn[64];
+
+	BUG_ON(!arch_irqs_disabled());
+
+	cpumask_clear_cpu(cpu, &cpu_depriv_mode_mask);
+
+	regs[__VCPU_REGS_RSP] = rsp;
+
+	(*cnt)++;
+	counter = *cnt;
+
+	if (reason & VMX_EXIT_REASONS_FAILED_VMENTRY) {
+		pr_err("depriv: cpu%d (%ld) VM-entry failed @ rip: %#lx rsp: %#lx\n",
+		       cpu, counter, rip, rsp);
+		reason &= ~VMX_EXIT_REASONS_FAILED_VMENTRY;
+
+		if (rip == (unsigned long)vmx_depriv_rip)
+			regs[__VCPU_REGS_RAX] = 2;
+
+		vmx_validate_vmcs();
+		DEPRIV_SWITCH_TO_ROOT_MODE;
+	}
+
+	if (!(counter % log_mod))
+		pr_info("depriv: cpu%d (%ld) exit reason: %d cpu depriv mode mask: %*pb[l]\n",
+			cpu, counter, reason, cpumask_pr_args(&cpu_depriv_mode_mask));
+
+	switch (reason) {
+	case EXIT_REASON_TRIPLE_FAULT:
+		pr_info("depriv: cpu%d (%ld) exit reason: %d cpu depriv mode mask: %*pb[l]\n",
+			cpu, counter, reason, cpumask_pr_args(&cpu_depriv_mode_mask));
+		dump_guest_insn(rip, vmcs_read32(VM_EXIT_INSTRUCTION_LEN), insn);
+		pr_info("depriv: cpu%d (%ld) exit reason: %d rip: %#lx rsp: %#lx insn: %s\n",
+			cpu, counter, reason, rip, rsp, insn);
+		dump_va_page_table_entry(rip);
+		DEPRIV_SWITCH_TO_ROOT_MODE;
+		break;
+	case EXIT_REASON_CPUID:
+		if (!(counter % log_mod))
+			pr_info("depriv: cpu%d (%ld) cpuid[%#x]\n",
+				cpu, counter, (u32)regs[__VCPU_REGS_RAX]);
+
+		native_cpuid((unsigned int *)&regs[__VCPU_REGS_RAX],
+			     (unsigned int *)&regs[__VCPU_REGS_RBX],
+			     (unsigned int *)&regs[__VCPU_REGS_RCX],
+			     (unsigned int *)&regs[__VCPU_REGS_RDX]);
+
+		if (!test_early_invalid_state && counter > 50) {
+			if (test_handle_invalid_host_state)
+				vmcs_write32(CR3_TARGET_COUNT, DEPRIV_INVALID_HOST_CR3_TARGET_COUNT);
+
+			if (test_handle_invalid_guest_state) {
+				u32 ar = vmcs_read32(GUEST_TR_AR_BYTES);
+				vmcs_write32(GUEST_TR_AR_BYTES, ar | VMX_AR_S_MASK);
+			}
+		}
+
+		insn_len = vmcs_read32(VM_EXIT_INSTRUCTION_LEN);
+		pr_debug("depriv: cpu%d (%ld) executed cpuid\n", cpu, counter);
+		DEPRIV_CONTINUE_IN_NON_ROOT_MODE(insn_len);
+		break;
+
+	case EXIT_REASON_VMCALL: {
+		long call_no = regs[__VCPU_REGS_RDI];
+		void *info = (void *)regs[__VCPU_REGS_RSI];
+		char comm[sizeof(current->comm)];
+
+		pr_debug("depriv: cpu%d (%ld) exit reason: %d cpu depriv mode mask: %*pb[l]\n",
+			 cpu, counter, reason, cpumask_pr_args(&cpu_depriv_mode_mask));
+
+		insn_len = vmcs_read32(VM_EXIT_INSTRUCTION_LEN);
+
+		switch (call_no) {
+		case VMX_VMCALL_REPRIV:
+			pr_debug("depriv: cpu%d (%ld) vmcall @ %#lx, switch to root mode\n",
+				 cpu, counter, rip);
+			vmcs_writel(GUEST_RIP, rip + insn_len);
+			DEPRIV_SWITCH_TO_ROOT_MODE;
+			break;
+		case VMX_VMCALL_CLEAR_VMCS:
+			pr_debug("depriv: cpu%d (%ld) vmcall to clear VMCS %p\n",
+				 cpu, counter, info);
+			vmcs_clear((struct vmcs *)info);
+			DEPRIV_CONTINUE_IN_NON_ROOT_MODE(insn_len);
+			break;
+		default:
+			get_task_comm(comm, current);
+			pr_err("depriv: cpu%d (%ld) invalid vmcall no %ld @ %#lx in task %s\n",
+			       cpu, counter, call_no, rip, comm);
+			DEPRIV_DUMP_GPRS(regs);
+			DEPRIV_CONTINUE_IN_NON_ROOT_MODE(insn_len);
+			break;
+		}
+		break;
+	}
+
+	default:
+		if (call_extra_exit_handlers && !vmx_depriv_handle(reason, regs, counter)) {
+			if (reason != EXIT_REASON_EXCEPTION_NMI)
+				insn_len = vmcs_read32(VM_EXIT_INSTRUCTION_LEN);
+			DEPRIV_CONTINUE_IN_NON_ROOT_MODE(insn_len);
+		}
+
+		/* switch to root mode */
+		dump_guest_insn(rip, vmcs_read32(VM_EXIT_INSTRUCTION_LEN), insn);
+		pr_info("depriv: cpu%d (%ld) exit reason: %d rip: %#lx rsp: %#lx insn: %s\n",
+			cpu, counter, reason, rip, rsp, insn);
+		DEPRIV_SWITCH_TO_ROOT_MODE;
+		break;
+	}
+}
+
+static bool vmx_depriv(struct task_struct *next)
+{
+	int cpu;
+	struct pid_namespace *ns = task_active_pid_ns(next);
+	struct vmcs *vmcs;
+	bool r = false;
+
+	BUG_ON(!arch_irqs_disabled());
+
+	if (!ns)
+		goto out;
+
+	if (depriv_exiting)
+		goto out;
+
+	if (test_handle_invalid_host_state || test_handle_invalid_guest_state)
+		goto out;
+
+	cpu = raw_smp_processor_id();
+	if (!cpumask_test_cpu(cpu, &cpu_vmx_operation_mask))
+		goto out;
+
+	vmcs = ns->depriv_context->cpu_context[cpu];
+	if (vmcs) {
+		if (per_cpu(current_vmcs, cpu) != vmcs) {
+			vmcs_load(vmcs);
+			indirect_branch_prediction_barrier();
+			per_cpu(current_vmcs, cpu) = vmcs;
+		}
+
+		vmcs_writel(GUEST_CR3, __read_cr3());
+		vmx_depriv_cpu_fsgs();
+
+		r = __vmx_depriv(false);
+	} else {
+		if (!init_vmcs(ns))
+			goto out;
+
+		r = __vmx_depriv(true);
+	}
+
+out:
+	return r;
+}
+
+static void vmx_repriv(void)
+{
+	int cpu;
+
+	BUG_ON(!arch_irqs_disabled());
+
+	if (test_handle_invalid_host_state || test_handle_invalid_guest_state)
+		return;
+
+	cpu = raw_smp_processor_id();
+	if (!cpumask_test_cpu(cpu, &cpu_vmx_operation_mask))
+		return;
+
+	__vmx_repriv();
+}
+
+static void vmx_free_loaded_vmcs(void *info)
+{
+	struct pid_namespace *ns = (struct pid_namespace *)info;
+	int cpu = raw_smp_processor_id();
+	struct vmcs *vmcs = ns->depriv_context->cpu_context[cpu];
+
+	BUG_ON(!arch_irqs_disabled());
+
+	pr_debug("depriv: cpu%d clearing vmcs %p for namespace %p\n", cpu, vmcs, ns);
+	vmx_vmcall(VMX_VMCALL_CLEAR_VMCS, vmcs);
+	free_vmcs(vmcs);
+	ns->depriv_context->cpu_context[cpu] = NULL;
+}
+
+static void vmx_on_destroy_pid_ns(struct pid_namespace *ns)
+{
+	struct cpumask depriv_cpu_context_mask;
+	int cpu;
+
+	if (!ns)
+		return;
+
+	cpumask_clear(&depriv_cpu_context_mask);
+
+	for_each_cpu(cpu, cpu_online_mask)
+		if (ns->depriv_context->cpu_context[cpu] != NULL)
+			cpumask_set_cpu(cpu, &depriv_cpu_context_mask);
+
+	on_each_cpu_mask(&depriv_cpu_context_mask, vmx_free_loaded_vmcs, (void *)ns, 0);
+}
+
+extern struct depriv_ops depriv_ops;
+
+static void vmx_dump_context(unsigned long va)
+{
+	int cpu;
+
+	cpu = raw_smp_processor_id();
+
+	if (!per_cpu(depriv_cpu_state, cpu))
+		return;
+
+	pr_err("depriv: cpu%d depriv mode mask: %*pb[l]\n",
+	       cpu, cpumask_pr_args(&cpu_depriv_mode_mask));
+	dump_va_page_table_entry(va);
+}
+
+static void vmx_depriv_cleanup(void)
+{
+	depriv_ops.enter = NULL;
+	depriv_ops.exit = NULL;
+	depriv_ops.on_destroy_pid_ns = NULL;
+	depriv_ops.dump_context = NULL;
+
+	pr_info("depriv: cpu vmx operation mask before repriv cpu: %*pb[l]\n",
+		cpumask_pr_args(&cpu_vmx_operation_mask));
+	pr_info("depriv: cpu depriv mode mask before repriv cpu: %*pb[l]\n",
+		cpumask_pr_args(&cpu_depriv_mode_mask));
+
+	on_each_cpu(vmx_repriv_cpu, NULL, 1);
+
+	pr_info("depriv: cpu depriv mode mask after repriv cpu: %*pb[l]\n",
+		cpumask_pr_args(&cpu_depriv_mode_mask));
+
+	on_each_cpu(vmx_repriv_cpu_release_resources, NULL, 1);
+
+	pr_info("depriv: successfully unloaded, cpu vmx operation mask: %*pb[l]\n",
+		cpumask_pr_args(&cpu_vmx_operation_mask));
+}
+
+static int vmx_depriv_reboot(struct notifier_block *notifier, unsigned long val, void *v)
+{
+	depriv_exiting = true;
+
+	pr_info("depriv: %d cpus deprivileged, reprivileging while rebooting...\n",
+		cpumask_weight(&cpu_vmx_operation_mask));
+
+	vmx_depriv_cleanup();
+	return NOTIFY_OK;
+}
+
+static struct notifier_block depriv_reboot_notifier = {
+	.notifier_call = vmx_depriv_reboot,
+	.priority = 0,
+};
+
+static int __init vmx_depriv_init(void)
+{
+	int r = setup_vmcs_config();
+
+	if (r) {
+		pr_err("depriv: error setting up deprivilege VMCS config\n");
+		return r;
+	}
+
+	r = -EIO;
+
+	cpumask_clear(&cpu_vmx_operation_mask);
+	cpumask_clear(&cpu_depriv_mode_mask);
+	depriv_exiting = false;
+
+	pr_info("depriv: cpu vmx operation mask before depriv cpu: %*pb[l]\n",
+		cpumask_pr_args(&cpu_vmx_operation_mask));
+	pr_info("depriv: cpu depriv mode mask before depriv cpu: %*pb[l]\n",
+		cpumask_pr_args(&cpu_depriv_mode_mask));
+
+	on_each_cpu(vmx_depriv_cpu, (void *)read_cr3_pa(), 1);
+
+	pr_info("depriv: cpu depriv mode mask after depriv cpu: %*pb[l]\n",
+		cpumask_pr_args(&cpu_depriv_mode_mask));
+	pr_info("depriv: cpu vmx operation mask after depriv cpu: %*pb[l]\n",
+		cpumask_pr_args(&cpu_vmx_operation_mask));
+
+	if (cpumask_empty(&cpu_vmx_operation_mask))
+		return r;
+
+	depriv_ops.enter = vmx_depriv;
+	depriv_ops.exit = vmx_repriv;
+	depriv_ops.on_destroy_pid_ns = vmx_on_destroy_pid_ns;
+	depriv_ops.dump_context = vmx_dump_context;
+
+	register_reboot_notifier(&depriv_reboot_notifier);
+	pr_info("depriv: successfully initialized\n");
+	return 0;
+}
+
+static void __exit vmx_depriv_exit(void)
+{
+	depriv_exiting = true;
+
+	pr_info("depriv: %d cpus deprivileged, reprivileging...\n",
+		cpumask_weight(&cpu_vmx_operation_mask));
+
+	if (test_handle_invalid_host_state || test_handle_invalid_guest_state) {
+		test_early_invalid_state = true;
+		test_handle_invalid_host_state = false;
+		test_handle_invalid_guest_state = false;
+
+		// XXX wait for all cpus to exit testing mode, there are better ways
+		msleep(100);
+	}
+
+	unregister_reboot_notifier(&depriv_reboot_notifier);
+
+	vmx_depriv_cleanup();
+}
+
+module_init(vmx_depriv_init);
+module_exit(vmx_depriv_exit);
diff --git a/arch/x86/depriv/vmx/entry.S b/arch/x86/depriv/vmx/entry.S
new file mode 100644
index 000000000000..93dbc0da53af
--- /dev/null
+++ b/arch/x86/depriv/vmx/entry.S
@@ -0,0 +1,192 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#include <linux/linkage.h>
+#include <asm/asm.h>
+#include <asm/kvm_vcpu_regs.h>
+#include <asm/msr-index.h>
+#include <asm/bitsperlong.h>
+
+#define WORD_SIZE (BITS_PER_LONG / 8)
+
+.macro PUSH_AND_CLEAR_ALL
+	push %r15
+	push %r14
+	push %r13
+	push %r12
+	push %r11
+	push %r10
+	push %r9
+	push %r8
+	push %rdi
+	push %rsi
+	push %rbp
+	push %rsp
+	push %rbx
+	push %rdx
+	push %rcx
+	push %rax
+
+	xor %rax, %rax
+	xor %rcx, %rcx
+	xor %rdx, %rdx
+	xor %rbx, %rbx
+	xor %rbp, %rbp
+	xor %rsi, %rsi
+	xor %rdi, %rdi
+	xor %r8,  %r8
+	xor %r9,  %r9
+	xor %r10, %r10
+	xor %r11, %r11
+	xor %r12, %r12
+	xor %r13, %r13
+	xor %r14, %r14
+	xor %r15, %r15
+.endm
+
+.macro POP_ALL
+	pop %rax
+	pop %rcx
+	pop %rdx
+	pop %rbx
+	pop %r8
+	pop %rbp
+	pop %rsi
+	pop %rdi
+	pop %r8
+	pop %r9
+	pop %r10
+	pop %r11
+	pop %r12
+	pop %r13
+	pop %r14
+	pop %r15
+.endm
+
+	.text
+
+SYM_CODE_START(vmx_depriv_switch_to_root_mode)
+	/*
+	 * switch to iret trampoline stack, which is mapped even in user space CR3
+	 */
+	mov 1*8(%rsp), %rsp
+
+	PUSH_AND_CLEAR_ALL
+
+	call vmx_repriv_cpu_state
+	cmpb $1, %al
+
+	POP_ALL
+
+	je asm_depriv_exit
+	vmresume
+
+	/*
+	 * VM resume failed, forced to switch to root mode with guest stack
+	 */
+	jmp asm_depriv_exit
+SYM_CODE_END(vmx_depriv_switch_to_root_mode)
+
+SYM_FUNC_START(asm_vmx_depriv)
+	cmpb $0, %_ASM_ARG1B
+	je 1f
+
+	/* assuming vmlaunch will succeed */
+	xor %rax, %rax
+	/* Enter non-root mode */
+	vmlaunch
+
+	/* vmlaunch failed, switch to root mode stask */
+	xor %rax, %rax
+	mov $1, %rax
+	ret
+
+1:	/* assuming vmresume will succeed */
+	xor %rax, %rax
+	/* Enter non-root mode */
+	vmresume
+
+	/* vmresume failed, switch to root mode stask */
+	xor %rax, %rax
+	mov $3, %rax
+	ret
+SYM_FUNC_END(asm_vmx_depriv)
+
+/*
+ * vmlaunch succeeded
+ */
+SYM_FUNC_START(vmx_depriv_rip)
+	/* to instruction immediately after asm_vmx_depriv */
+	ret
+SYM_FUNC_END(vmx_depriv_rip)
+
+SYM_FUNC_START(vmx_depriv_vmexit)
+	PUSH_AND_CLEAR_ALL
+
+	mov %rsp, %_ASM_ARG1
+	call vmx_depriv_vmexit_handler
+	cmpb $1, %al
+
+	POP_ALL
+
+	/*
+	 * upon vmx_depriv_vmexit_handler returning false, switch back to root
+	 * mode with guest stack
+	 */
+	jne vmx_depriv_switch_to_root_mode
+
+	/*
+	 * upon vmx_depriv_vmexit_handler returning true, continue non-root mode
+	 */
+	vmresume
+
+	/*
+	 * VM resume failed, switch back to root mode with guest stack
+	 */
+	jmp vmx_depriv_switch_to_root_mode
+SYM_FUNC_END(vmx_depriv_vmexit)
+
+
+.section .text, "ax"
+
+/**
+ * vmread_error_trampoline - Trampoline from inline asm to vmread_error()
+ * @field:	VMCS field encoding that failed
+ * @fault:	%true if the VMREAD faulted, %false if it failed
+
+ * Save and restore volatile registers across a call to vmread_error().  Note,
+ * all parameters are passed on the stack.
+ */
+SYM_FUNC_START(vmread_error_trampoline)
+	push %_ASM_BP
+	mov  %_ASM_SP, %_ASM_BP
+
+	push %_ASM_AX
+	push %_ASM_CX
+	push %_ASM_DX
+	push %rdi
+	push %rsi
+	push %r8
+	push %r9
+	push %r10
+	push %r11
+	/* Load @field and @fault to arg1 and arg2 respectively. */
+	mov 3*WORD_SIZE(%rbp), %_ASM_ARG2
+	mov 2*WORD_SIZE(%rbp), %_ASM_ARG1
+
+	call vmread_error
+
+	/* Zero out @fault, which will be popped into the result register. */
+	_ASM_MOV $0, 3*WORD_SIZE(%_ASM_BP)
+
+	pop %r11
+	pop %r10
+	pop %r9
+	pop %r8
+	pop %rsi
+	pop %rdi
+	pop %_ASM_DX
+	pop %_ASM_CX
+	pop %_ASM_AX
+	pop %_ASM_BP
+
+	ret
+SYM_FUNC_END(vmread_error_trampoline)
diff --git a/arch/x86/depriv/vmx/handler.c b/arch/x86/depriv/vmx/handler.c
new file mode 100644
index 000000000000..0e3207ad591a
--- /dev/null
+++ b/arch/x86/depriv/vmx/handler.c
@@ -0,0 +1,399 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * non-essential VM-exit handlers
+ *
+ * Authors:
+ * 	Xin Li <fantry@gmail.com>
+ */
+
+#include <linux/thread_info.h>
+
+#include <asm/insn.h>
+#include <asm/smp.h>
+#include <asm/string.h>
+#include <asm/x86_vcpu_regs.h>
+
+#include "vmx.h"
+
+/*
+ * if non-root mode rip is a user level virtual address then it's mostly not
+ * valid in root mode, because it is mapped using non-root mode cr3 and page
+ * tables.
+ */
+void dump_guest_insn(unsigned long rip, int insn_len, char *insn)
+{
+	int i;
+
+	// don't try to access user level virtual address
+	if (!(rip & 0xf000000000000000ul)) {
+		memset(insn, 0, MAX_INSN_SIZE * 3 + 1);
+		return;
+	}
+
+	if (insn_len == 0)
+		insn_len = MAX_INSN_SIZE;
+
+	for (i = 0; i < insn_len; i++)
+		sprintf(insn + 3 * i, " %02x", *(u8 *)(rip + i));
+	insn[3 * i] = '\0';
+}
+
+extern struct cpumask cpu_depriv_mode_mask;
+
+void dump_va_page_table_entry(unsigned long va)
+{
+	int cpu = raw_smp_processor_id(), i;
+	unsigned long host_cr3;
+	unsigned long guest_cr3;
+	unsigned long *host_pte;
+	unsigned long *guest_pte;
+	unsigned int va_shift = PGDIR_SHIFT;
+	bool host_pte_valid = true;
+	bool guest_pte_valid = true;
+	bool guest_sharing_host_pte = false;
+
+	pr_info("depriv: cpu%d depriv mode mask: %*pb[l]\n",
+		cpu, cpumask_pr_args(&cpu_depriv_mode_mask));
+
+	if (cpumask_test_cpu(cpu, &cpu_depriv_mode_mask)) {
+		host_cr3 = read_cr3_pa();
+		guest_cr3 = host_cr3;
+	} else {
+		host_cr3 = vmcs_readl(HOST_CR3);
+		guest_cr3 = vmcs_readl(GUEST_CR3);
+	}
+
+	pr_info("depriv: cpu%d  host cr3: 0x%016lx\n", cpu, host_cr3);
+	pr_info("depriv: cpu%d guest cr3: 0x%016lx\n", cpu, guest_cr3);
+
+	host_pte = (unsigned long *)__va(host_cr3 & CR3_ADDR_MASK);
+	guest_pte = (unsigned long *)__va(guest_cr3 & CR3_ADDR_MASK);
+
+	for (i = 4; i > 0; i--) {
+		u32 slot = (u32)(va >> va_shift) & 0x1fful;
+		va_shift -= 9;
+
+		if (host_pte_valid)
+			pr_info("depriv: cpu%d  host p%dd pte[%03d]: 0x%016lx\n",
+				cpu, i, slot, host_pte[slot]);
+
+		if (!guest_sharing_host_pte && guest_pte_valid)
+			pr_info("depriv: cpu%d guest p%dd pte[%03d]: 0x%016lx\n",
+				cpu, i, slot, guest_pte[slot]);
+
+		if (host_pte_valid && !guest_sharing_host_pte && guest_pte_valid) {
+			if (host_pte[slot] != guest_pte[slot])
+				pr_err("depriv: cpu%d host/guest mismatch pte at level %d\n",
+				       cpu, i);
+			else {
+				pr_info("depriv: cpu%d guest shares host p%dd\n",
+					cpu, i - 1);
+				guest_sharing_host_pte = true;
+			}
+		}
+
+		if (host_pte_valid && host_pte[slot] & 0x1)
+			host_pte = __va(host_pte[slot] & CR3_ADDR_MASK);
+		else {
+			pr_err("depriv: cpu%d  host pte missing at level %d\n", cpu, i);
+			host_pte_valid = false;
+		}
+
+		if (!guest_sharing_host_pte) {
+			if (guest_pte_valid && guest_pte[slot] & 0x1)
+				guest_pte = __va(guest_pte[slot] & CR3_ADDR_MASK);
+			else {
+				pr_err("depriv: cpu%d guest pte missing at level %d\n", cpu, i);
+				guest_pte_valid = false;
+			}
+		}
+
+		if (!host_pte_valid && (guest_sharing_host_pte || !guest_pte_valid))
+			break;
+	}
+}
+
+static void handle_cr3_access(unsigned long *regs, unsigned long cnt)
+{
+	int cpu = raw_smp_processor_id();
+	unsigned long qualification = vmcs_readl(EXIT_QUALIFICATION);
+	int reg = (qualification >> 8) & 0xf;
+
+	switch ((qualification >> 4) & 3) {
+	case 0: { /* mov to cr */
+		unsigned long cr3 = regs[reg];
+		unsigned long cr4 = vmcs_readl(GUEST_CR4);
+
+		if (!(cr4 & X86_CR4_PCIDE))
+			pr_debug("depriv: cpu%d (%ld) PCID disabled\n", cpu, cnt);
+		else
+			pr_debug("depriv: cpu%d (%ld) PCID enabled\n", cpu, cnt);
+
+		cr3 &= ~X86_CR3_PCID_NOFLUSH;
+		vmcs_writel(GUEST_CR3, cr3);
+
+		if (cnt % 300)
+			break;
+
+		pr_info("depriv: cpu%d (%ld) wrote cr3 from reg%d: %#lx (%#lx)\n",
+			cpu, cnt, reg, cr3, regs[reg]);
+		break;
+	}
+	case 1: /*mov from cr*/
+		regs[reg] = vmcs_readl(GUEST_CR3);
+
+		/* XXX RSP in regs won't be loaded into non-root mode */
+		if (reg == __VCPU_REGS_RSP)
+			vmcs_writel(GUEST_RSP, regs[reg]);
+
+		if (cnt % 300)
+			break;
+
+		pr_info("depriv: cpu%d (%ld) read cr3 to reg%d: %#lx\n",
+			cpu, cnt, reg, regs[reg]);
+		break;
+	}
+}
+
+static void handle_cr4_access(unsigned long *regs, unsigned long cnt)
+{
+	int cpu = raw_smp_processor_id();
+	unsigned long qualification = vmcs_readl(EXIT_QUALIFICATION);
+	int reg = (qualification >> 8) & 0xf;
+
+	switch ((qualification >> 4) & 3) {
+	case 0: { /* mov to cr */
+		unsigned long val = regs[reg];
+		unsigned long cr4 = vmcs_readl(CR4_READ_SHADOW);
+
+		vmcs_writel(CR4_READ_SHADOW, val);
+
+		if ((val ^ cr4) & X86_CR4_SMEP) {
+			if (val & X86_CR4_SMEP)
+				pr_info("depriv: cpu%d (%ld) setting SMEP\n", cpu, cnt);
+			else {
+				pr_err("depriv: cpu%d (%ld) clearing SMEP\n", cpu, cnt);
+				val |= X86_CR4_SMEP;
+			}
+		}
+
+		vmcs_writel(GUEST_CR4, val);
+
+		pr_info("depriv: cpu%d (%ld) wrote cr4 from reg%d: %#lx (%#lx)\n",
+			cpu, cnt, reg, val, regs[reg]);
+		break;
+	}
+	case 1: /*mov from cr*/
+		regs[reg] = vmcs_readl(CR4_READ_SHADOW);
+
+		/* XXX RSP in regs won't be loaded into non-root mode */
+		if (reg == __VCPU_REGS_RSP)
+			vmcs_writel(GUEST_RSP, regs[reg]);
+
+		pr_info("depriv: cpu%d (%ld) read cr4 to reg%d: %#lx\n",
+			cpu, cnt, reg, regs[reg]);
+		break;
+	}
+}
+
+static int handle_exception_nmi(unsigned long *regs, unsigned long cnt)
+{
+	int cpu = raw_smp_processor_id();
+	unsigned long rip = vmcs_readl(GUEST_RIP);
+	unsigned long rsp = vmcs_readl(GUEST_RSP);
+	char insn[64];
+	u32 insn_len = 0;
+	bool continue_in_root_mode = true;
+	u32 intr_info = vmcs_read32(VM_EXIT_INTR_INFO);
+	u8 vector = intr_info & INTR_INFO_VECTOR_MASK;
+	u32 error_code = 0, intr;
+
+	if (intr_info & INTR_INFO_DELIVER_CODE_MASK)
+		error_code = vmcs_read32(VM_EXIT_INTR_ERROR_CODE);
+
+	insn_len = vmcs_read32(VM_EXIT_INSTRUCTION_LEN);
+	dump_guest_insn(rip, insn_len, insn);
+
+	if (is_page_fault(intr_info)) {
+		unsigned long cr2 = vmcs_readl(EXIT_QUALIFICATION);
+		if (!(cnt % 100))
+			pr_info("depriv: cpu%d (%ld) page fault @ %#lx with error code %#x "
+				"rip: %#lx rsp: %#lx insn: %s\n",
+				cpu, cnt, cr2, error_code, rip, rsp, insn);
+		native_write_cr2(cr2);
+		if (false)
+			dump_va_page_table_entry(cr2);
+		continue_in_root_mode = false;
+	} else if (is_gp_fault(intr_info))
+		continue_in_root_mode = false;
+	else if (is_machine_check(intr_info))
+		pr_info("depriv: cpu%d (%ld) to handle machine check in root mode\n", cpu, cnt);
+	else if (is_machine_check(intr_info) || is_nmi(intr_info))
+		pr_info("depriv: cpu%d (%ld) to handle NMI in root mode\n", cpu, cnt);
+	else
+		pr_info("depriv: cpu%d (%ld) hit exception %d @ rip %#lx with error code %#x "
+			"insn: %s\n",
+			cpu, cnt, vector, rip, error_code, insn);
+
+	if (continue_in_root_mode) {
+		pr_info("depriv: cpu%d (%ld) switch to root mode to handle exception %d\n",
+			cpu, cnt, vector);
+		return 1;
+	}
+
+	intr = vector | INTR_INFO_VALID_MASK;
+	if (vector == DF_VECTOR || vector == GP_VECTOR || vector == PF_VECTOR)
+		intr |= INTR_TYPE_HARD_EXCEPTION;
+	if (vector == GP_VECTOR || vector == PF_VECTOR)
+		intr |= INTR_INFO_DELIVER_CODE_MASK;
+	vmcs_write32(VM_ENTRY_INTR_INFO_FIELD, intr);
+	vmcs_write32(VM_ENTRY_INSTRUCTION_LEN, insn_len);
+	vmcs_write32(VM_ENTRY_EXCEPTION_ERROR_CODE, error_code);
+
+	if (!(cnt % 100) && vector == PF_VECTOR) {
+		pr_info("depriv: cpu%d (%ld) injecting exception %d\n", cpu, cnt, vector);
+		DEPRIV_DUMP_GPRS(regs);
+	}
+
+	return 0;
+}
+
+static int handle_cr_access(unsigned long *regs, unsigned long cnt)
+{
+	int cpu = raw_smp_processor_id();
+	unsigned long qualification = vmcs_readl(EXIT_QUALIFICATION);
+	int cr = qualification & 0xf;
+
+	switch (cr) {
+	case 3:
+		handle_cr3_access(regs, cnt);
+		break;
+	case 4:
+		handle_cr4_access(regs, cnt);
+		break;
+	default:
+		return 1;
+		break;
+	}
+
+	pr_debug("depriv: cpu%d (%ld) accessed cr%d\n", cpu, cnt, cr);
+	return 0;
+}
+
+static int handle_msr_read(unsigned long *regs, unsigned long cnt)
+{
+	int cpu = raw_smp_processor_id();
+	u32 ecx = (u32)regs[__VCPU_REGS_RCX];
+	unsigned long val;
+
+	if (ecx == MSR_FS_BASE) {
+		val = vmcs_readl(GUEST_FS_BASE);
+		pr_info("depriv: cpu%d (%ld) FS base MSR: %#lx\n", cpu, cnt, val);
+	} else if (ecx == MSR_GS_BASE) {
+		/*
+		 * never read GS base MSR directly when running in root mode,
+		 * which now points to kernel mode per-CPU data.
+		 */
+		val = vmcs_readl(GUEST_GS_BASE);
+		pr_info("depriv: cpu%d (%ld) GS base MSR: %#lx\n", cpu, cnt, val);
+	} else if (rdmsrl_safe(ecx, (unsigned long long *)&val))
+		pr_info("depriv: cpu%d (%ld) MSR[%#x]: %#lx failed\n", cpu, cnt, ecx, val);
+	else
+		pr_info("depriv: cpu%d (%ld) MSR[%#x]: %#lx\n", cpu, cnt, ecx, val);
+
+	*((u32 *)&regs[__VCPU_REGS_RAX]) = (u32)val;
+	*((u32 *)&regs[__VCPU_REGS_RDX]) = (u32)(val >> 32);
+
+	pr_info("depriv: cpu%d (%ld) executed rdmsr\n", cpu, cnt);
+	return 0;
+}
+
+static int handle_msr_write(unsigned long *regs, unsigned long cnt)
+{
+	int cpu = raw_smp_processor_id();
+	bool continue_in_root_mode = false;
+	u32 ecx = (u32)regs[__VCPU_REGS_RCX];
+	unsigned long val = (unsigned long)(u32)regs[__VCPU_REGS_RAX] |
+			    ((unsigned long)(u32)regs[__VCPU_REGS_RDX] << 32);
+
+	if (ecx == MSR_GS_BASE) {
+		/*
+		 * never write GS base MSR directly when running in root mode,
+		 * which now points to kernel mode per-CPU data.
+		 */
+		pr_info("depriv: cpu%d (%ld) GS base MSR = %#lx\n", cpu, cnt, val);
+	} else if (wrmsrl_safe(ecx, val))
+		continue_in_root_mode = true;
+
+	switch (ecx) {
+	case MSR_IA32_SPEC_CTRL:
+		pr_debug("depriv: cpu%d (%ld) speculation control MSR = %#lx\n", cpu, cnt, val);
+		break;
+	case MSR_IA32_PRED_CMD:
+		pr_debug("depriv: cpu%d (%ld) prediction command MSR = %#lx\n", cpu, cnt, val);
+		break;
+	case MSR_FS_BASE:
+		pr_debug("depriv: cpu%d (%ld) FS base MSR = %#lx\n", cpu, cnt, val);
+		/*
+		 * guest FS base needs to be syned up with MSR_FS_BASE, thus we will
+		 * have correct FS base value in non-root mode after all future VM-entries.
+		 */
+		vmcs_writel(GUEST_FS_BASE, val);
+		/*
+		 * host FS base doesn't need to be syned up with non-root MSR_FS_BASE,
+		 * because we never use FS base in root mode.
+		 */
+		break;
+	case MSR_GS_BASE:
+		pr_info("depriv: cpu%d (%ld) GS base MSR = %#lx\n", cpu, cnt, val);
+		/*
+		 * guest GS base needs to be syned up with MSR_GS_BASE, thus we will
+		 * have correct GS base value in non-root mode after all future VM-entries.
+		 */
+		vmcs_writel(GUEST_GS_BASE, val);
+		/*
+		 * host GS base should NEVER be syned up with non-root MSR_GS_BASE,
+		 * because root mode MSR_GS_BASE, loaded from HOST_GS_BASE upon VM-exit,
+		 * points to kernel mode per-CPU data, and is used in root mode.
+		 */
+		break;
+	case MSR_KERNEL_GS_BASE:
+		pr_info("depriv: cpu%d (%ld) kernel GS base MSR = %#lx\n", cpu, cnt, val);
+		break;
+	case MSR_IA32_TSC_DEADLINE:
+		pr_debug("depriv: cpu%d (%ld) TSC deadline timer MSR = %#lx\n", cpu, cnt, val);
+		break;
+	case 0x80b: // EOI virtualization MSR
+		pr_debug("depriv: cpu%d (%ld) EOI MSR = %#lx\n", cpu, cnt, val);
+		break;
+	default:
+		pr_info("depriv: cpu%d (%ld) MSR[%#x] = %#lx\n", cpu, cnt, ecx, val);
+		break;
+	}
+
+	if (unlikely(continue_in_root_mode)) {
+		pr_info("depriv: cpu%d (%ld) MSR[%#x] = %#lx failed, continue in root mode\n",
+			cpu, cnt, ecx, val);
+		return 1;
+	} else {
+		pr_debug("depriv: cpu%d (%ld) executed wrmsr\n", cpu, cnt);
+		return 0;
+	}
+}
+
+/*
+ * The exit handlers return 0 if the exit was handled fully and non-root execution
+ * may resume.  Otherwise they return none zero values to indicate the exit handler
+ * caller to continue in root mode.
+ */
+static int (*vmx_depriv_exit_handlers[])(unsigned long *regs, unsigned long cnt) = {
+	[EXIT_REASON_EXCEPTION_NMI]           = handle_exception_nmi,
+	[EXIT_REASON_CR_ACCESS]               = handle_cr_access,
+	[EXIT_REASON_MSR_READ]                = handle_msr_read,
+	[EXIT_REASON_MSR_WRITE]               = handle_msr_write,
+};
+
+int vmx_depriv_handle(u32 reason, unsigned long *regs, unsigned long cnt)
+{
+	return vmx_depriv_exit_handlers[reason](regs, cnt);
+}
diff --git a/arch/x86/depriv/vmx/validator.c b/arch/x86/depriv/vmx/validator.c
new file mode 100644
index 000000000000..8594b7d3ecad
--- /dev/null
+++ b/arch/x86/depriv/vmx/validator.c
@@ -0,0 +1,483 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Deprivilege is to run Linux kernel in VMX non-root mode
+ *
+ * Authors:
+ * 	Xin Li <fantry@gmail.com>
+ */
+
+#include <asm/insn.h>
+#include <asm/msr.h>
+#include <asm/processor-flags.h>
+#include <asm/segment.h>
+
+#include "vmx.h"
+
+#define check(_c) do {							\
+	if (!(_c))							\
+		pr_err("depriv: invalid guest state (%d): %s\n",	\
+		       __LINE__, #_c);					\
+} while (0)
+
+#define CHECK_VMX_CTLS(ctls, val)					\
+	check((~(ctls) & ((u32)(val))) == 0 &&				\
+	      ((ctls) & ~((u32)((val) >> 32))) == 0)
+
+#define CHECK_HOST_SEG(seg)						\
+	check((vmcs_read16(HOST_##seg##_SELECTOR) & 0x7) == 0)
+
+static inline u64 get_canonical(u64 la, u8 vaddr_bits)
+{
+	return ((int64_t)la << (64 - vaddr_bits)) >> (64 - vaddr_bits);
+}
+
+static inline bool is_canonical_address(u64 la, u64 cr4)
+{
+	return get_canonical(la, cr4 & X86_CR4_LA57 ? 57 : 48) == la;
+}
+
+#define CHECK_IS_ADDR_CANONICAL(a, gh)					\
+	check(is_canonical_address(a, vmcs_readl(gh##_CR4)))
+
+#define CHECK_IS_HOST_ADDR_CANONICAL(a)					\
+	CHECK_IS_ADDR_CANONICAL(a, HOST)
+
+#define CHECK_IS_GUEST_ADDR_CANONICAL(a)				\
+	CHECK_IS_ADDR_CANONICAL(a, GUEST)
+
+#define CHECK_IS_VMCS_FIELD_ADDR_CANONICAL(vmcs_field, gh)		\
+	CHECK_IS_ADDR_CANONICAL(vmcs_readl(vmcs_field), gh)
+
+#define CHECK_IS_HOST_VMCS_FIELD_ADDR_CANONICAL(addr)			\
+	CHECK_IS_VMCS_FIELD_ADDR_CANONICAL(HOST_##addr, HOST)
+
+#define CHECK_IS_GUEST_VMCS_FIELD_ADDR_CANONICAL(addr)			\
+	CHECK_IS_VMCS_FIELD_ADDR_CANONICAL(GUEST_##addr, GUEST)
+
+#define CHECK_IS_HOST_TABLE_BASE_CANONICAL(tab)				\
+	CHECK_IS_HOST_VMCS_FIELD_ADDR_CANONICAL(tab##_BASE)
+
+#define CHECK_IS_GUEST_TABLE_BASE_CANONICAL(tab)			\
+	CHECK_IS_GUEST_VMCS_FIELD_ADDR_CANONICAL(tab##_BASE)
+
+#define PAGE_M (PAGE_SIZE - 1)
+
+enum {
+	VCPU_SREG_ES,
+	VCPU_SREG_CS,
+	VCPU_SREG_SS,
+	VCPU_SREG_DS,
+	VCPU_SREG_FS,
+	VCPU_SREG_GS,
+	VCPU_SREG_TR,
+	VCPU_SREG_LDTR,
+};
+
+static u32 vmx_segment_sel(u8 seg)
+{
+	switch (seg) {
+	case VCPU_SREG_ES:
+		return GUEST_ES_SELECTOR;
+	case VCPU_SREG_CS:
+		return GUEST_CS_SELECTOR;
+	case VCPU_SREG_SS:
+		return GUEST_SS_SELECTOR;
+	case VCPU_SREG_DS:
+		return GUEST_DS_SELECTOR;
+	case VCPU_SREG_FS:
+		return GUEST_FS_SELECTOR;
+	case VCPU_SREG_GS:
+		return GUEST_GS_SELECTOR;
+	case VCPU_SREG_TR:
+		return GUEST_TR_SELECTOR;
+	case VCPU_SREG_LDTR:
+		return GUEST_LDTR_SELECTOR;
+	default:
+		return GUEST_ES_SELECTOR;
+	}
+}
+
+static void vmx_validate_guest_selector(u8 seg, bool vm86_active,
+					bool long_mode_active, bool unrestricted)
+{
+	u32 sel = vmx_segment_sel(seg);
+	u16 selector = vmcs_read16(sel);
+	unsigned long base = vmcs_readl(sel + GUEST_ES_BASE - GUEST_ES_SELECTOR);
+	u32 limit = vmcs_read32(sel + GUEST_ES_LIMIT - GUEST_ES_SELECTOR);
+	u32 ar = vmcs_read32(sel + GUEST_ES_AR_BYTES - GUEST_ES_SELECTOR);
+	u8 rpl = selector & SEGMENT_RPL_MASK;
+	u8 ti = selector & SEGMENT_TI_MASK;
+	u8 type = ar & VMX_AR_TYPE_MASK;
+	u8 dpl = VMX_AR_DPL(ar);
+
+	if (ar & VMX_AR_UNUSABLE_MASK)
+		return;
+
+	pr_debug("depriv: seg%d sel=0x%04x, attr=0x%05x, limit=0x%08x, base=0x%016lx\n",
+		 seg, selector, ar, limit, base);
+
+	check(ar & VMX_AR_P_MASK);
+
+	if (seg == VCPU_SREG_SS)
+		check(vm86_active || unrestricted ||
+		      rpl == (vmcs_read16(GUEST_CS_SELECTOR) & SEGMENT_RPL_MASK));
+	if (seg < VCPU_SREG_TR && vm86_active)
+		check(base == (unsigned long)(selector << 4));
+	if (seg == VCPU_SREG_TR || seg == VCPU_SREG_FS || seg == VCPU_SREG_GS)
+		CHECK_IS_GUEST_ADDR_CANONICAL(base);
+	else if (seg == VCPU_SREG_LDTR)
+		check(is_canonical_address(base, vmcs_readl(GUEST_CR4)));
+	else if (seg == VCPU_SREG_CS)
+		check(!((u32)(base >> 32)));
+	else
+		check(!((u32)(base >> 32)));
+
+	if (seg < VCPU_SREG_TR && vm86_active)
+		check(limit == 0xffff);
+
+	if (seg == VCPU_SREG_LDTR) {
+		check(ti == 0);
+		check(type == VMX_AR_TYPE_READABLE_MASK);
+		check(!(ar & VMX_AR_S_MASK));
+		check((ar & VMX_AR_RESERVD_MASK) == 0);
+		check((limit & PAGE_M) == PAGE_M || !(ar & VMX_AR_G_MASK));
+		check(limit >> 20 == 0 || ar & VMX_AR_G_MASK);
+		return;
+	}
+
+	if (seg == VCPU_SREG_TR) {
+		check(ti == 0);
+		check((!long_mode_active &&
+		       type == (VMX_AR_TYPE_ACCESSES_MASK | VMX_AR_TYPE_READABLE_MASK)) ||
+		      type == (VMX_AR_TYPE_ACCESSES_MASK | VMX_AR_TYPE_READABLE_MASK |
+			       VMX_AR_TYPE_CODE_MASK));
+		check(!(ar & VMX_AR_S_MASK));
+		check((ar & VMX_AR_RESERVD_MASK) == 0);
+		check((limit & PAGE_M) == PAGE_M || !(ar & VMX_AR_G_MASK));
+		check(limit >> 20 == 0 || ar & VMX_AR_G_MASK);
+		return;
+	}
+
+	if (vm86_active) {
+		check(ar == (VMX_AR_TYPE_ACCESSES_MASK |
+			     VMX_AR_TYPE_READABLE_MASK |
+			     VMX_AR_S_MASK |
+			     (3 << VMX_AR_DPL_SHIFT) |
+			     VMX_AR_P_MASK));
+		return;
+	}
+
+	// Type
+	if (seg == VCPU_SREG_CS)
+		check((unrestricted &&
+		       type == (VMX_AR_TYPE_ACCESSES_MASK | VMX_AR_TYPE_READABLE_MASK)) ||
+		      (type & VMX_AR_TYPE_ACCESSES_MASK && type & VMX_AR_TYPE_CODE_MASK));
+	else if (seg == VCPU_SREG_SS)
+		check(type == (VMX_AR_TYPE_ACCESSES_MASK | VMX_AR_TYPE_READABLE_MASK) ||
+		      type == (VMX_AR_TYPE_ACCESSES_MASK | VMX_AR_TYPE_READABLE_MASK |
+			       VMX_AR_TYPE_WRITEABLE_MASK));
+	else
+		check(type & VMX_AR_TYPE_ACCESSES_MASK &&
+		      (!(type & VMX_AR_TYPE_CODE_MASK) || type & VMX_AR_TYPE_READABLE_MASK));
+	// S
+	check(ar & VMX_AR_S_MASK);
+	// DPL
+	if (seg == VCPU_SREG_CS)
+		if (type == (VMX_AR_TYPE_ACCESSES_MASK | VMX_AR_TYPE_READABLE_MASK))
+			check(dpl == 0 && unrestricted);
+		else if (!(type & VMX_AR_TYPE_WRITEABLE_MASK))
+			check(dpl == VMX_AR_DPL(vmcs_read32(GUEST_SS_AR_BYTES)));
+		else
+			check(dpl <= VMX_AR_DPL(vmcs_read32(GUEST_SS_AR_BYTES)));
+	else if (seg == VCPU_SREG_SS)
+		if (!(vmcs_readl(GUEST_CR0) & X86_CR0_PE) ||
+		    (vmcs_read32(GUEST_CS_AR_BYTES) & VMX_AR_TYPE_MASK) ==
+		    (VMX_AR_TYPE_ACCESSES_MASK | VMX_AR_TYPE_READABLE_MASK))
+			check(dpl == 0);
+		else
+			check(dpl == rpl);
+	else if (!unrestricted && type <= VMX_AR_TYPE_BUSY_64_TSS)
+		check(dpl >= rpl);
+	// reserved bits
+	check((ar & VMX_AR_RESERVD_MASK) == 0);
+	// D/B
+	if (seg == VCPU_SREG_CS)
+		check(!long_mode_active || !(ar & VMX_AR_L_MASK) || !(ar & VMX_AR_DB_MASK));
+	// G
+	check((limit & PAGE_M) == PAGE_M || !(ar & VMX_AR_G_MASK));
+	check(limit >> 20 == 0 || ar & VMX_AR_G_MASK);
+}
+
+void vmx_validate_vmcs(void)
+{
+	u32 vmentry_ctl = vmcs_read32(VM_ENTRY_CONTROLS);
+	u32 vmexit_ctl = vmcs_read32(VM_EXIT_CONTROLS);
+	u32 cpu_based_exec_ctrl = vmcs_read32(CPU_BASED_VM_EXEC_CONTROL);
+	u32 pin_based_exec_ctrl = vmcs_read32(PIN_BASED_VM_EXEC_CONTROL);
+	u32 secondary_exec_control = cpu_has_secondary_exec_ctrls() ?
+		vmcs_read32(SECONDARY_VM_EXEC_CONTROL) : 0;
+	u64 rflags = vmcs_readl(GUEST_RFLAGS);
+	bool vm86_active = rflags & X86_EFLAGS_VM;
+	bool long_mode_active = vmentry_ctl & VM_ENTRY_IA32E_MODE;
+	u32 vmentry_intr_info = vmcs_read32(VM_ENTRY_INTR_INFO_FIELD);
+	bool unrestricted = cpu_has_secondary_exec_ctrls() ?
+		secondary_exec_control & SECONDARY_EXEC_UNRESTRICTED_GUEST :
+		false;
+
+	u64 cr0_must_be_zeros = ~read_msr(MSR_IA32_VMX_CR0_FIXED1);
+	u64 host_cr0_must_be_ones = read_msr(MSR_IA32_VMX_CR0_FIXED0);
+	u64 guest_cr0_must_be_ones = host_cr0_must_be_ones &
+		~(unrestricted ? X86_CR0_PG | X86_CR0_PE : 0);
+	u64 cr4_must_be_zeros = ~read_msr(MSR_IA32_VMX_CR4_FIXED1);
+	u64 cr4_must_be_ones = read_msr(MSR_IA32_VMX_CR4_FIXED0);
+
+	u64 pin_based_ctrls = read_msr(MSR_IA32_VMX_TRUE_PINBASED_CTLS);
+	u64 proc_based_ctrls = read_msr(MSR_IA32_VMX_TRUE_PROCBASED_CTLS);
+
+	u64 debug_ctrl = vmcs_read64(GUEST_IA32_DEBUGCTL);
+	u32 activity_state = vmcs_read32(GUEST_ACTIVITY_STATE);
+	u32 interruptibility = vmcs_read32(GUEST_INTERRUPTIBILITY_INFO);
+	u64 pending_dbg_exceptions = vmcs_readl(GUEST_PENDING_DBG_EXCEPTIONS);
+
+	u8 seg;
+
+	// 26.2.1.1: VM-Execution Control Fields
+	CHECK_VMX_CTLS(pin_based_exec_ctrl, pin_based_ctrls);
+	CHECK_VMX_CTLS(cpu_based_exec_ctrl, proc_based_ctrls);
+
+	if (cpu_based_exec_ctrl & CPU_BASED_ACTIVATE_SECONDARY_CONTROLS) {
+		u64 secondary_ctrls = read_msr(MSR_IA32_VMX_PROCBASED_CTLS2);
+		CHECK_VMX_CTLS(secondary_exec_control, secondary_ctrls);
+	}
+
+	check(vmcs_read32(CR3_TARGET_COUNT) == 0);
+
+	check(!(cpu_based_exec_ctrl & CPU_BASED_USE_IO_BITMAPS));
+	check(!(cpu_based_exec_ctrl & CPU_BASED_TPR_SHADOW));
+	check(pin_based_exec_ctrl & PIN_BASED_NMI_EXITING ||
+	      !(pin_based_exec_ctrl & PIN_BASED_VIRTUAL_NMIS));
+	check(pin_based_exec_ctrl & PIN_BASED_VIRTUAL_NMIS ||
+	      !(cpu_based_exec_ctrl & CPU_BASED_NMI_WINDOW_EXITING));
+	check(!(pin_based_exec_ctrl & PIN_BASED_POSTED_INTR));
+
+	// 26.2.1.2: VM-Exit Control Fields
+	CHECK_VMX_CTLS(vmexit_ctl, read_msr(MSR_IA32_VMX_TRUE_EXIT_CTLS));
+	check(pin_based_exec_ctrl & PIN_BASED_VMX_PREEMPTION_TIMER ||
+	      !(vmexit_ctl & VM_EXIT_SAVE_VMX_PREEMPTION_TIMER));
+
+	// 26.2.1.3: VM-Entry Control Fields
+	CHECK_VMX_CTLS(vmentry_ctl, read_msr(MSR_IA32_VMX_TRUE_ENTRY_CTLS));
+	if (vmentry_intr_info & INTR_INFO_VALID_MASK) {
+		u32 type = vmentry_intr_info & INTR_INFO_INTR_TYPE_MASK;
+		u8 vector = vmentry_intr_info & INTR_INFO_VECTOR_MASK;
+		u32 insn_len = vmcs_read32(VM_ENTRY_INSTRUCTION_LEN);
+		bool has_error_code = vmentry_intr_info & INTR_INFO_DELIVER_CODE_MASK;
+
+		check(type != INTR_TYPE_RESERVED);
+		check((u32)(proc_based_ctrls >> 32) & CPU_BASED_MONITOR_TRAP_FLAG ||
+		      type != INTR_TYPE_OTHER_EVENT);
+		check(type != INTR_TYPE_NMI_INTR || vector == BP_VECTOR);
+		check(type != INTR_TYPE_HARD_EXCEPTION || vector <= 31);
+		check(type != INTR_TYPE_OTHER_EVENT || vector == DE_VECTOR);
+
+		check(has_error_code == (vmcs_readl(GUEST_CR0) & X86_CR0_PE &&
+					 (type == INTR_TYPE_HARD_EXCEPTION &&
+					  (vector == DF_VECTOR ||
+					   vector == TS_VECTOR ||
+					   vector == NP_VECTOR ||
+					   vector == SS_VECTOR ||
+					   vector == GP_VECTOR ||
+					   vector == PF_VECTOR ||
+					   vector == AC_VECTOR))));
+
+		check(!(vmentry_intr_info &
+			(INTR_INFO_RESVD_BITS_MASK | INTR_INFO_UNBLOCK_NMI)));
+
+		check(!has_error_code ||
+		      (vmcs_read32(VM_ENTRY_EXCEPTION_ERROR_CODE) & 0xffff8000) == 0);
+		check(!(type == INTR_TYPE_SOFT_INTR ||
+			type == INTR_TYPE_PRIV_SW_EXCEPTION ||
+			type == INTR_TYPE_SOFT_EXCEPTION) ||
+		      insn_len < MAX_INSN_SIZE);
+	}
+
+	check(!(vmentry_ctl & VM_ENTRY_SMM));
+	check(!(vmentry_ctl & VM_ENTRY_DEACT_DUAL_MONITOR));
+
+	// 26.2.2: Checks on Host Control Registers and MSRs
+	check((~vmcs_readl(HOST_CR0) & cr0_must_be_zeros) == cr0_must_be_zeros);
+	check((vmcs_readl(HOST_CR0) & host_cr0_must_be_ones) == host_cr0_must_be_ones);
+
+	check((~vmcs_readl(HOST_CR4) & cr4_must_be_zeros) == cr4_must_be_zeros);
+	check((vmcs_readl(HOST_CR4) & cr4_must_be_ones) == cr4_must_be_ones);
+
+	CHECK_IS_HOST_VMCS_FIELD_ADDR_CANONICAL(IA32_SYSENTER_ESP);
+	CHECK_IS_HOST_VMCS_FIELD_ADDR_CANONICAL(IA32_SYSENTER_EIP);
+
+	if (vmexit_ctl & VM_EXIT_LOAD_IA32_PAT) {
+		u64 pat = vmcs_read64(HOST_IA32_PAT);
+		unsigned i;
+		for (i = 0; i < 8; i++) {
+			u8 byte = pat & 0xff;
+			check(byte < 8 && byte != 2 && byte != 3);
+			pat >>= 8;
+		}
+	}
+
+	if (vmexit_ctl & VM_EXIT_LOAD_IA32_EFER) {
+		u64 efer = vmcs_read64(HOST_IA32_EFER);
+		check((efer & 0xffffffffffff0200ull) == 0);
+		if (vmexit_ctl & VM_EXIT_HOST_ADDR_SPACE_SIZE)
+			check((efer & (EFER_LMA | EFER_LME)) ==
+			      (EFER_LMA | EFER_LME));
+		else
+			check((efer & (EFER_LMA | EFER_LME)) == 0);
+	}
+
+	// 26.2.3: Checks on Host Segment and Descriptor-Table Registers
+	CHECK_HOST_SEG(ES);
+	CHECK_HOST_SEG(CS);
+	CHECK_HOST_SEG(SS);
+	CHECK_HOST_SEG(DS);
+	CHECK_HOST_SEG(FS);
+	CHECK_HOST_SEG(GS);
+	CHECK_HOST_SEG(TR);
+
+	check(vmcs_read16(HOST_CS_SELECTOR) != 0);
+	check(vmcs_read16(HOST_TR_SELECTOR) != 0);
+
+	CHECK_IS_HOST_TABLE_BASE_CANONICAL(FS);
+	CHECK_IS_HOST_TABLE_BASE_CANONICAL(GS);
+	CHECK_IS_HOST_TABLE_BASE_CANONICAL(GDTR);
+	CHECK_IS_HOST_TABLE_BASE_CANONICAL(IDTR);
+	CHECK_IS_HOST_TABLE_BASE_CANONICAL(TR);
+
+	// 26.2.4: Checks Related to Address-Space Size
+	check(vmexit_ctl & VM_EXIT_HOST_ADDR_SPACE_SIZE);
+
+	check(vmcs_readl(HOST_CR4) & X86_CR4_PAE);
+	CHECK_IS_HOST_VMCS_FIELD_ADDR_CANONICAL(RIP);
+
+	// 26.3.1.1: Checks on Guest Control Registers, Debug Registers, and MSRs
+	check((~vmcs_readl(GUEST_CR0) & cr0_must_be_zeros) ==
+	      cr0_must_be_zeros);
+	check((vmcs_readl(GUEST_CR0) & guest_cr0_must_be_ones) ==
+	      guest_cr0_must_be_ones);
+
+	check((~vmcs_readl(GUEST_CR4) & cr4_must_be_zeros) == cr4_must_be_zeros);
+	check((vmcs_readl(GUEST_CR4) & cr4_must_be_ones) == cr4_must_be_ones);
+
+	if (vmentry_ctl & VM_ENTRY_LOAD_DEBUG_CONTROLS) {
+		u64 debug_ctrl_reserved = 0xffffffffffff003cull;
+		check((debug_ctrl & debug_ctrl_reserved) == 0);
+	}
+
+	check(!long_mode_active || vmcs_readl(GUEST_CR4) & X86_CR4_PAE);
+
+	if (vmentry_ctl & VM_ENTRY_LOAD_DEBUG_CONTROLS)
+		check(!((u32)(vmcs_readl(GUEST_DR7) >> 32)));
+
+	CHECK_IS_GUEST_VMCS_FIELD_ADDR_CANONICAL(SYSENTER_ESP);
+	CHECK_IS_GUEST_VMCS_FIELD_ADDR_CANONICAL(SYSENTER_EIP);
+
+	if (vmentry_ctl & VM_ENTRY_LOAD_IA32_PAT) {
+		u64 pat = vmcs_read64(GUEST_IA32_PAT);
+		unsigned i;
+		for (i = 0; i < 8; i++) {
+			u8 byte = pat & 0xff;
+			check(byte < 8 && byte != 2 && byte != 3);
+			pat >>= 8;
+		}
+	}
+
+	if (vmentry_ctl & VM_ENTRY_LOAD_IA32_EFER) {
+		u64 efer = vmcs_read64(GUEST_IA32_EFER);
+		check((efer & 0xffffffffffff0200ull) == 0);
+		if ((vmcs_readl(GUEST_CR0) & X86_CR0_PG) != 0) {
+			if (long_mode_active)
+				check((efer & (EFER_LMA | EFER_LME)) ==
+				      (EFER_LMA | EFER_LME));
+			else
+				check((efer & (EFER_LMA | EFER_LME)) == 0);
+		}
+	}
+
+	if (vmentry_ctl & VM_ENTRY_LOAD_BNDCFGS) {
+		u64 bndcfgs = vmcs_read64(GUEST_BNDCFGS);
+		CHECK_IS_GUEST_ADDR_CANONICAL(bndcfgs & 0xfffffffffffff000ull);
+		check((bndcfgs & 0x00000ffc) == 0);
+	}
+
+	// 26.3.1.2: Checks on Guest Segment Registers
+	for (seg = VCPU_SREG_ES; seg <= VCPU_SREG_LDTR; seg++)
+		vmx_validate_guest_selector(seg, vm86_active, long_mode_active, unrestricted);
+
+	// 26.3.1.3: Checks on Guest Descriptor-Table Registers
+	CHECK_IS_GUEST_TABLE_BASE_CANONICAL(GDTR);
+	CHECK_IS_GUEST_TABLE_BASE_CANONICAL(IDTR);
+
+	// 26.3.1.4: Checks on Guest RIP and RFLAGS
+	check((long_mode_active && vmcs_read32(GUEST_CS_AR_BYTES) & VMX_AR_L_MASK) ||
+	      (u32)(vmcs_readl(GUEST_RIP) >> 32) == 0);
+	CHECK_IS_GUEST_VMCS_FIELD_ADDR_CANONICAL(RIP);
+
+	check((rflags & ((-1ull << 22) | (1 << 15) | (1 << 5) | (1 << 3))) == 0);
+	check((rflags & X86_EFLAGS_FIXED) != 0);
+	check(!long_mode_active || !vm86_active);
+	check(((vmentry_intr_info & (INTR_INFO_VALID_MASK |
+				     INTR_INFO_INTR_TYPE_MASK)) !=
+	       (INTR_INFO_VALID_MASK | INTR_TYPE_EXT_INTR)) ||
+	      (rflags & X86_EFLAGS_IF));
+
+	// 26.3.1.5: Checks on Guest Non-Register State
+	check(activity_state <= GUEST_ACTIVITY_WAIT_SIPI);
+	check(activity_state != GUEST_ACTIVITY_HLT ||
+	      VMX_AR_DPL(vmcs_read32(GUEST_SS_AR_BYTES)) == 0);
+	check(!(interruptibility & (GUEST_INTR_STATE_STI | GUEST_INTR_STATE_MOV_SS)) ||
+	      activity_state == GUEST_ACTIVITY_ACTIVE);
+	if (vmentry_intr_info & INTR_INFO_VALID_MASK) {
+		u32 type = vmentry_intr_info & INTR_INFO_INTR_TYPE_MASK;
+		u8 vector = vmentry_intr_info & INTR_INFO_VECTOR_MASK;
+
+		check(activity_state != GUEST_ACTIVITY_HLT ||
+		      (type == INTR_TYPE_EXT_INTR ||
+		       type == INTR_TYPE_NMI_INTR ||
+		       (type == INTR_TYPE_HARD_EXCEPTION &&
+			(vector == DB_VECTOR || vector == MC_VECTOR)) ||
+		       (type == INTR_TYPE_OTHER_EVENT && vector == DB_VECTOR)));
+		check(activity_state != GUEST_ACTIVITY_SHUTDOWN ||
+		      (type == INTR_TYPE_NMI_INTR ||
+		       (type == INTR_TYPE_HARD_EXCEPTION && vector == MC_VECTOR)));
+		check(activity_state != GUEST_ACTIVITY_WAIT_SIPI);
+	}
+
+	check((interruptibility & 0xFFFFFFE0) == 0);
+	check((interruptibility & (GUEST_INTR_STATE_STI | GUEST_INTR_STATE_MOV_SS)) !=
+	      (GUEST_INTR_STATE_STI | GUEST_INTR_STATE_MOV_SS));
+	check(rflags & X86_EFLAGS_IF || !(interruptibility & GUEST_INTR_STATE_STI));
+	check(((vmentry_intr_info & (INTR_INFO_VALID_MASK | INTR_INFO_INTR_TYPE_MASK)) !=
+	       (INTR_INFO_VALID_MASK | INTR_TYPE_EXT_INTR)) ||
+	      (interruptibility & (GUEST_INTR_STATE_STI | GUEST_INTR_STATE_MOV_SS)) == 0);
+	check(((vmentry_intr_info & (INTR_INFO_VALID_MASK | INTR_INFO_INTR_TYPE_MASK)) !=
+	       (INTR_INFO_VALID_MASK | INTR_TYPE_NMI_INTR)) ||
+	      (interruptibility & GUEST_INTR_STATE_MOV_SS) == 0);
+	check((interruptibility & GUEST_INTR_STATE_SMI) == 0);
+	// Some processors require the following check; others do not.
+	check(((vmentry_intr_info & (INTR_INFO_VALID_MASK | INTR_INFO_INTR_TYPE_MASK)) !=
+	       (INTR_INFO_VALID_MASK | INTR_TYPE_NMI_INTR)) ||
+	      (interruptibility & GUEST_INTR_STATE_STI) == 0);
+	check(((vmentry_intr_info & (INTR_INFO_VALID_MASK | INTR_INFO_INTR_TYPE_MASK)) !=
+	       (INTR_INFO_VALID_MASK | INTR_TYPE_NMI_INTR)) ||
+	      (pin_based_exec_ctrl & PIN_BASED_VIRTUAL_NMIS) == 0 ||
+	      (interruptibility & GUEST_INTR_STATE_NMI) == 0);
+
+	check((u32)(pending_dbg_exceptions >> 32) == 0);
+	check((pending_dbg_exceptions & 0xfffeaff0) == 0);
+	if ((interruptibility & (GUEST_INTR_STATE_STI | GUEST_INTR_STATE_MOV_SS)) != 0 ||
+	    activity_state == GUEST_ACTIVITY_HLT) {
+		check(!((rflags & X86_EFLAGS_TF) && !(debug_ctrl & 0x2)) ||
+		      (pending_dbg_exceptions & 0x00004000));
+		check(!(!(rflags & X86_EFLAGS_TF) || (debug_ctrl & 0x2)) ||
+		      !(pending_dbg_exceptions & 0x00004000));
+	}
+}
diff --git a/arch/x86/depriv/vmx/vmx.c b/arch/x86/depriv/vmx/vmx.c
new file mode 100644
index 000000000000..d9562e54e288
--- /dev/null
+++ b/arch/x86/depriv/vmx/vmx.c
@@ -0,0 +1,986 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Deprivilege is to run Linux kernel in VMX non-root mode
+ *
+ * Authors:
+ * 	Xin Li <fantry@gmail.com>
+ */
+
+#include <linux/mm.h>
+#include <linux/module.h>
+#include <linux/percpu.h>
+
+#include <asm/debugreg.h>
+#include <asm/desc.h>
+#include <asm/msr.h>
+#include <asm/perf_event.h>
+#include <asm/tlbflush.h>
+#include <asm/mtrr.h>
+
+#include "vmx.h"
+
+#define PT_PRESENT_MASK	(1ULL << 0)
+#define SPPT_ALLOC_ORDER	16
+
+static unsigned int __read_mostly debug_host_in_non_root_mode = 0;
+module_param(debug_host_in_non_root_mode, uint, 0444);
+
+static unsigned int __read_mostly exception_bitmap = 0;
+module_param(exception_bitmap, uint, 0444);
+
+static bool __read_mostly intercept_cr3 = 0;
+module_param(intercept_cr3, bool, S_IRUGO);
+
+struct vmcs_config depriv_vmcs_config;
+
+static inline bool cpu_has_load_perf_global_ctrl(void)
+{
+	return (depriv_vmcs_config.vmentry_ctrl & VM_ENTRY_LOAD_IA32_PERF_GLOBAL_CTRL) &&
+	       (depriv_vmcs_config.vmexit_ctrl & VM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL);
+}
+
+#if 0
+static inline bool cpu_has_vmx_ept_2m_page(void)
+{
+	return depriv_vmcs_config.vmx_cap_ept & VMX_EPT_2MB_PAGE_BIT;
+}
+#endif
+
+static inline bool cpu_has_vmx_ept_1g_page(void)
+{
+	return depriv_vmcs_config.vmx_cap_ept & VMX_EPT_1GB_PAGE_BIT;
+}
+
+inline struct vmcs *alloc_vmcs(void)
+{
+	int cpu = raw_smp_processor_id(), i;
+	struct page *pages;
+	struct vmcs *vmcs = NULL;
+	int alloc_order = depriv_vmcs_config.order - 9;
+	void *ept;
+	u64 *epte;
+	int nr_ept_pages = 2;
+	void *sppt;
+	u64 *sppte;
+	int nr_sppt_pages = (1 << alloc_order) - 1 - nr_ept_pages;
+
+	pages = __alloc_pages_node(cpu_to_node(cpu), GFP_ATOMIC, alloc_order);
+	if (!pages)
+		goto exit;
+
+	vmcs = page_address(pages);
+	memset(vmcs, 0, PAGE_SIZE << alloc_order);
+	vmcs->hdr.revision_id = depriv_vmcs_config.revision_id;
+
+	/* set up nGB physical address space address translation */
+	epte = (void *)vmcs + PAGE_SIZE; // epte points to ept l4e
+	ept = (void *)epte + PAGE_SIZE; // ept points to ept l3 page
+	epte[0] = __pa(ept) | VMX_EPT_RWX_MASK;
+	--nr_ept_pages;
+	pr_debug("depriv: cpu%d ept l4e[0]=%#llx\n", cpu, epte[0]);
+#if 0
+	// upto 512GB physical memory, instead of 1TB
+	if (cpu_has_vmx_ept_1g_page()) {
+		epte[1] = (__pa(ept) + PAGE_SIZE) | VMX_EPT_RWX_MASK;
+		pr_debug("depriv: ept l4e[1]=%#llx\n",  epte[1]);
+	}
+#endif
+
+	epte = ept; // epte points to ept l3e
+	if (cpu_has_vmx_ept_1g_page()) {
+		for (i = 0; i < 512 * nr_ept_pages; i++) {
+			epte[i] = PUD_PAGE_SIZE * i |
+				  (1ull << 7) |
+				  (MTRR_TYPE_WRBACK << VMX_EPT_MT_EPTE_SHIFT);
+			epte[i] |= cpu_has_vmx_spp() ?
+				   (1ull << 61) | VMX_EPT_READABLE_MASK | VMX_EPT_EXECUTABLE_MASK :
+				   VMX_EPT_RWX_MASK;
+			pr_debug("depriv: cpu%d ept l3e[%4d]=%#llx\n", cpu, i, epte[i]);
+		}
+#if 0
+	} else if (cpu_has_vmx_ept_2m_page()) {
+		ept += PAGE_SIZE; // ept points to pmd
+		--nr_ept_pages;
+		for (i = 0; i < nr_ept_pages; i++) {
+			epte[i] = (__pa(ept) + PAGE_SIZE * i) | VMX_EPT_RWX_MASK;
+			pr_debug("depriv: ept pud[%2d]=%#llx\n", i, epte[i]);
+		}
+
+		epte = ept; // epte points to pmd
+		for (i = 0; i < 512 * nr_ept_pages; i++) {
+			epte[i] = PMD_PAGE_SIZE * i |
+				  (1ull << 7) |
+				  (MTRR_TYPE_WRBACK << VMX_EPT_MT_EPTE_SHIFT) |
+				  VMX_EPT_RWX_MASK;
+			pr_debug("depriv: ept pmd[%4d]=%#llx\n", i, epte[i]);
+		}
+#endif
+	}
+
+	sppte = ept + PAGE_SIZE; // sppte points to sppt l4e
+	sppt = (void *)sppte + PAGE_SIZE; // sppt points to sppt l3 page
+	sppte[0] = __pa(sppt) | PT_PRESENT_MASK;
+	--nr_sppt_pages;
+	pr_debug("depriv: cpu%d sppt l4e[0]=%#llx\n", cpu, sppte[0]);
+
+	sppte = sppt; // sppte points to sppt l3e
+	sppt += PAGE_SIZE; // sppt points to sppt l2 pages
+	--nr_sppt_pages;
+	for (i = 0; i < nr_sppt_pages - 1; i++) { // up to N GB physical memory
+		sppte[i] = __pa(sppt) | PT_PRESENT_MASK;
+		sppt += PAGE_SIZE;
+		pr_debug("depriv: cpu%d sppt l3e[%3d]=%#llx\n", cpu, i, sppte[i]);
+	}
+
+	pr_info("depriv: cpu%d sppt l1 page %#lx\n", cpu, __pa(sppt));
+
+	sppte = (void *)sppte + PAGE_SIZE; // sppte points to sppt l2e, and sppt to l1 page
+	for (i = 0; i < PTRS_PER_PMD * (nr_sppt_pages - 1); i++) {
+		sppte[i] = __pa(sppt) | PT_PRESENT_MASK;
+		pr_debug("depriv: cpu%d sppt l2e[%5d]=%#llx\n", cpu, i, sppte[i]);
+	}
+
+	sppte = sppt; // sppte points to sppt l1e
+	for (i = 0; i < PAGE_SIZE / 8; i++) {
+		sppte[i] = 0x5555555555555555ull;
+		pr_debug("depriv: cpu%d sppt l1e[%3d]=%#llx\n", cpu, i, sppte[i]);
+	}
+
+exit:
+	return vmcs;
+}
+
+inline void free_vmcs(struct vmcs *vmcs)
+{
+	free_pages((unsigned long)vmcs, depriv_vmcs_config.order - 9);
+}
+
+static __init int adjust_vmx_controls(u32 ctl_min, u32 ctl_opt,
+				      u32 msr, u32 *result)
+{
+	u32 vmx_msr_low, vmx_msr_high;
+	u32 ctl = ctl_min | ctl_opt;
+
+	rdmsr(msr, vmx_msr_low, vmx_msr_high);
+	pr_debug("depriv: msr[%#08x] = %#08x:%#08x\n", msr, vmx_msr_high, vmx_msr_low);
+
+	ctl &= vmx_msr_high; /* bit == 0 in high word ==> must be zero */
+	ctl |= vmx_msr_low;  /* bit == 1 in low word  ==> must be one  */
+
+	/* Ensure minimum (required) set of control bits are supported. */
+	if (ctl_min & ~ctl)
+		return -EIO;
+
+	*result = ctl;
+	return 0;
+}
+
+int __init setup_vmcs_config(void)
+{
+	u32 min, opt, min2, opt2;
+	u32 _pin_based_exec_control = 0;
+	u32 _cpu_based_exec_control = 0;
+	u32 _cpu_based_2nd_exec_control = 0;
+	u32 _vmexit_control = 0;
+	u32 _vmentry_control = 0;
+	u32 vmx_msr_low = 0, vmx_msr_high = 0;
+
+	memset(&depriv_vmcs_config, 0, sizeof(depriv_vmcs_config));
+	min = 0;
+	opt = CPU_BASED_USE_MSR_BITMAPS |
+	      CPU_BASED_ACTIVATE_SECONDARY_CONTROLS;
+	if (adjust_vmx_controls(min, opt, MSR_IA32_VMX_PROCBASED_CTLS,
+				&_cpu_based_exec_control) < 0)
+		return -EIO;
+
+	if (_cpu_based_exec_control & CPU_BASED_ACTIVATE_SECONDARY_CONTROLS) {
+		min2 = 0;
+		opt2 = SECONDARY_EXEC_ENABLE_RDTSCP |
+		       SECONDARY_EXEC_ENABLE_EPT |
+		       SECONDARY_EXEC_ENABLE_INVPCID |
+		       SECONDARY_EXEC_XSAVES |
+		       SECONDARY_EXEC_ENABLE_SPP;
+		if (adjust_vmx_controls(min2, opt2,
+					MSR_IA32_VMX_PROCBASED_CTLS2,
+					&_cpu_based_2nd_exec_control) < 0)
+			return -EIO;
+	}
+
+	if (_cpu_based_exec_control & CPU_BASED_CR3_LOAD_EXITING) {
+		if (intercept_cr3)
+			pr_info("depriv: load cr3 causes VM exits\n");
+		else {
+			_cpu_based_exec_control &= ~CPU_BASED_CR3_LOAD_EXITING;
+			pr_debug("depriv: disabled cr3 load exiting\n");
+		}
+	}
+
+	if (_cpu_based_exec_control & CPU_BASED_CR3_STORE_EXITING) {
+		if (intercept_cr3)
+			pr_info("depriv: store cr3 causes VM exits\n");
+		else {
+			_cpu_based_exec_control &= ~CPU_BASED_CR3_STORE_EXITING;
+			pr_debug("depriv: disabled cr3 store exiting\n");
+		}
+	}
+
+	if (_cpu_based_exec_control & CPU_BASED_INVLPG_EXITING)
+		pr_info("depriv: invlpg causes VM exits\n");
+
+	min = VM_EXIT_SAVE_DEBUG_CONTROLS |
+	      VM_EXIT_HOST_ADDR_SPACE_SIZE;
+	opt = VM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL |
+	      VM_EXIT_LOAD_IA32_PAT |
+	      VM_EXIT_LOAD_IA32_EFER;
+	if (adjust_vmx_controls(min, opt, MSR_IA32_VMX_EXIT_CTLS,
+				&_vmexit_control) < 0)
+		return -EIO;
+
+	min = 0;
+	opt = 0;
+	if (adjust_vmx_controls(min, opt, MSR_IA32_VMX_PINBASED_CTLS,
+				&_pin_based_exec_control) < 0)
+		return -EIO;
+
+	if (_cpu_based_2nd_exec_control & SECONDARY_EXEC_ENABLE_EPT) {
+		rdmsr_safe(MSR_IA32_VMX_EPT_VPID_CAP,
+			   &depriv_vmcs_config.vmx_cap_ept, &depriv_vmcs_config.vmx_cap_vpid);
+
+		/* CR3 accesses and invlpg don't need to cause VM Exits when EPT
+		   enabled */
+		_cpu_based_exec_control &= ~(CPU_BASED_CR3_LOAD_EXITING |
+					     CPU_BASED_CR3_STORE_EXITING |
+					     CPU_BASED_INVLPG_EXITING);
+		pr_info("depriv: disabled cr3 load/store exiting\n");
+	}
+
+	if (!(_cpu_based_2nd_exec_control &
+		SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY))
+		_pin_based_exec_control &= ~PIN_BASED_POSTED_INTR;
+
+	min = VM_ENTRY_LOAD_DEBUG_CONTROLS |
+	      VM_ENTRY_IA32E_MODE;
+	opt = VM_ENTRY_LOAD_IA32_PERF_GLOBAL_CTRL |
+	      VM_ENTRY_LOAD_IA32_PAT |
+	      VM_ENTRY_LOAD_IA32_EFER;
+	if (adjust_vmx_controls(min, opt, MSR_IA32_VMX_ENTRY_CTLS,
+				&_vmentry_control) < 0)
+		return -EIO;
+
+	rdmsr(MSR_IA32_VMX_BASIC, vmx_msr_low, vmx_msr_high);
+
+	/* IA-32 SDM Vol 3B: VMCS size is never greater than 4kB. */
+	if ((vmx_msr_high & 0x1fff) > PAGE_SIZE)
+		return -EIO;
+
+	/* IA-32 SDM Vol 3B: 64-bit CPUs always have VMX_BASIC_MSR[48]==0. */
+	if (vmx_msr_high & (1u<<16))
+		return -EIO;
+
+	/* Require Write-Back (WB) memory type for VMCS accesses. */
+	if (((vmx_msr_high >> 18) & 15) != 6)
+		return -EIO;
+
+	depriv_vmcs_config.size = vmx_msr_high & 0x1fff;
+	depriv_vmcs_config.order = get_order(depriv_vmcs_config.size);
+
+	/*
+	 * Should be get_order(depriv_vmcs_config.size), which is always 0 according to
+	 * Intel SDM as of today: IA-32 SDM Vol 3B: VMCS size is never greater than 4kB.
+	 *
+	 * However we need some extra pages for EPTP;
+	 */
+	if (cpu_has_vmx_ept_1g_page())
+		depriv_vmcs_config.order = SPPT_ALLOC_ORDER;
+#if 0
+	else if (cpu_has_vmx_ept_2m_page()) // when running on VM, i.e., nested.
+		depriv_vmcs_config.order = 6;
+#endif
+	else // No super page no EPT
+		_cpu_based_2nd_exec_control &= ~SECONDARY_EXEC_ENABLE_EPT;
+
+	depriv_vmcs_config.basic_cap = vmx_msr_high & ~0x1fff;
+	depriv_vmcs_config.revision_id = vmx_msr_low;
+
+	depriv_vmcs_config.pin_based_exec_ctrl = _pin_based_exec_control;
+	depriv_vmcs_config.cpu_based_exec_ctrl = _cpu_based_exec_control;
+	depriv_vmcs_config.cpu_based_2nd_exec_ctrl = _cpu_based_2nd_exec_control;
+	depriv_vmcs_config.vmexit_ctrl         = _vmexit_control;
+	depriv_vmcs_config.vmentry_ctrl        = _vmentry_control;
+
+	pr_info("depriv: VMCS size: %d (size order %d)\n",
+		depriv_vmcs_config.size, depriv_vmcs_config.order);
+	pr_info("depriv: pin based controls: %#x\n",
+		depriv_vmcs_config.pin_based_exec_ctrl);
+	pr_info("depriv: processor based controls: %#x\n",
+		depriv_vmcs_config.cpu_based_exec_ctrl);
+	pr_info("depriv: processor based 2nd controls: %#x\n",
+		depriv_vmcs_config.cpu_based_2nd_exec_ctrl);
+	pr_info("depriv: vm exit controls: %#x\n",
+		depriv_vmcs_config.vmexit_ctrl);
+	pr_info("depriv: vm entry controls: %#x\n",
+		depriv_vmcs_config.vmentry_ctrl);
+
+	return 0;
+}
+
+#define vmx_insn_failed(fmt...)		\
+do {					\
+	WARN_ONCE(1, fmt);		\
+	pr_warn_ratelimited(fmt);	\
+} while (0)
+
+asmlinkage void vmread_error(unsigned long field, bool fault)
+{
+	if (fault)
+		BUG();
+	else
+		vmx_insn_failed("depriv: vmread failed: field=%lx\n", field);
+}
+
+noinline void vmwrite_error(unsigned long field, unsigned long value)
+{
+	vmx_insn_failed("depriv: vmwrite failed: field=%lx val=%lx err=%d\n",
+			field, value, vmcs_read32(VM_INSTRUCTION_ERROR));
+}
+
+noinline void vmclear_error(struct vmcs *vmcs, u64 phys_addr)
+{
+	vmx_insn_failed("depriv: vmclear failed: %p/%llx\n", vmcs, phys_addr);
+}
+
+noinline void vmptrld_error(struct vmcs *vmcs, u64 phys_addr)
+{
+	vmx_insn_failed("depriv: vmptrld failed: %p/%llx\n", vmcs, phys_addr);
+}
+
+noinline void invvpid_error(unsigned long ext, u16 vpid, gva_t gva)
+{
+	vmx_insn_failed("depriv: invvpid failed: ext=0x%lx vpid=%u gva=0x%lx\n",
+			ext, vpid, gva);
+}
+
+noinline void invept_error(unsigned long ext, u64 eptp, gpa_t gpa)
+{
+	vmx_insn_failed("depriv: invept failed: ext=0x%lx eptp=%llx gpa=0x%llx\n",
+			ext, eptp, gpa);
+}
+
+static void vmx_depriv_cpu_controls(void)
+{
+	vmcs_write32(PIN_BASED_VM_EXEC_CONTROL,
+		     depriv_vmcs_config.pin_based_exec_ctrl);
+	vmcs_write32(CPU_BASED_VM_EXEC_CONTROL,
+		     depriv_vmcs_config.cpu_based_exec_ctrl);
+
+	if (cpu_has_secondary_exec_ctrls()) {
+		vmcs_write32(SECONDARY_VM_EXEC_CONTROL,
+		     depriv_vmcs_config.cpu_based_2nd_exec_ctrl);
+	}
+
+	vmcs_write32(VM_EXIT_CONTROLS,
+		     depriv_vmcs_config.vmexit_ctrl |
+		     VM_EXIT_HOST_ADDR_SPACE_SIZE);
+	vmcs_write32(VM_ENTRY_CONTROLS,
+		     depriv_vmcs_config.vmentry_ctrl);
+
+	vmcs_write32(EXCEPTION_BITMAP, exception_bitmap);
+	vmcs_write32(PAGE_FAULT_ERROR_CODE_MASK, 0);
+	vmcs_write32(PAGE_FAULT_ERROR_CODE_MATCH, 0);
+	vmcs_write32(CR3_TARGET_COUNT, 0);
+
+	vmcs_write32(VM_EXIT_MSR_STORE_COUNT, 0);
+	vmcs_write64(VM_EXIT_MSR_STORE_ADDR, 0);
+	vmcs_write32(VM_EXIT_MSR_LOAD_COUNT, 0);
+	vmcs_write64(VM_EXIT_MSR_LOAD_ADDR, 0);
+	vmcs_write32(VM_ENTRY_MSR_LOAD_COUNT, 0);
+	vmcs_write64(VM_ENTRY_MSR_LOAD_ADDR, 0);
+}
+
+#define DEPRIV_CR4_NON_ROOT_OWNED_BITS				      \
+	(X86_CR4_PVI | X86_CR4_DE | X86_CR4_PCE | X86_CR4_OSFXSR      \
+	 | X86_CR4_OSXMMEXCPT | X86_CR4_LA57 | X86_CR4_TSD)
+
+static void vmx_depriv_cpu_crs(void)
+{
+	unsigned long cr0, cr4;
+	u64 pat, efer;
+
+	cr0 = read_cr0();
+	vmcs_writel(HOST_CR0, cr0);
+	vmcs_writel(CR0_READ_SHADOW, cr0);
+	vmcs_writel(GUEST_CR0, cr0);
+
+	vmcs_writel(GUEST_CR3, __read_cr3());
+
+	cr4 = __read_cr4();
+	vmcs_writel(HOST_CR4, cr4);
+	vmcs_writel(CR4_READ_SHADOW, cr4);
+	vmcs_writel(GUEST_CR4, cr4);
+
+	vmcs_writel(CR0_GUEST_HOST_MASK, ~X86_CR0_TS);
+	vmcs_writel(CR4_GUEST_HOST_MASK, ~DEPRIV_CR4_NON_ROOT_OWNED_BITS);
+
+	pat = read_msr(MSR_IA32_CR_PAT);
+	vmcs_write64(HOST_IA32_PAT, pat);
+	vmcs_write64(GUEST_IA32_PAT, pat);
+
+	efer = read_msr(MSR_EFER);
+	vmcs_write64(HOST_IA32_EFER, efer);
+	vmcs_write64(GUEST_IA32_EFER, efer);
+
+	if (cpu_has_load_perf_global_ctrl()) {
+		u64 perf_global_ctrl;
+		rdmsrl_safe(MSR_CORE_PERF_GLOBAL_CTRL, &perf_global_ctrl);
+		vmcs_write64(GUEST_IA32_PERF_GLOBAL_CTRL, perf_global_ctrl);
+		vmcs_write64(HOST_IA32_PERF_GLOBAL_CTRL, perf_global_ctrl);
+	}
+}
+
+static inline bool is_desc_16byte(struct desc_struct *dentry)
+{
+	// s = 0 : system descriptor
+	return dentry->p && !dentry->s;
+}
+
+static inline u32 get_desc_limit_in_byte(struct desc_struct *dentry)
+{
+	u32 limit = get_desc_limit(dentry);
+	if (dentry->g)
+		limit = (limit << PAGE_SHIFT) | (PAGE_SIZE - 1);
+	return limit;
+}
+
+static inline void dump_desc_entry(struct desc_struct *dentry)
+{
+	int cpu = raw_smp_processor_id();
+	bool is_16byte = is_desc_16byte(dentry);
+	u16 *entry = (u16 *)dentry;
+	u32 limit = get_desc_limit_in_byte(dentry);
+	unsigned long base = get_desc_base(dentry);
+
+	if (is_16byte) {
+		pr_info("depriv: cpu%d %04x %04x %04x %04x %04x %04x %04x %04x\n",
+			cpu, entry[0], entry[1], entry[2], entry[3],
+			entry[4], entry[5], entry[6], entry[7]);
+		base += (u64)(*((u32 *)(dentry + 1))) << 32;
+	} else {
+		pr_info("depriv: cpu%d %04x %04x %04x %04x\n",
+			cpu, entry[0], entry[1], entry[2], entry[3]);
+	}
+
+	pr_info("depriv: cpu%d type %x, S %x, DPL %x, P %x, AVL %x, "
+		"L %x, D %x, G %x, limit %#x, base %#lx\n",
+		cpu, dentry->type, dentry->s, dentry->dpl, dentry->p,
+		dentry->avl, dentry->l, dentry->d, dentry->g, limit, base);
+}
+
+static inline struct desc_struct *get_gdt_entry(unsigned long addr)
+{
+	struct desc_struct *dentry = (struct desc_struct *)addr;
+	if (false)
+		dump_desc_entry(dentry);
+	return dentry;
+}
+
+static inline u32 get_desc_ar(struct desc_struct *dentry,
+			      bool is_null, bool is_segment)
+{
+	int cpu = raw_smp_processor_id();
+	u32 unusable = is_null ? 1 : 0; // 0 = usable; 1 = unusable
+	/*
+	 * 26.3.1.2 Checks on Guest Segment Registers, AR bytes:
+	 */
+	bool s = (unusable ? dentry->s :
+			     (is_segment ? 1 : 0));
+	u32 ar = (dentry->type) |
+		 (s ? VMX_AR_S_MASK : 0) |
+		 (dentry->dpl << VMX_AR_DPL_SHIFT) |
+		 (dentry->p ? VMX_AR_P_MASK : 0) |
+		 (dentry->avl << 12) |
+		 (dentry->l ? VMX_AR_L_MASK : 0) |
+		 (dentry->d ? VMX_AR_DB_MASK : 0) |
+		 (dentry->g ? VMX_AR_G_MASK : 0) |
+		 (unusable ? VMX_AR_UNUSABLE_MASK : 0);
+	pr_debug("depriv: cpu%d entry ar %#x\n", cpu, ar);
+	return ar;
+}
+
+#define DEPRIV_SELECTOR(name, sel) {						\
+	pr_debug("depriv: cpu%d " #name " %#x\n", cpu, sel);			\
+	dentry = get_gdt_entry(gdt_base + sel);					\
+	base = get_desc_base(dentry);						\
+	if (is_desc_16byte(dentry))						\
+		base += (u64)(*((u32 *)(dentry + 1))) << 32;			\
+	vmcs_write16(GUEST_##name##_SELECTOR, sel);				\
+	vmcs_writel(GUEST_##name##_BASE, base);					\
+	vmcs_write32(GUEST_##name##_LIMIT, get_desc_limit_in_byte(dentry));	\
+	vmcs_write32(GUEST_##name##_AR_BYTES,					\
+		     get_desc_ar(dentry, sel == 0, is_segment));		\
+}
+
+#define DEPRIV_SEGMENT(SEG) {							\
+	u16 seg;								\
+	savesegment(SEG, seg);							\
+	vmcs_write16(HOST_##SEG##_SELECTOR, seg);				\
+	DEPRIV_SELECTOR(SEG, seg);						\
+}
+
+static void vmx_depriv_cpu_ldtr(unsigned long gdt_base)
+{
+	int cpu = raw_smp_processor_id();
+	struct desc_struct *dentry;
+	unsigned long base;
+	u16 ldtr;
+	bool is_segment = false;
+
+	store_ldt(ldtr);
+	DEPRIV_SELECTOR(LDTR, ldtr);
+}
+
+static void vmx_depriv_cpu_tr(unsigned long gdt_base)
+{
+	int cpu = raw_smp_processor_id();
+	struct desc_struct *dentry;
+	unsigned long base, tss_base;
+	u16 tr;
+	u32 ar;
+	bool is_segment = false;
+
+	store_tr(tr);
+	if (tr != GDT_ENTRY_TSS*8)
+		pr_err("depriv: cpu%d tr selector mismatch %#x : %#x\n",
+		       cpu, tr, GDT_ENTRY_TSS*8);
+	vmcs_write16(HOST_TR_SELECTOR, tr);
+	DEPRIV_SELECTOR(TR, tr);
+	vmcs_writel(HOST_TR_BASE, base);
+	tss_base = (unsigned long)&get_cpu_entry_area(cpu)->tss.x86_tss;
+	if (base != tss_base)
+		pr_err("depriv: cpu%d tr base mismatch %#lx : %#lx\n",
+		       cpu, base, tss_base);
+
+	ar = vmcs_read32(GUEST_TR_AR_BYTES);
+	if ((ar & VMX_AR_TYPE_MASK) != VMX_AR_TYPE_BUSY_64_TSS) {
+		pr_err("depriv: cpu%d tr ar %#x, fix it up\n", cpu, ar);
+		vmcs_write32(GUEST_TR_AR_BYTES,
+			     (ar & ~VMX_AR_TYPE_MASK) | VMX_AR_TYPE_BUSY_64_TSS);
+	}
+}
+
+void vmx_depriv_cpu_fsgs(void)
+{
+	int cpu = raw_smp_processor_id();
+	unsigned long gdt_base = vmcs_readl(HOST_GDTR_BASE);
+	struct desc_struct *dentry;
+	unsigned long base;
+	bool is_segment = true;
+
+	DEPRIV_SEGMENT(FS);
+	DEPRIV_SEGMENT(GS);
+
+	base = read_msr(MSR_FS_BASE);
+	pr_debug("depriv: cpu%d FS base MSR %#lx\n", cpu, base);
+	vmcs_writel(HOST_FS_BASE, base);
+	vmcs_writel(GUEST_FS_BASE, base);
+
+	base = read_msr(MSR_GS_BASE);
+	pr_debug("depriv: cpu%d GS base MSR %#lx\n", cpu, base);
+	vmcs_writel(HOST_GS_BASE, base);
+	vmcs_writel(GUEST_GS_BASE, base);
+}
+
+static void vmx_depriv_cpu_segments(void)
+{
+	int cpu = raw_smp_processor_id();
+	unsigned long gdt_base = vmcs_readl(HOST_GDTR_BASE);
+	struct desc_struct *dentry;
+	unsigned long base;
+	bool is_segment = true;
+
+	DEPRIV_SEGMENT(CS);
+	DEPRIV_SEGMENT(DS);
+	DEPRIV_SEGMENT(ES);
+	DEPRIV_SEGMENT(SS);
+
+	vmx_depriv_cpu_fsgs();
+
+	vmx_depriv_cpu_ldtr(gdt_base);
+	vmx_depriv_cpu_tr(gdt_base);
+}
+
+static void vmx_depriv_cpu_desc_tables(void)
+{
+	int cpu = raw_smp_processor_id();
+	struct desc_ptr gdt, idt;
+	unsigned long gdt_base;
+
+	store_gdt(&gdt);
+	gdt_base = gdt.address;
+	if (gdt_base != (unsigned long)get_current_gdt_ro())
+		pr_err("depriv: cpu%d gdt base mismatch %#lx : %#lx\n",
+		       cpu, gdt_base, (unsigned long)get_current_gdt_ro());
+	vmcs_writel(HOST_GDTR_BASE, gdt_base);
+	vmcs_writel(GUEST_GDTR_BASE, gdt_base);
+	/* there is no host gdt limit */
+	vmcs_write32(GUEST_GDTR_LIMIT, gdt.size);
+
+	store_idt(&idt);
+	/* host should never handle interrupts */
+	vmcs_writel(HOST_IDTR_BASE, idt.address);
+	vmcs_writel(GUEST_IDTR_BASE, idt.address);
+	/* there is no host idt limit */
+	vmcs_write32(GUEST_IDTR_LIMIT, idt.size);
+
+	vmx_depriv_cpu_segments();
+}
+
+static void vmx_depriv_cpu_sysenter_msrs(void)
+{
+	u32 low32, high32;
+	unsigned long msr;
+
+	msr = read_msr(MSR_IA32_SYSENTER_ESP);
+	vmcs_writel(HOST_IA32_SYSENTER_ESP, msr);
+	vmcs_writel(GUEST_SYSENTER_ESP, msr);
+
+	rdmsr(MSR_IA32_SYSENTER_CS, low32, high32);
+	vmcs_write32(HOST_IA32_SYSENTER_CS, low32);
+	vmcs_write32(GUEST_SYSENTER_CS, low32);
+
+	msr = read_msr(MSR_IA32_SYSENTER_EIP);
+	vmcs_writel(HOST_IA32_SYSENTER_EIP, msr);
+	vmcs_writel(GUEST_SYSENTER_EIP, msr);
+}
+
+static void vmx_depriv_cpu_misc(void)
+{
+	unsigned long dr7;
+	u64 dbg_ctrl;
+
+	get_debugreg(dr7, 7);
+	vmcs_writel(GUEST_DR7, dr7);
+
+	dbg_ctrl = read_msr(MSR_IA32_DEBUGCTLMSR);
+	vmcs_write64(GUEST_IA32_DEBUGCTL, dbg_ctrl);
+}
+
+void vmx_depriv_vmexit(void);
+
+/*
+ * sync host states to guest states
+ */
+void vmx_depriv_cpu_state(void)
+{
+	vmx_depriv_cpu_controls();
+	vmx_depriv_cpu_crs();
+	vmx_depriv_cpu_desc_tables();
+	vmx_depriv_cpu_sysenter_msrs();
+	vmx_depriv_cpu_misc();
+
+	vmcs_writel(HOST_RIP, (unsigned long)vmx_depriv_vmexit);
+}
+
+inline void __cpu_vmxoff(void)
+{
+	if (!(__read_cr4() & X86_CR4_VMXE)) {
+		pr_err("depriv: CR4.VMXE already cleared on cpu%d\n", raw_smp_processor_id());
+		return;
+	}
+
+	asm volatile("vmxoff");
+
+	intel_pt_handle_vmx(0);
+	cr4_clear_bits(X86_CR4_VMXE);
+}
+
+inline int __cpu_vmxon(u64 vmxon_pointer)
+{
+	int cpu;
+
+	cr4_set_bits(X86_CR4_VMXE);
+	intel_pt_handle_vmx(1);
+
+	asm_volatile_goto("1: vmxon %[vmxon_pointer]\n\t"
+			  _ASM_EXTABLE(1b, %l[fault])
+			  : : [vmxon_pointer] "m"(vmxon_pointer)
+			  : : fault);
+	return 0;
+
+fault:
+	cpu = raw_smp_processor_id();
+	pr_err("depriv: cpu%d VMXON faulted\n", cpu);
+	intel_pt_handle_vmx(0);
+	cr4_clear_bits(X86_CR4_VMXE);
+	return -EFAULT;
+}
+
+static void vmx_repriv_cpu_crs(void)
+{
+	int cpu = raw_smp_processor_id();
+	unsigned long host_cr0 = read_cr0();
+	unsigned long host_cr4 = __read_cr4();
+	unsigned long cr0 = vmcs_readl(GUEST_CR0);
+	unsigned long cr4 = vmcs_readl(GUEST_CR4);
+
+	if (host_cr0 != cr0) {
+		pr_info("depriv: repriv cpu%d cr0 %#lx : %#lx : %#lx\n",
+			cpu, host_cr0, vmcs_readl(HOST_CR0), cr0);
+		write_cr0(cr0);
+		vmcs_writel(HOST_CR0, cr0);
+	}
+
+	if (host_cr4 != cr4) {
+		pr_info("depriv: repriv cpu%d cr4 %#lx : %#lx : %#lx\n",
+			cpu, host_cr4, vmcs_readl(HOST_CR4), cr4);
+		vmcs_writel(HOST_CR4, cr4);
+	}
+}
+
+static inline void vmx_repriv_cpu_sysenter_msrs(void)
+{
+	wrmsrl(MSR_IA32_SYSENTER_ESP, vmcs_readl(GUEST_SYSENTER_ESP));
+	wrmsr(MSR_IA32_SYSENTER_CS, vmcs_read32(GUEST_SYSENTER_CS), 0);
+	wrmsrl(MSR_IA32_SYSENTER_EIP, vmcs_readl(GUEST_SYSENTER_EIP));
+}
+
+static inline void vmx_repriv_cpu_misc(void)
+{
+	set_debugreg(vmcs_readl(GUEST_DR7), 7);
+	wrmsrl(MSR_IA32_DEBUGCTLMSR, vmcs_read64(GUEST_IA32_DEBUGCTL));
+}
+
+#define REPRIV_SEGMENT(tag, TAG) do {						\
+	ar = vmcs_read32(GUEST_##TAG##S_AR_BYTES);				\
+	if (ar & VMX_AR_UNUSABLE_MASK)						\
+		pr_debug("depriv: repriv cpu%d " #TAG "S unusable\n", cpu);	\
+	sel = vmcs_read16(GUEST_##TAG##S_SELECTOR);				\
+	loadsegment(tag##s, sel);						\
+	vmcs_write16(HOST_##TAG##S_SELECTOR, sel);				\
+	pr_debug("depriv: repriv cpu%d " #TAG "S %#x\n", cpu, sel);		\
+} while (0)
+
+static inline void vmx_repriv_cpu_segments(void)
+{
+	int cpu = raw_smp_processor_id();
+	unsigned long host_base, base;
+	u32 ar;
+	u16 sel;
+
+	REPRIV_SEGMENT(d, D);
+	REPRIV_SEGMENT(e, E);
+
+	ar = vmcs_read32(GUEST_FS_AR_BYTES);
+	if (ar & VMX_AR_UNUSABLE_MASK)
+		pr_debug("depriv: repriv cpu%d FS unusable\n", cpu);
+
+	sel = vmcs_read16(GUEST_FS_SELECTOR);
+	loadsegment(fs, sel);
+	pr_debug("depriv: repriv cpu%d FS selector: %#x\n", cpu, sel);
+	vmcs_write16(HOST_FS_SELECTOR, sel);
+
+	host_base = read_msr(MSR_FS_BASE);
+	base = vmcs_readl(GUEST_FS_BASE);
+	pr_debug("depriv: repriv cpu%d FS base: %#lx\n", cpu, base);
+	if (host_base != base)
+		wrmsrl(MSR_FS_BASE, base);
+	vmcs_writel(HOST_FS_BASE, base);
+
+	// never change GS BASE, which points to kernel mode per-CPU data
+	ar = vmcs_read32(GUEST_GS_AR_BYTES);
+	if (ar & VMX_AR_UNUSABLE_MASK)
+		pr_debug("depriv: repriv cpu%d GS unusable\n", cpu);
+
+	sel = vmcs_read16(GUEST_GS_SELECTOR);
+	load_gs_index(sel);
+	pr_debug("depriv: repriv cpu%d GS selector: %#x\n", cpu, sel);
+	vmcs_write16(HOST_GS_SELECTOR, sel);
+
+	base = vmcs_readl(GUEST_GS_BASE);
+	pr_debug("depriv: repriv cpu%d GS base: %#lx\n", cpu, base);
+}
+
+static inline void vmx_repriv_cpu_ldtr(void)
+{
+	int cpu = raw_smp_processor_id();
+	u16 ldtr = vmcs_read16(GUEST_LDTR_SELECTOR), host_ldtr;
+
+	store_ldt(host_ldtr);
+	if (host_ldtr != ldtr) {
+		pr_info("depriv: repriv cpu%d LDTR mismatch %#x : %#x\n",
+			cpu, host_ldtr, ldtr);
+		load_ldt(ldtr);
+	}
+}
+
+static inline void vmx_repriv_cpu_tr(void)
+{
+	int cpu = raw_smp_processor_id();
+	u16 tr = vmcs_read16(GUEST_TR_SELECTOR), host_tr;
+
+	store_tr(host_tr);
+	if (host_tr != tr) {
+		pr_info("depriv: repriv cpu%d TR mismatch %#x : %#x\n",
+			cpu, host_tr, tr);
+		if (tr == 0)
+			return;
+		load_tr(tr);
+		vmcs_write16(HOST_TR_SELECTOR, tr);
+	}
+}
+
+#define REPRIV_DESC_TABLE(tag, TAG) do {						\
+	store_##tag##dt(&host_dt);							\
+	dt_base = vmcs_readl(GUEST_##TAG##DTR_BASE);					\
+	if (host_dt.address != dt_base)							\
+		pr_err("depriv: repriv cpu%d " #tag "dt base mismatch %#lx : %#lx\n",	\
+		       cpu, host_dt.address, dt_base);					\
+	vmcs_writel(HOST_##TAG##DTR_BASE, dt_base);					\
+	dt_limit = vmcs_read32(GUEST_##TAG##DTR_LIMIT);					\
+	if (host_dt.size != dt_limit) {							\
+		pr_debug("depriv: repriv cpu%d " #tag "dt limit mismatch %#x : %#x\n",	\
+			 cpu, host_dt.size , dt_limit);					\
+		host_dt.size = dt_limit;						\
+		load_##tag##dt(&host_dt);						\
+	}										\
+} while (0)
+
+static inline void vmx_repriv_cpu_desc_tables(void)
+{
+	int cpu = raw_smp_processor_id();
+	struct desc_ptr host_dt;
+	unsigned long dt_base;
+	u32 dt_limit;
+
+	REPRIV_DESC_TABLE(g, G);
+	REPRIV_DESC_TABLE(i, I);
+
+	vmx_repriv_cpu_segments();
+	vmx_repriv_cpu_ldtr();
+	vmx_repriv_cpu_tr();
+}
+
+/*
+ * needed to iret to root mode kernel or user space when the VM exit happened
+ */
+#define DEPRIV_IRET_STACK_GUEST_RIP		(0 * 8)
+#define DEPRIV_IRET_STACK_GUEST_CS		(1 * 8)
+#define DEPRIV_IRET_STACK_GUEST_RFLAGS		(2 * 8)
+#define DEPRIV_IRET_STACK_GUEST_RSP		(3 * 8)
+#define DEPRIV_IRET_STACK_GUEST_SS		(4 * 8)
+
+#define DEPRIV_IRET_STACK_RIP(base)						\
+	(*(unsigned long *)(base + DEPRIV_IRET_STACK_GUEST_RIP))
+#define DEPRIV_IRET_STACK_CS(base)						\
+	(*(unsigned long *)(base + DEPRIV_IRET_STACK_GUEST_CS))
+#define DEPRIV_IRET_STACK_RFLAGS(base)						\
+	(*(unsigned long *)(base + DEPRIV_IRET_STACK_GUEST_RFLAGS))
+#define DEPRIV_IRET_STACK_RSP(base)						\
+	(*(unsigned long *)(base + DEPRIV_IRET_STACK_GUEST_RSP))
+#define DEPRIV_IRET_STACK_SS(base)						\
+	(*(unsigned long *)(base + DEPRIV_IRET_STACK_GUEST_SS))
+
+#define DEPRIV_SET_ROOT_MODE_TRAMPOLINE_STACK(base) do {			\
+	DEPRIV_IRET_STACK_RIP(base)	= vmcs_readl(GUEST_RIP);		\
+	DEPRIV_IRET_STACK_CS(base)	= vmcs_read16(GUEST_CS_SELECTOR);	\
+	DEPRIV_IRET_STACK_RFLAGS(base)	= vmcs_readl(GUEST_RFLAGS);		\
+	DEPRIV_IRET_STACK_RSP(base)	= vmcs_readl(GUEST_RSP);		\
+	DEPRIV_IRET_STACK_SS(base)	= vmcs_read16(GUEST_SS_SELECTOR);	\
+} while (0)
+
+static void vmx_depriv_debug_with_non_root_mode(void)
+{
+	int cpu = raw_smp_processor_id();
+	unsigned long gdt_base = vmcs_readl(HOST_GDTR_BASE), base;
+	struct desc_struct *dentry;
+	u16 seg;
+	bool is_segment = true;
+
+	vmcs_writel(GUEST_RIP, (unsigned long)asm_depriv_exit);
+
+	seg = vmcs_read16(HOST_CS_SELECTOR);
+	vmcs_write16(GUEST_CS_SELECTOR, seg);
+	DEPRIV_SELECTOR(CS, seg);
+
+	vmcs_writel(GUEST_RFLAGS, 0x2);
+	vmcs_writel(GUEST_RSP, depriv_iret_trampoline_stack(cpu));
+
+	seg = vmcs_read16(HOST_SS_SELECTOR);
+	vmcs_write16(GUEST_SS_SELECTOR, seg);
+	DEPRIV_SELECTOR(SS, seg);
+
+	vmcs_writel(GUEST_CR3, __read_cr3());
+	vmcs_writel(GUEST_GS_BASE, vmcs_readl(HOST_GS_BASE));
+
+	vmcs_write32(CR3_TARGET_COUNT, 0);
+	vmcs_write32(GUEST_TR_AR_BYTES,
+		     vmcs_read32(GUEST_TR_AR_BYTES) & ~VMX_AR_S_MASK);
+
+	pr_info("depriv: cpu%d switching to \"root mode\" with rip %#lx rsp %#lx "
+		"non-root GS base %#lx kernel GS base MSR %#lx (GS base MSR %#lx)\n",
+		cpu, vmcs_readl(GUEST_RIP), vmcs_readl(GUEST_RSP),
+		vmcs_readl(GUEST_GS_BASE), read_msr(MSR_KERNEL_GS_BASE), read_msr(MSR_GS_BASE));
+}
+
+extern struct cpumask cpu_depriv_mode_mask;
+
+/*
+ * sync guest state to host w/o changing guest state
+ */
+bool vmx_repriv_cpu_state(void)
+{
+	int cpu = raw_smp_processor_id();
+	unsigned long stack = depriv_iret_trampoline_stack(cpu);
+	unsigned long cr3 = vmcs_readl(GUEST_CR3);
+	unsigned long trampoline_cr3_pa = cr3 & CR3_ADDR_MASK;
+
+	cpumask_clear_cpu(cpu, &cpu_depriv_mode_mask);
+
+#ifdef CONFIG_PAGE_TABLE_ISOLATION
+/*
+ * the following macros are from arch/x86/entry/calling.h
+ */
+#define PTI_USER_PGTABLE_BIT		PAGE_SHIFT
+#define PTI_USER_PGTABLE_MASK		(1 << PTI_USER_PGTABLE_BIT)
+
+	if (boot_cpu_has(X86_FEATURE_PTI))
+		trampoline_cr3_pa &= ~PTI_USER_PGTABLE_MASK;
+#endif
+
+	// make sure we can execute non-root mode code in root mode
+	native_write_cr3(trampoline_cr3_pa | (cr3 & 0x7ff));
+
+	if (vmcs_read32(CR3_TARGET_COUNT) == DEPRIV_INVALID_HOST_CR3_TARGET_COUNT)
+		pr_err("depriv: cpu%d invalid host state @ rip: %#lx rsp: %#lx\n",
+		       cpu, vmcs_readl(GUEST_RIP), vmcs_readl(GUEST_RSP));
+
+	vmx_repriv_cpu_crs();
+	vmx_repriv_cpu_misc();
+	vmx_repriv_cpu_sysenter_msrs();
+	vmx_repriv_cpu_desc_tables();
+
+	DEPRIV_SET_ROOT_MODE_TRAMPOLINE_STACK(stack);
+
+	/* prepare for swapgs in asm_depriv_exit */
+	wrmsrl(MSR_KERNEL_GS_BASE, vmcs_readl(GUEST_GS_BASE));
+
+	/* powerful switch to debug issues in non-root mode */
+	if (debug_host_in_non_root_mode) {
+		unsigned long rsp;
+
+		asm volatile("mov %%rsp,%0" : "=m"(rsp));
+		pr_info("depriv: cpu%d current rsp %#lx CS selector %x in %s\n",
+			cpu, rsp, vmcs_read16(GUEST_CS_SELECTOR), __FUNCTION__);
+
+		vmx_depriv_debug_with_non_root_mode();
+		if (debug_host_in_non_root_mode++ == 10)
+			debug_host_in_non_root_mode = 0;
+		return false;
+	}
+
+	pr_debug("depriv: cpu%d switching to root mode with rip %#lx rsp %#lx "
+		 "non-root GS base %#lx kernel GS base MSR %#lx (GS base MSR %#lx)\n",
+		 cpu, vmcs_readl(GUEST_RIP), vmcs_readl(GUEST_RSP),
+		 vmcs_readl(GUEST_GS_BASE), read_msr(MSR_KERNEL_GS_BASE), read_msr(MSR_GS_BASE));
+
+	return true;
+}
diff --git a/arch/x86/depriv/vmx/vmx.h b/arch/x86/depriv/vmx/vmx.h
new file mode 100644
index 000000000000..795525f3cd78
--- /dev/null
+++ b/arch/x86/depriv/vmx/vmx.h
@@ -0,0 +1,347 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef __DEPRIV_X86_VMX_H
+#define __DEPRIV_X86_VMX_H
+
+#include <linux/depriv_types.h>
+#include <linux/list.h>
+
+#include <asm/bug.h>
+#include <asm/depriv.h>
+#include <asm/page.h>
+#include <asm/vmx.h>
+
+#define DE_VECTOR 0
+#define DB_VECTOR 1
+#define BP_VECTOR 3
+#define OF_VECTOR 4
+#define BR_VECTOR 5
+#define UD_VECTOR 6
+#define NM_VECTOR 7
+#define DF_VECTOR 8
+#define TS_VECTOR 10
+#define NP_VECTOR 11
+#define SS_VECTOR 12
+#define GP_VECTOR 13
+#define PF_VECTOR 14
+#define MF_VECTOR 16
+#define AC_VECTOR 17
+#define MC_VECTOR 18
+#define XM_VECTOR 19
+#define VE_VECTOR 20
+
+struct vmcs_config {
+	int size;
+	int order;
+	u32 basic_cap;
+	u32 vmx_cap_ept;
+	u32 vmx_cap_vpid;
+	u32 revision_id;
+	u32 pin_based_exec_ctrl;
+	u32 cpu_based_exec_ctrl;
+	u32 cpu_based_2nd_exec_ctrl;
+	u32 vmexit_ctrl;
+	u32 vmentry_ctrl;
+};
+
+struct vmcs_hdr {
+	u32 revision_id:31;
+	u32 shadow_vmcs:1;
+};
+
+struct vmcs {
+	struct vmcs_hdr hdr;
+	u32 abort;
+	char data[];
+};
+
+static inline bool is_intr_type_n(u32 intr_info, u32 type, u8 vector)
+{
+	const u32 mask = INTR_INFO_VALID_MASK | INTR_INFO_INTR_TYPE_MASK |
+			 INTR_INFO_VECTOR_MASK;
+
+	return (intr_info & mask) == (INTR_INFO_VALID_MASK | type | vector);
+}
+
+static inline bool is_exception_n(u32 intr_info, u8 vector)
+{
+	return is_intr_type_n(intr_info, INTR_TYPE_HARD_EXCEPTION, vector);
+}
+
+static inline bool is_debug(u32 intr_info)
+{
+	return is_exception_n(intr_info, DB_VECTOR);
+}
+
+static inline bool is_breakpoint(u32 intr_info)
+{
+	return is_exception_n(intr_info, BP_VECTOR);
+}
+
+static inline bool is_page_fault(u32 intr_info)
+{
+	return is_exception_n(intr_info, PF_VECTOR);
+}
+
+static inline bool is_invalid_opcode(u32 intr_info)
+{
+	return is_exception_n(intr_info, UD_VECTOR);
+}
+
+static inline bool is_gp_fault(u32 intr_info)
+{
+	return is_exception_n(intr_info, GP_VECTOR);
+}
+
+static inline bool is_machine_check(u32 intr_info)
+{
+	return is_exception_n(intr_info, MC_VECTOR);
+}
+
+static inline bool is_intr_type(u32 intr_info, u32 type)
+{
+	const u32 mask = INTR_INFO_VALID_MASK | INTR_INFO_INTR_TYPE_MASK;
+
+	return (intr_info & mask) == (INTR_INFO_VALID_MASK | type);
+}
+
+static inline bool is_nmi(u32 intr_info)
+{
+	return is_intr_type(intr_info, INTR_TYPE_NMI_INTR);
+}
+
+static __always_inline void vmcs_check16(unsigned long field)
+{
+	BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6001) == 0x2000,
+			 "16-bit accessor invalid for 64-bit field");
+	BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6001) == 0x2001,
+			 "16-bit accessor invalid for 64-bit high field");
+	BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6000) == 0x4000,
+			 "16-bit accessor invalid for 32-bit high field");
+	BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6000) == 0x6000,
+			 "16-bit accessor invalid for natural width field");
+}
+
+static __always_inline void vmcs_check32(unsigned long field)
+{
+	BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6000) == 0,
+			 "32-bit accessor invalid for 16-bit field");
+	BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6000) == 0x6000,
+			 "32-bit accessor invalid for natural width field");
+}
+
+static __always_inline void vmcs_check64(unsigned long field)
+{
+	BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6000) == 0,
+			 "64-bit accessor invalid for 16-bit field");
+	BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6001) == 0x2001,
+			 "64-bit accessor invalid for 64-bit high field");
+	BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6000) == 0x4000,
+			 "64-bit accessor invalid for 32-bit field");
+	BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6000) == 0x6000,
+			 "64-bit accessor invalid for natural width field");
+}
+
+static __always_inline void vmcs_checkl(unsigned long field)
+{
+	BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6000) == 0,
+			 "Natural width accessor invalid for 16-bit field");
+	BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6001) == 0x2000,
+			 "Natural width accessor invalid for 64-bit field");
+	BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6001) == 0x2001,
+			 "Natural width accessor invalid for 64-bit high field");
+	BUILD_BUG_ON_MSG(__builtin_constant_p(field) && ((field) & 0x6000) == 0x4000,
+			 "Natural width accessor invalid for 32-bit field");
+}
+
+asmlinkage void vmread_error(unsigned long field, bool fault);
+__attribute__((regparm(0))) void vmread_error_trampoline(unsigned long field,
+							 bool fault);
+
+static __always_inline unsigned long __vmcs_readl(unsigned long field)
+{
+	unsigned long value;
+
+	asm volatile("1: vmread %2, %1\n\t"
+		     ".byte 0x3e\n\t" /* branch taken hint */
+		     "ja 3f\n\t"
+
+		     /*
+		      * VMREAD failed.  Push '0' for @fault, push the failing
+		      * @field, and bounce through the trampoline to preserve
+		      * volatile registers.
+		      */
+		     "push $0\n\t"
+		     "push %2\n\t"
+		     "2:call vmread_error_trampoline\n\t"
+
+		     /*
+		      * Unwind the stack.  Note, the trampoline zeros out the
+		      * memory for @fault so that the result is '0' on error.
+		      */
+		     "pop %2\n\t"
+		     "pop %1\n\t"
+		     "3:\n\t"
+
+		     /* VMREAD faulted.  As above, except push '1' for @fault. */
+		     ".pushsection .fixup, \"ax\"\n\t"
+		     "4: push $1\n\t"
+		     "push %2\n\t"
+		     "jmp 2b\n\t"
+		     ".popsection\n\t"
+		     _ASM_EXTABLE(1b, 4b)
+		     : ASM_CALL_CONSTRAINT, "=r"(value) : "r"(field) : "cc");
+	return value;
+}
+
+static __always_inline u16 vmcs_read16(unsigned long field)
+{
+	vmcs_check16(field);
+	return __vmcs_readl(field);
+}
+
+static __always_inline u32 vmcs_read32(unsigned long field)
+{
+	vmcs_check32(field);
+	return __vmcs_readl(field);
+}
+
+static __always_inline u64 vmcs_read64(unsigned long field)
+{
+	vmcs_check64(field);
+	return __vmcs_readl(field);
+}
+
+static __always_inline unsigned long vmcs_readl(unsigned long field)
+{
+	vmcs_checkl(field);
+	return __vmcs_readl(field);
+}
+
+#define vmx_asm1(insn, op1, error_args...)				\
+do {									\
+	asm_volatile_goto("1: " __stringify(insn) " %0\n\t"		\
+			  ".byte 0x2e\n\t" /* branch not taken hint */	\
+			  "jna %l[error]\n\t"				\
+			  _ASM_EXTABLE(1b, %l[fault])			\
+			  : : op1 : "cc" : error, fault);		\
+	return;								\
+error:									\
+	instrumentation_begin();					\
+	insn##_error(error_args);					\
+	instrumentation_end();						\
+	return;								\
+fault:									\
+	BUG();								\
+} while (0)
+
+#define vmx_asm2(insn, op1, op2, error_args...)				\
+do {									\
+	asm_volatile_goto("1: "  __stringify(insn) " %1, %0\n\t"	\
+			  ".byte 0x2e\n\t" /* branch not taken hint */	\
+			  "jna %l[error]\n\t"				\
+			  _ASM_EXTABLE(1b, %l[fault])			\
+			  : : op1, op2 : "cc" : error, fault);		\
+	return;								\
+error:									\
+	instrumentation_begin();					\
+	insn##_error(error_args);					\
+	instrumentation_end();						\
+	return;								\
+fault:									\
+	BUG();								\
+} while (0)
+
+void vmwrite_error(unsigned long field, unsigned long value);
+void vmclear_error(struct vmcs *vmcs, u64 phys_addr);
+void vmptrld_error(struct vmcs *vmcs, u64 phys_addr);
+void invept_error(unsigned long ext, u64 eptp, gpa_t gpa);
+
+static __always_inline void __vmcs_writel(unsigned long field, unsigned long value)
+{
+	vmx_asm2(vmwrite, "r"(field), "rm"(value), field, value);
+}
+
+static __always_inline void vmcs_write16(unsigned long field, u16 value)
+{
+	vmcs_check16(field);
+	__vmcs_writel(field, value);
+}
+
+static __always_inline void vmcs_write32(unsigned long field, u32 value)
+{
+	vmcs_check32(field);
+	__vmcs_writel(field, value);
+}
+
+static __always_inline void vmcs_write64(unsigned long field, u64 value)
+{
+	vmcs_check64(field);
+	__vmcs_writel(field, value);
+}
+
+static __always_inline void vmcs_writel(unsigned long field, unsigned long value)
+{
+	vmcs_checkl(field);
+	__vmcs_writel(field, value);
+}
+
+static inline void vmcs_clear(struct vmcs *vmcs)
+{
+	u64 phys_addr = __pa(vmcs);
+	vmx_asm1(vmclear, "m"(phys_addr), vmcs, phys_addr);
+}
+
+static inline void vmcs_load(struct vmcs *vmcs)
+{
+	u64 phys_addr = __pa(vmcs);
+	vmx_asm1(vmptrld, "m"(phys_addr), vmcs, phys_addr);
+}
+
+static inline void __invept(unsigned long ext, u64 eptp, gpa_t gpa)
+{
+	struct {
+		u64 eptp, gpa;
+	} operand = {eptp, gpa};
+
+	vmx_asm2(invept, "r"(ext), "m"(operand), ext, eptp, gpa);
+}
+
+static inline void ept_sync_global(void)
+{
+	__invept(VMX_EPT_EXTENT_GLOBAL, 0, 0);
+}
+
+extern struct vmcs_config depriv_vmcs_config;
+
+static inline bool cpu_has_secondary_exec_ctrls(void)
+{
+	return depriv_vmcs_config.cpu_based_exec_ctrl &
+		CPU_BASED_ACTIVATE_SECONDARY_CONTROLS;
+}
+
+static inline bool cpu_has_vmx_ept(void)
+{
+	return depriv_vmcs_config.cpu_based_2nd_exec_ctrl &
+		SECONDARY_EXEC_ENABLE_EPT;
+}
+
+static inline bool cpu_has_vmx_spp(void)
+{
+	return depriv_vmcs_config.cpu_based_2nd_exec_ctrl &
+		SECONDARY_EXEC_ENABLE_SPP;
+}
+
+#define DEPRIV_INVALID_HOST_CR3_TARGET_COUNT	0x100000
+
+int __init setup_vmcs_config(void);
+
+inline struct vmcs *alloc_vmcs(void);
+inline void free_vmcs(struct vmcs *vmcs);
+
+inline int __cpu_vmxon(u64 vmxon_pointer);
+inline void __cpu_vmxoff(void);
+
+void vmx_depriv_cpu_state(void);
+void vmx_depriv_cpu_fsgs(void);
+
+#endif /* __DEPRIV_X86_VMX_H */
diff --git a/arch/x86/depriv/x86.c b/arch/x86/depriv/x86.c
new file mode 100644
index 000000000000..18917b7d3d20
--- /dev/null
+++ b/arch/x86/depriv/x86.c
@@ -0,0 +1,56 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Deprivilege is to run Linux in hardware guest mode whenever possible
+ *
+ * Authors:
+ * 	Xin Li <fantry@gmail.com>
+ */
+
+#include <linux/sched.h>
+#include <linux/pid_namespace.h>
+
+#include <asm/depriv.h>
+#include <asm/percpu.h>
+#include <asm/x86_vcpu_regs.h>
+
+DEFINE_PER_CPU(void *, depriv_cpu_state) = NULL;
+EXPORT_PER_CPU_SYMBOL(depriv_cpu_state);
+
+struct depriv_ops depriv_ops = {
+	.enter = NULL,
+	.exit = NULL,
+	.on_destroy_pid_ns = NULL,
+	.dump_context = NULL,
+};
+EXPORT_SYMBOL_GPL(depriv_ops);
+
+/*
+ * WARNING: must be called with interrupt disabled!
+ */
+void depriv_switch(struct task_struct *prev, struct task_struct *next)
+{
+	BUG_ON(!arch_irqs_disabled());
+
+	if (task_active_pid_ns(prev) == task_active_pid_ns(next))
+		return;
+
+	if (!per_cpu(depriv_cpu_state, smp_processor_id()))
+		return;
+
+	if (depriv_ops.exit)
+		depriv_ops.exit();
+	if (depriv_ops.enter)
+		depriv_ops.enter(next);
+}
+
+void on_destroy_pid_ns(struct pid_namespace *ns)
+{
+	if (depriv_ops.on_destroy_pid_ns)
+		depriv_ops.on_destroy_pid_ns(ns);
+}
+
+void depriv_dump_context(unsigned long va)
+{
+	if (depriv_ops.dump_context)
+		depriv_ops.dump_context(va);
+}
diff --git a/arch/x86/entry/Makefile b/arch/x86/entry/Makefile
index 7fec5dcf6438..261a39737ec4 100644
--- a/arch/x86/entry/Makefile
+++ b/arch/x86/entry/Makefile
@@ -12,7 +12,7 @@ CFLAGS_REMOVE_common.o		= $(CC_FLAGS_FTRACE)
 CFLAGS_common.o			+= -fno-stack-protector
 
 obj-y				:= entry_$(BITS).o thunk_$(BITS).o syscall_$(BITS).o
-obj-y				+= common.o
+obj-y				+= common.o ../depriv/x86.o
 
 obj-y				+= vdso/
 obj-y				+= vsyscall/
diff --git a/arch/x86/entry/entry_64.S b/arch/x86/entry/entry_64.S
index 466df3e50276..74046a34e447 100644
--- a/arch/x86/entry/entry_64.S
+++ b/arch/x86/entry/entry_64.S
@@ -1414,6 +1414,19 @@ nmi_restore:
 	iretq
 SYM_CODE_END(asm_exc_nmi)
 
+SYM_CODE_START(asm_depriv_exit)
+	testb	$3, 8(%rsp)
+	jz	1f
+
+	pushq	%rdi
+	SWITCH_TO_USER_CR3_STACK scratch_reg=%rdi
+	popq	%rdi
+
+1:	SWAPGS
+	INTERRUPT_RETURN
+SYM_CODE_END(asm_depriv_exit)
+EXPORT_SYMBOL(asm_depriv_exit)
+
 #ifndef CONFIG_IA32_EMULATION
 /*
  * This handles SYSCALL from 32-bit code.  There is no way to program
diff --git a/arch/x86/include/asm/cpufeatures.h b/arch/x86/include/asm/cpufeatures.h
index 6db4e2932b3d..9b4504c18e5e 100644
--- a/arch/x86/include/asm/cpufeatures.h
+++ b/arch/x86/include/asm/cpufeatures.h
@@ -230,6 +230,7 @@
 #define X86_FEATURE_FLEXPRIORITY	( 8*32+ 2) /* Intel FlexPriority */
 #define X86_FEATURE_EPT			( 8*32+ 3) /* Intel Extended Page Table */
 #define X86_FEATURE_VPID		( 8*32+ 4) /* Intel Virtual Processor ID */
+#define X86_FEATURE_SPP			( 8*32+ 5) /* Intel EPT-based Sub-Page Write Protection */
 
 #define X86_FEATURE_VMMCALL		( 8*32+15) /* Prefer VMMCALL to VMCALL */
 #define X86_FEATURE_XENPV		( 8*32+16) /* "" Xen paravirtual guest */
diff --git a/arch/x86/include/asm/depriv.h b/arch/x86/include/asm/depriv.h
new file mode 100644
index 000000000000..8b81951c9ab2
--- /dev/null
+++ b/arch/x86/include/asm/depriv.h
@@ -0,0 +1,36 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef _ASM_X86_DEPRIV_H
+#define _ASM_X86_DEPRIV_H
+
+#include <linux/depriv.h>
+
+#include <asm/percpu.h>
+#include <asm/cpu_entry_area.h>
+
+#define DEPRIV_DUMP_GPRS(regs) do {					\
+	int i, cpu = raw_smp_processor_id();				\
+	unsigned long *r = &regs[0];					\
+	pr_info("depriv: cpu%d non-root mode GPRs:\n", cpu);		\
+	for (i = __VCPU_REGS_RAX; i <= __VCPU_REGS_R15; i += 4) {	\
+		pr_info("\tcpu%d\t%016lx %016lx %016lx %016lx\n",	\
+			cpu, r[i], r[i + 1], r[i + 2], r[i + 3]);	\
+	}								\
+} while (0)
+
+struct depriv_ops {
+	bool (*enter)(struct task_struct *);
+	void (*exit)(void);
+	void (*on_destroy_pid_ns)(struct pid_namespace *ns);
+	void (*dump_context)(unsigned long va);
+};
+
+DECLARE_PER_CPU(void *, depriv_cpu_state);
+
+extern void asm_depriv_exit(void);
+
+static inline unsigned long depriv_iret_trampoline_stack(int cpu)
+{
+	return get_cpu_entry_area(cpu)->tss.x86_tss.ist[IST_INDEX_NMI] - 64 * 8;
+}
+
+#endif /* _ASM_X86_DEPRIV_H */
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 0677b9ea01c9..5d20ec0846c9 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -1822,15 +1822,10 @@ static inline void kvm_load_ldt(u16 sel)
 	asm("lldt %0" : : "rm"(sel));
 }
 
-#ifdef CONFIG_X86_64
-static inline unsigned long read_msr(unsigned long msr)
+static inline u32 get_rdx_init_val(void)
 {
-	u64 value;
-
-	rdmsrl(msr, value);
-	return value;
+	return 0x600; /* P6 family */
 }
-#endif
 
 static inline void kvm_inject_gp(struct kvm_vcpu *vcpu, u32 error_code)
 {
diff --git a/arch/x86/include/asm/kvm_vcpu_regs.h b/arch/x86/include/asm/kvm_vcpu_regs.h
index 1af2cb59233b..3bfb862af3f9 100644
--- a/arch/x86/include/asm/kvm_vcpu_regs.h
+++ b/arch/x86/include/asm/kvm_vcpu_regs.h
@@ -2,24 +2,6 @@
 #ifndef _ASM_X86_KVM_VCPU_REGS_H
 #define _ASM_X86_KVM_VCPU_REGS_H
 
-#define __VCPU_REGS_RAX  0
-#define __VCPU_REGS_RCX  1
-#define __VCPU_REGS_RDX  2
-#define __VCPU_REGS_RBX  3
-#define __VCPU_REGS_RSP  4
-#define __VCPU_REGS_RBP  5
-#define __VCPU_REGS_RSI  6
-#define __VCPU_REGS_RDI  7
-
-#ifdef CONFIG_X86_64
-#define __VCPU_REGS_R8   8
-#define __VCPU_REGS_R9   9
-#define __VCPU_REGS_R10 10
-#define __VCPU_REGS_R11 11
-#define __VCPU_REGS_R12 12
-#define __VCPU_REGS_R13 13
-#define __VCPU_REGS_R14 14
-#define __VCPU_REGS_R15 15
-#endif
+#include <asm/x86_vcpu_regs.h>
 
 #endif /* _ASM_X86_KVM_VCPU_REGS_H */
diff --git a/arch/x86/include/asm/msr.h b/arch/x86/include/asm/msr.h
index d42e6c6b47b1..bd3eb52525aa 100644
--- a/arch/x86/include/asm/msr.h
+++ b/arch/x86/include/asm/msr.h
@@ -266,6 +266,16 @@ static inline void wrmsr(unsigned int msr, u32 low, u32 high)
 #define rdmsrl(msr, val)			\
 	((val) = native_read_msr((msr)))
 
+#ifdef CONFIG_X86_64
+static inline unsigned long read_msr(unsigned long msr)
+{
+	u64 value;
+
+	rdmsrl(msr, value);
+	return value;
+}
+#endif
+
 static inline void wrmsrl(unsigned int msr, u64 val)
 {
 	native_write_msr(msr, (u32)(val & 0xffffffffULL), (u32)(val >> 32));
diff --git a/arch/x86/include/asm/switch_to.h b/arch/x86/include/asm/switch_to.h
index b5f0d2ff47e4..c6202fee085b 100644
--- a/arch/x86/include/asm/switch_to.h
+++ b/arch/x86/include/asm/switch_to.h
@@ -12,6 +12,8 @@ struct task_struct *__switch_to_asm(struct task_struct *prev,
 __visible struct task_struct *__switch_to(struct task_struct *prev,
 					  struct task_struct *next);
 
+void depriv_switch(struct task_struct *prev, struct task_struct *next);
+
 asmlinkage void ret_from_fork(void);
 
 /*
@@ -46,6 +48,7 @@ struct fork_frame {
 
 #define switch_to(prev, next, last)					\
 do {									\
+	depriv_switch((prev), (next));					\
 	((last) = __switch_to_asm((prev), (next)));			\
 } while (0)
 
diff --git a/arch/x86/include/asm/vmx.h b/arch/x86/include/asm/vmx.h
index 0ffaa3156a4e..c18f3cc64030 100644
--- a/arch/x86/include/asm/vmx.h
+++ b/arch/x86/include/asm/vmx.h
@@ -70,6 +70,7 @@
 #define SECONDARY_EXEC_PT_CONCEAL_VMX		VMCS_CONTROL_BIT(PT_CONCEAL_VMX)
 #define SECONDARY_EXEC_XSAVES			VMCS_CONTROL_BIT(XSAVES)
 #define SECONDARY_EXEC_MODE_BASED_EPT_EXEC	VMCS_CONTROL_BIT(MODE_BASED_EPT_EXEC)
+#define SECONDARY_EXEC_ENABLE_SPP		VMCS_CONTROL_BIT(SPP)
 #define SECONDARY_EXEC_PT_USE_GPA		VMCS_CONTROL_BIT(PT_USE_GPA)
 #define SECONDARY_EXEC_TSC_SCALING              VMCS_CONTROL_BIT(TSC_SCALING)
 #define SECONDARY_EXEC_ENABLE_USR_WAIT_PAUSE	VMCS_CONTROL_BIT(USR_WAIT_PAUSE)
@@ -219,6 +220,8 @@ enum vmcs_field {
 	XSS_EXIT_BITMAP_HIGH            = 0x0000202D,
 	ENCLS_EXITING_BITMAP		= 0x0000202E,
 	ENCLS_EXITING_BITMAP_HIGH	= 0x0000202F,
+	SPPT_POINTER                    = 0x00002030,
+	SPPT_POINTER_HIGH               = 0x00002031,
 	TSC_MULTIPLIER                  = 0x00002032,
 	TSC_MULTIPLIER_HIGH             = 0x00002033,
 	GUEST_PHYSICAL_ADDRESS          = 0x00002400,
diff --git a/arch/x86/include/asm/vmxfeatures.h b/arch/x86/include/asm/vmxfeatures.h
index d9a74681a77d..514eea76f3bc 100644
--- a/arch/x86/include/asm/vmxfeatures.h
+++ b/arch/x86/include/asm/vmxfeatures.h
@@ -79,6 +79,7 @@
 #define VMX_FEATURE_PT_CONCEAL_VMX	( 2*32+ 19) /* "" Suppress VMX indicators in Processor Trace */
 #define VMX_FEATURE_XSAVES		( 2*32+ 20) /* "" Enable XSAVES and XRSTORS in guest */
 #define VMX_FEATURE_MODE_BASED_EPT_EXEC	( 2*32+ 22) /* "ept_mode_based_exec" Enable separate EPT EXEC bits for supervisor vs. user */
+#define VMX_FEATURE_SPP			( 2*32+ 23) /* "" Enable EPT sub-page protection */
 #define VMX_FEATURE_PT_USE_GPA		( 2*32+ 24) /* "" Processor Trace logs GPAs */
 #define VMX_FEATURE_TSC_SCALING		( 2*32+ 25) /* Scale hardware TSC when read in guest */
 #define VMX_FEATURE_USR_WAIT_PAUSE	( 2*32+ 26) /* Enable TPAUSE, UMONITOR, UMWAIT in guest */
diff --git a/arch/x86/include/asm/x86_vcpu_regs.h b/arch/x86/include/asm/x86_vcpu_regs.h
new file mode 100644
index 000000000000..cb56520bd49b
--- /dev/null
+++ b/arch/x86/include/asm/x86_vcpu_regs.h
@@ -0,0 +1,25 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef _ASM_X86_VCPU_REGS_H
+#define _ASM_X86_VCPU_REGS_H
+
+#define __VCPU_REGS_RAX  0
+#define __VCPU_REGS_RCX  1
+#define __VCPU_REGS_RDX  2
+#define __VCPU_REGS_RBX  3
+#define __VCPU_REGS_RSP  4
+#define __VCPU_REGS_RBP  5
+#define __VCPU_REGS_RSI  6
+#define __VCPU_REGS_RDI  7
+
+#ifdef CONFIG_X86_64
+#define __VCPU_REGS_R8   8
+#define __VCPU_REGS_R9   9
+#define __VCPU_REGS_R10 10
+#define __VCPU_REGS_R11 11
+#define __VCPU_REGS_R12 12
+#define __VCPU_REGS_R13 13
+#define __VCPU_REGS_R14 14
+#define __VCPU_REGS_R15 15
+#endif
+
+#endif /* _ASM_X86_VCPU_REGS_H */
diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c
index 7b8382c11788..5c3d63e17e46 100644
--- a/arch/x86/kernel/cpu/common.c
+++ b/arch/x86/kernel/cpu/common.c
@@ -1798,6 +1798,17 @@ static void wrmsrl_cstar(unsigned long val)
 		wrmsrl(MSR_CSTAR, val);
 }
 
+#if IS_ENABLED(CONFIG_KVM_INTEL_DEPRIV)
+DEFINE_PER_CPU(void *, depriv_cpu_state) = NULL;
+EXPORT_PER_CPU_SYMBOL(depriv_cpu_state);
+
+struct depriv_ops depriv_ops = {
+	.enter = NULL,
+	.exit = NULL,
+};
+EXPORT_SYMBOL_GPL(depriv_ops);
+#endif
+
 /* May not be marked __init: used by software suspend */
 void syscall_init(void)
 {
diff --git a/arch/x86/kernel/cpu/feat_ctl.c b/arch/x86/kernel/cpu/feat_ctl.c
index da696eb4821a..534de8385a58 100644
--- a/arch/x86/kernel/cpu/feat_ctl.c
+++ b/arch/x86/kernel/cpu/feat_ctl.c
@@ -86,6 +86,8 @@ static void init_vmx_capabilities(struct cpuinfo_x86 *c)
 		set_cpu_cap(c, X86_FEATURE_VNMI);
 	if (c->vmx_capability[SECONDARY_CTLS] & VMX_F(EPT))
 		set_cpu_cap(c, X86_FEATURE_EPT);
+	if (c->vmx_capability[SECONDARY_CTLS] & VMX_F(SPP))
+		set_cpu_cap(c, X86_FEATURE_SPP);
 	if (c->vmx_capability[MISC_FEATURES] & VMX_F(EPT_AD))
 		set_cpu_cap(c, X86_FEATURE_EPT_AD);
 	if (c->vmx_capability[MISC_FEATURES] & VMX_F(VPID))
diff --git a/arch/x86/kvm/vmx/vmx.c b/arch/x86/kvm/vmx/vmx.c
index 1b2e9d8c5cc9..539ca4e6e878 100644
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@ -38,6 +38,9 @@
 #include <asm/fpu/api.h>
 #include <asm/fpu/xstate.h>
 #include <asm/idtentry.h>
+#if IS_ENABLED(CONFIG_KVM_INTEL_DEPRIV_HOST)
+#include <asm/insn.h>
+#endif
 #include <asm/io.h>
 #include <asm/irq_remapping.h>
 #include <asm/kexec.h>
diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index f8fc7441baea..81f0c6b7630a 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -577,4 +577,9 @@ static inline int vmx_get_instr_info_reg2(u32 vmx_instr_info)
 	return (vmx_instr_info >> 28) & 0xf;
 }
 
+#if IS_ENABLED(CONFIG_KVM_INTEL_DEPRIV_HOST)
+bool vmx_depriv_vmexit_handler(unsigned long *guset_regs);
+void vmx_check_guest_state(void);
+#endif
+
 #endif /* __KVM_X86_VMX_H */
diff --git a/drivers/misc/Makefile b/drivers/misc/Makefile
index a086197af544..8d6f58982ee2 100644
--- a/drivers/misc/Makefile
+++ b/drivers/misc/Makefile
@@ -59,3 +59,4 @@ obj-$(CONFIG_UACCE)		+= uacce/
 obj-$(CONFIG_XILINX_SDFEC)	+= xilinx_sdfec.o
 obj-$(CONFIG_HISI_HIKEY_USB)	+= hisi_hikey_usb.o
 obj-$(CONFIG_HI6421V600_IRQ)	+= hi6421v600-irq.o
+obj-$(CONFIG_KVM_INTEL_DEPRIV)	+= smep_flipor.o
diff --git a/drivers/misc/smep_flipor.c b/drivers/misc/smep_flipor.c
new file mode 100644
index 000000000000..b4558bf382ea
--- /dev/null
+++ b/drivers/misc/smep_flipor.c
@@ -0,0 +1,60 @@
+#include <uapi/asm/processor-flags.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/init.h>
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Xin Li");
+
+static int target_cpu;
+
+static void smep(bool enable)
+{
+	int cpu = raw_smp_processor_id();
+	unsigned long val;
+	unsigned long *pv = &val;
+
+	asm volatile("mov %%cr4,%0" : "=r" (val) : __FORCE_ORDER);
+	pr_info("smep: cpu%d cr4: %#lx\n", cpu, val);
+
+	if (enable)
+		val |= X86_CR4_SMEP;
+	else {
+		val &= ~X86_CR4_SMEP;
+		target_cpu = cpu;
+	}
+	pr_info("smep: cpu%d cr4 being set to %#lx\n", cpu, val);
+
+	asm volatile("mov %0,%%cr4" : : "r" (val) : "memory");
+
+	asm volatile("mov %%cr4,%0" : "=r" (val) : __FORCE_ORDER);
+	pr_info("smep: cpu%d cr4 set to %#lx\n", cpu, val);
+
+	asm volatile("mov %%rsp,%0" : "=m" (val));
+	for (cpu = 0; cpu < 8; cpu++)
+		pr_debug("smep: stack[%d]=%#lx\n", cpu, pv[cpu]);
+}
+
+static int __init smep_init(void)
+{
+	preempt_disable();
+	smep(false);
+	preempt_enable();
+
+	pr_info("smep: smep driver loaded\n");
+	return 0;
+}
+
+static void smep_set(void *info)
+{
+	smep(true);
+}
+
+static void __exit smep_exit(void)
+{
+	smp_call_function_single(target_cpu, smep_set, NULL, 1);
+	pr_info("smep: smep driver unloaded\n");
+}
+
+module_init(smep_init);
+module_exit(smep_exit);
diff --git a/include/linux/depriv.h b/include/linux/depriv.h
new file mode 100644
index 000000000000..c4cbc799ca03
--- /dev/null
+++ b/include/linux/depriv.h
@@ -0,0 +1,15 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef _DEPRIV_H
+#define _DEPRIV_H
+
+#include <linux/threads.h>
+
+struct depriv_context {
+	void *cpu_context[NR_CPUS];
+};
+
+struct pid_namespace;
+void on_destroy_pid_ns(struct pid_namespace *ns);
+void depriv_dump_context(unsigned long va);
+
+#endif /* _DEPRIV_H */
diff --git a/include/linux/depriv_types.h b/include/linux/depriv_types.h
new file mode 100644
index 000000000000..d5db8271a712
--- /dev/null
+++ b/include/linux/depriv_types.h
@@ -0,0 +1,8 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+
+#ifndef __DEPRIV_TYPES_H__
+#define __DEPRIV_TYPES_H__
+
+#include <linux/virt_types.h>
+
+#endif /* __DEPRIV_TYPES_H__ */
diff --git a/include/linux/kvm_types.h b/include/linux/kvm_types.h
index dceac12c1ce5..63c065b7edc5 100644
--- a/include/linux/kvm_types.h
+++ b/include/linux/kvm_types.h
@@ -20,30 +20,10 @@ enum kvm_mr_change;
 
 #include <linux/types.h>
 #include <linux/spinlock_types.h>
+#include <linux/virt_types.h>
 
 #include <asm/kvm_types.h>
 
-/*
- * Address types:
- *
- *  gva - guest virtual address
- *  gpa - guest physical address
- *  gfn - guest frame number
- *  hva - host virtual address
- *  hpa - host physical address
- *  hfn - host frame number
- */
-
-typedef unsigned long  gva_t;
-typedef u64            gpa_t;
-typedef u64            gfn_t;
-
-#define GPA_INVALID	(~(gpa_t)0)
-
-typedef unsigned long  hva_t;
-typedef u64            hpa_t;
-typedef u64            hfn_t;
-
 typedef hfn_t kvm_pfn_t;
 
 struct gfn_to_hva_cache {
diff --git a/include/linux/pid_namespace.h b/include/linux/pid_namespace.h
index 7c7e627503d2..8a8bf4c71282 100644
--- a/include/linux/pid_namespace.h
+++ b/include/linux/pid_namespace.h
@@ -15,6 +15,7 @@
 #define MAX_PID_NS_LEVEL 32
 
 struct fs_pin;
+struct depriv_context;
 
 struct pid_namespace {
 	struct idr idr;
@@ -31,6 +32,7 @@ struct pid_namespace {
 	struct ucounts *ucounts;
 	int reboot;	/* group exit code if this pidns was rebooted */
 	struct ns_common ns;
+	struct depriv_context *depriv_context;
 } __randomize_layout;
 
 extern struct pid_namespace init_pid_ns;
diff --git a/include/linux/virt_types.h b/include/linux/virt_types.h
new file mode 100644
index 000000000000..d78e2c71ddbb
--- /dev/null
+++ b/include/linux/virt_types.h
@@ -0,0 +1,29 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+
+#ifndef __VIRT_TYPES_H__
+#define __VIRT_TYPES_H__
+
+#include <linux/types.h>
+
+/*
+ * Address types:
+ *
+ *  gva - guest virtual address
+ *  gpa - guest physical address
+ *  gfn - guest frame number
+ *  hva - host virtual address
+ *  hpa - host physical address
+ *  hfn - host frame number
+ */
+
+typedef unsigned long  gva_t;
+typedef u64            gpa_t;
+typedef u64            gfn_t;
+
+#define GPA_INVALID	(~(gpa_t)0)
+
+typedef unsigned long  hva_t;
+typedef u64            hpa_t;
+typedef u64            hfn_t;
+
+#endif /* __VIRT_TYPES_H__ */
diff --git a/kernel/pid_namespace.c b/kernel/pid_namespace.c
index a46a3723bc66..0d04b6705d01 100644
--- a/kernel/pid_namespace.c
+++ b/kernel/pid_namespace.c
@@ -23,9 +23,12 @@
 #include <linux/sched/task.h>
 #include <linux/sched/signal.h>
 #include <linux/idr.h>
+#include <linux/depriv.h>
 
 static DEFINE_MUTEX(pid_caches_mutex);
+static struct kmem_cache *depriv_ctx_cachep;
 static struct kmem_cache *pid_ns_cachep;
+
 /* Write once array, filled from the beginning. */
 static struct kmem_cache *pid_cache[MAX_PID_NS_LEVEL];
 
@@ -71,6 +74,7 @@ static void dec_pid_namespaces(struct ucounts *ucounts)
 static struct pid_namespace *create_pid_namespace(struct user_namespace *user_ns,
 	struct pid_namespace *parent_pid_ns)
 {
+	struct depriv_context *depriv_context;
 	struct pid_namespace *ns;
 	unsigned int level = parent_pid_ns->level + 1;
 	struct ucounts *ucounts;
@@ -88,9 +92,13 @@ static struct pid_namespace *create_pid_namespace(struct user_namespace *user_ns
 		goto out;
 
 	err = -ENOMEM;
+	depriv_context = kmem_cache_zalloc(depriv_ctx_cachep, GFP_KERNEL);
+	if (depriv_context == NULL)
+		goto out_dec;
+
 	ns = kmem_cache_zalloc(pid_ns_cachep, GFP_KERNEL);
 	if (ns == NULL)
-		goto out_dec;
+		goto out_free_depriv_context;
 
 	idr_init(&ns->idr);
 
@@ -109,12 +117,15 @@ static struct pid_namespace *create_pid_namespace(struct user_namespace *user_ns
 	ns->user_ns = get_user_ns(user_ns);
 	ns->ucounts = ucounts;
 	ns->pid_allocated = PIDNS_ADDING;
+	ns->depriv_context = depriv_context;
 
 	return ns;
 
 out_free_idr:
 	idr_destroy(&ns->idr);
 	kmem_cache_free(pid_ns_cachep, ns);
+out_free_depriv_context:
+	kmem_cache_free(depriv_ctx_cachep, depriv_context);
 out_dec:
 	dec_pid_namespaces(ucounts);
 out:
@@ -128,11 +139,14 @@ static void delayed_free_pidns(struct rcu_head *p)
 	dec_pid_namespaces(ns->ucounts);
 	put_user_ns(ns->user_ns);
 
+	kmem_cache_free(depriv_ctx_cachep, ns->depriv_context);
 	kmem_cache_free(pid_ns_cachep, ns);
 }
 
 static void destroy_pid_namespace(struct pid_namespace *ns)
 {
+	on_destroy_pid_ns(ns);
+
 	ns_free_inum(&ns->ns);
 
 	idr_destroy(&ns->idr);
@@ -450,6 +464,9 @@ const struct proc_ns_operations pidns_for_children_operations = {
 
 static __init int pid_namespaces_init(void)
 {
+	depriv_ctx_cachep = KMEM_CACHE(depriv_context, SLAB_PANIC | SLAB_ACCOUNT);
+	init_pid_ns.depriv_context = kmem_cache_zalloc(depriv_ctx_cachep, GFP_KERNEL);
+
 	pid_ns_cachep = KMEM_CACHE(pid_namespace, SLAB_PANIC | SLAB_ACCOUNT);
 
 #ifdef CONFIG_CHECKPOINT_RESTORE
diff --git a/tools/arch/x86/include/asm/cpufeatures.h b/tools/arch/x86/include/asm/cpufeatures.h
index d5b5f2ab87a0..a35d86ce8d00 100644
--- a/tools/arch/x86/include/asm/cpufeatures.h
+++ b/tools/arch/x86/include/asm/cpufeatures.h
@@ -230,6 +230,7 @@
 #define X86_FEATURE_FLEXPRIORITY	( 8*32+ 2) /* Intel FlexPriority */
 #define X86_FEATURE_EPT			( 8*32+ 3) /* Intel Extended Page Table */
 #define X86_FEATURE_VPID		( 8*32+ 4) /* Intel Virtual Processor ID */
+#define X86_FEATURE_SPP			( 8*32+ 5) /* Intel EPT-based Sub-Page Write Protection */
 
 #define X86_FEATURE_VMMCALL		( 8*32+15) /* Prefer VMMCALL to VMCALL */
 #define X86_FEATURE_XENPV		( 8*32+16) /* "" Xen paravirtual guest */
diff --git a/tools/testing/selftests/depriv/test_depriv.sh b/tools/testing/selftests/depriv/test_depriv.sh
new file mode 100755
index 000000000000..3bc4d01980c4
--- /dev/null
+++ b/tools/testing/selftests/depriv/test_depriv.sh
@@ -0,0 +1,19 @@
+sudo modprobe -r depriv-intel && sudo modprobe depriv-intel intercept_msr=1
+sudo modprobe -r depriv-intel && sudo modprobe depriv-intel intercept_cr3=1
+
+sudo modprobe -r depriv-intel && sudo modprobe depriv-intel exception_bitmap=0x4000
+sudo modprobe -r depriv-intel && sudo modprobe depriv-intel exception_bitmap=0x2000
+
+sudo modprobe -r depriv-intel && sudo modprobe depriv-intel test_handle_invalid_host_state=1
+sudo modprobe -r depriv-intel && sudo modprobe depriv-intel test_handle_invalid_guest_state=1
+
+sudo modprobe -r depriv-intel && sudo modprobe depriv-intel test_early_invalid_state=0 test_handle_invalid_host_state=1 debug_host_in_non_root_mode=1 && make -j5 modules
+sudo modprobe -r depriv-intel && sudo modprobe depriv-intel test_early_invalid_state=0 test_handle_invalid_guest_state=1 debug_host_in_non_root_mode=1 && make -j5 modules
+
+sudo modprobe -r depriv-intel && sudo modprobe depriv-intel call_extra_exit_handlers=0
+sudo modprobe smep-flipor && sudo modprobe -r smep-flipor
+sudo modprobe -r depriv-intel && sudo modprobe depriv-intel
+sudo modprobe smep-flipor && sudo modprobe -r smep-flipor
+sudo modprobe -r depriv-intel && sudo modprobe depriv-intel
+make -j5 modules
+sudo cp arch/x86/depriv/depriv-intel.ko /lib/modules/5.10.0-rc1+/kernel/arch/x86/depriv/
diff --git a/virt/depriv/Kconfig b/virt/depriv/Kconfig
new file mode 100644
index 000000000000..e8687b0165d6
--- /dev/null
+++ b/virt/depriv/Kconfig
@@ -0,0 +1,5 @@
+# SPDX-License-Identifier: GPL-2.0
+# Depriv common configuration items and defaults
+
+config HAVE_DEPRIV
+       bool
diff --git a/virt/depriv/depriv_main.c b/virt/depriv/depriv_main.c
new file mode 100644
index 000000000000..25a2e8d05e70
--- /dev/null
+++ b/virt/depriv/depriv_main.c
@@ -0,0 +1,7 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Deprivilege is to run Linux in hardware guest mode
+ *
+ * Authors:
+ * 	Xin Li <fantry@gmail.com>
+ */
diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c
index 504158f0e131..6ce518dd2103 100644
--- a/virt/kvm/kvm_main.c
+++ b/virt/kvm/kvm_main.c
@@ -5779,6 +5779,11 @@ EXPORT_SYMBOL_GPL(kvm_init);
 void kvm_exit(void)
 {
 	int cpu;
+#if IS_ENABLED(CONFIG_KVM_INTEL_DEPRIV) // XXX: hacky
+	raw_spin_lock(&kvm_count_lock);
+	kvm_usage_count--;
+	raw_spin_unlock(&kvm_count_lock);
+#endif
 
 	debugfs_remove_recursive(kvm_debugfs_dir);
 	misc_deregister(&kvm_dev);
