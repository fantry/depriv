From b630a2106676c26f9e1b2a617b074f0626233548 Mon Sep 17 00:00:00 2001
From: Xin Li <fantry@msn.com>
Date: Sat, 13 Jun 2020 19:10:04 -0700
Subject: [PATCH 020/140] refactor file structure

---
 arch/x86/kvm/Makefile           |    1 +
 arch/x86/kvm/vmx/depriv.c       | 1683 +++++++++++++++++++++++++++++++
 arch/x86/kvm/vmx/depriv.h       |    8 +
 arch/x86/kvm/vmx/depriv_entry.S |  122 +++
 arch/x86/kvm/vmx/vmenter.S      |  103 --
 arch/x86/kvm/vmx/vmx.c          | 1632 +-----------------------------
 6 files changed, 1820 insertions(+), 1729 deletions(-)
 create mode 100644 arch/x86/kvm/vmx/depriv.c
 create mode 100644 arch/x86/kvm/vmx/depriv.h
 create mode 100644 arch/x86/kvm/vmx/depriv_entry.S

diff --git a/arch/x86/kvm/Makefile b/arch/x86/kvm/Makefile
index 30f244b64523..85678e841df0 100644
--- a/arch/x86/kvm/Makefile
+++ b/arch/x86/kvm/Makefile
@@ -24,6 +24,7 @@ kvm-$(CONFIG_KVM_XEN)	+= xen.o
 kvm-intel-y		+= vmx/vmx.o vmx/vmenter.o vmx/pmu_intel.o vmx/vmcs12.o \
 			   vmx/evmcs.o vmx/nested.o vmx/posted_intr.o
 kvm-intel-$(CONFIG_X86_SGX_KVM)	+= vmx/sgx.o
+kvm-intel-$(CONFIG_KVM_INTEL_DEPRIV_HOST)	+= vmx/depriv.o vmx/depriv_entry.o
 
 kvm-amd-y		+= svm/svm.o svm/vmenter.o svm/pmu.o svm/nested.o svm/avic.o svm/sev.o
 
diff --git a/arch/x86/kvm/vmx/depriv.c b/arch/x86/kvm/vmx/depriv.c
new file mode 100644
index 000000000000..a2fc1fbcbd60
--- /dev/null
+++ b/arch/x86/kvm/vmx/depriv.c
@@ -0,0 +1,1683 @@
+// SPDX-License-Identifier: GPL-2.0-only
+/*
+ * Deprivilege is to run Linux kernel in VMX non-root mode
+ *
+ * Authors:
+ * 	Xin Li <fantry@gmail.com>
+ */
+
+#include <linux/kvm_host.h>
+
+#include <asm/debugreg.h>
+#include <asm/insn.h>
+
+#include "cpuid.h"
+#include "vmx.h"
+
+MODULE_AUTHOR("Xin Li");
+MODULE_LICENSE("GPL");
+
+/*
+ * host state memory buffer page order
+ */
+#define DEPRIV_CPU_STATE_PAGE_ORDER		1
+#define DEPRIV_CPU_STATE_BUFFER_SIZE		(PAGE_SIZE << DEPRIV_CPU_STATE_PAGE_ORDER)
+#define DEPRIV_CPU_STATE_VMCS_MSR_BITMAP	(DEPRIV_CPU_STATE_BUFFER_SIZE - PAGE_SIZE)
+
+/*
+ * needed to iret to root mode kernel or user space when the VM exit happened
+ */
+#define DEPRIV_HOST_STACK_RESERVED_BYTES (16 * 8)
+
+static struct vmcs_config depriv_vmcs_conf;
+static DEFINE_PER_CPU(struct vmcs *, depriv_vmcs);
+static DEFINE_PER_CPU(void *, depriv_cpu_state);
+
+// found_issue should be per-CPU */
+static bool found_issue = false;
+
+#define check(_c) do {							\
+	if (!(_c)) {							\
+		pr_err("depriv: guest state check failed (%d): %s\n",	\
+		       __LINE__, #_c);					\
+		found_issue = true;					\
+	}								\
+} while (0)
+
+#define CHECK_VMX_CTLS(ctls, val)					\
+	check((~(ctls) & ((u32)(val))) == 0 &&				\
+	      ((ctls) & ~((u32)((val) >> 32))) == 0)
+
+#define CHECK_HOST_SEG(seg)						\
+	check((vmcs_read16(HOST_##seg##_SELECTOR) & 0x7) == 0)
+
+static inline bool is_canonical_address(u64 la, u64 cr4)
+{
+	return get_canonical(la, cr4 & X86_CR4_LA57 ? 57 : 48) == la;
+}
+
+#define CHECK_IS_ADDR_CANONICAL(a, gh)					\
+	check(is_canonical_address(a, vmcs_readl(gh##_CR4)))
+
+#define CHECK_IS_HOST_ADDR_CANONICAL(a)					\
+	CHECK_IS_ADDR_CANONICAL(a, HOST)
+
+#define CHECK_IS_GUEST_ADDR_CANONICAL(a)				\
+	CHECK_IS_ADDR_CANONICAL(a, GUEST)
+
+#define CHECK_IS_VMCS_FIELD_ADDR_CANONICAL(vmcs_field, gh)		\
+	CHECK_IS_ADDR_CANONICAL(vmcs_readl(vmcs_field), gh)
+
+#define CHECK_IS_HOST_VMCS_FIELD_ADDR_CANONICAL(addr)			\
+	CHECK_IS_VMCS_FIELD_ADDR_CANONICAL(HOST_##addr, HOST)
+
+#define CHECK_IS_GUEST_VMCS_FIELD_ADDR_CANONICAL(addr)			\
+	CHECK_IS_VMCS_FIELD_ADDR_CANONICAL(GUEST_##addr, GUEST)
+
+#define CHECK_IS_HOST_TABLE_BASE_CANONICAL(tab)				\
+	CHECK_IS_HOST_VMCS_FIELD_ADDR_CANONICAL(tab##_BASE)
+
+#define CHECK_IS_GUEST_TABLE_BASE_CANONICAL(tab)			\
+	CHECK_IS_GUEST_VMCS_FIELD_ADDR_CANONICAL(tab##_BASE)
+
+#define PAGE_M (PAGE_SIZE - 1)
+
+static u32 vmx_segment_sel(u8 seg)
+{
+	switch (seg) {
+	case VCPU_SREG_ES:
+		return GUEST_ES_SELECTOR;
+	case VCPU_SREG_CS:
+		return GUEST_CS_SELECTOR;
+	case VCPU_SREG_SS:
+		return GUEST_SS_SELECTOR;
+	case VCPU_SREG_DS:
+		return GUEST_DS_SELECTOR;
+	case VCPU_SREG_FS:
+		return GUEST_FS_SELECTOR;
+	case VCPU_SREG_GS:
+		return GUEST_GS_SELECTOR;
+	case VCPU_SREG_TR:
+		return GUEST_TR_SELECTOR;
+	case VCPU_SREG_LDTR:
+		return GUEST_LDTR_SELECTOR;
+	default:
+		return GUEST_ES_SELECTOR;
+	}
+}
+
+static void vmx_check_guest_segment(u8 seg, bool vm86_active,
+				    bool long_mode_active, bool unrestricted)
+{
+	u32 sel = vmx_segment_sel(seg);
+	u16 selector = vmcs_read16(sel);
+	unsigned long base = vmcs_readl(sel + GUEST_ES_BASE - GUEST_ES_SELECTOR);
+	u32 limit = vmcs_read32(sel + GUEST_ES_LIMIT - GUEST_ES_SELECTOR);
+	u32 ar = vmcs_read32(sel + GUEST_ES_AR_BYTES - GUEST_ES_SELECTOR);
+	bool unusable = !!(ar & VMX_AR_UNUSABLE_MASK);
+	bool present = !!(ar & VMX_AR_P_MASK);
+	unsigned rpl = selector & SEGMENT_RPL_MASK;
+	unsigned ti = selector & SEGMENT_TI_MASK;
+	unsigned type = ar & VMX_AR_TYPE_MASK;
+	unsigned dpl = VMX_AR_DPL(ar);
+
+	if (unusable) {
+		pr_err("depriv: seg%d sel=0x%04x, attr=0x%05x, limit=0x%08x, base=0x%016lx\n",
+		       seg, selector, ar, limit, base);
+		return;
+	}
+
+	if (seg == VCPU_SREG_TR)
+		check(ti == 0);
+	else if (seg == VCPU_SREG_LDTR)
+		check(!present || ti == 0);
+	else if (seg == VCPU_SREG_SS)
+		check(vm86_active ||
+		      unrestricted ||
+		      rpl == (vmcs_read16(GUEST_CS_SELECTOR) & SEGMENT_RPL_MASK));
+
+	if (seg < VCPU_SREG_LDTR && vm86_active)
+		check(base == (unsigned long)(selector << 4));
+	if (seg == VCPU_SREG_TR || seg == VCPU_SREG_FS || seg == VCPU_SREG_GS)
+		CHECK_IS_GUEST_ADDR_CANONICAL(base);
+	else if (seg == VCPU_SREG_LDTR)
+		check(!present || is_canonical_address(base, vmcs_readl(GUEST_CR4)));
+	else if (seg == VCPU_SREG_CS)
+		check(!((u32)(base >> 32)));
+	else
+		check(!present || !((u32)(base >> 32)));
+
+	if (seg < VCPU_SREG_LDTR && vm86_active)
+		check(limit == 0xffff);
+
+	if (seg < VCPU_SREG_LDTR)
+		if (vm86_active)
+			check(ar == 0xF3);
+		else {
+			// Type
+			if (seg == VCPU_SREG_CS)
+				check((unrestricted && type == 3) ||
+				      (type & 9) == 9);
+			else if (seg == VCPU_SREG_SS)
+				check(!present || type == 3 || type == 7);
+			else if (present)
+				check((type & 1) == 1 &&
+				      ((type & 8) == 0 || (type & 2) == 2));
+			// S
+			if (seg == VCPU_SREG_TR)
+				check(!(ar & VMX_AR_S_MASK));
+			if (seg != VCPU_SREG_TR && present) {
+				check(ar & VMX_AR_S_MASK);
+			}
+			// DPL
+			if (seg == VCPU_SREG_CS)
+				if (type == 3) /* data segment => real mode */
+					check(dpl == 0);
+				else if ((type & 4) == 0) /* non-conforming code segment */
+					check(dpl == VMX_AR_DPL(vmcs_read32(GUEST_SS_AR_BYTES)));
+				else /* conforming code segment */
+					check(dpl <= VMX_AR_DPL(vmcs_read32(GUEST_SS_AR_BYTES)));
+			else if (seg == VCPU_SREG_SS)
+				if (!(vmcs_readl(GUEST_CR0) & X86_CR0_PE) ||
+				    (vmcs_read32(GUEST_CS_AR_BYTES) & VMX_AR_TYPE_MASK) == 3)
+					check(dpl == 0);
+				else
+					check(dpl == rpl);
+			else if (!unrestricted && present && type <= 11)
+				/* not a conforming code segment */
+				check(dpl >= rpl);
+			// P
+			if (seg == VCPU_SREG_CS || present)
+				check(present);
+			// reserved bits
+			if (seg == VCPU_SREG_CS || present)
+				check((ar & 0xfffe0f00) == 0);
+			// D/B
+			if (seg == VCPU_SREG_CS)
+				check(!long_mode_active || !(ar & VMX_AR_L_MASK) || !(ar & VMX_AR_DB_MASK));
+			// G
+			if (seg == VCPU_SREG_CS || present) {
+				check((limit & PAGE_M) == PAGE_M || !(ar & VMX_AR_G_MASK));
+				check(limit >> 20 == 0 || ar & VMX_AR_G_MASK);
+			}
+		}
+	else if (seg == VCPU_SREG_TR) {
+		check((!long_mode_active && type == 3) || type == 11);
+		check(!(ar & VMX_AR_S_MASK));
+		check(present);
+		check((ar & 0xfffe0f00) == 0);
+		check((limit & PAGE_M) == PAGE_M || !(ar & VMX_AR_G_MASK));
+		check(limit >> 20 == 0 || ar & VMX_AR_G_MASK);
+	} else if (seg == VCPU_SREG_LDTR && present) {
+		check(type == 2);
+		check(!(ar & VMX_AR_S_MASK));
+		check(present);
+		check((ar & 0xfffe0f00) == 0);
+		check((limit & PAGE_M) == PAGE_M || !(ar & VMX_AR_G_MASK));
+		check(limit >> 20 == 0 || ar & VMX_AR_G_MASK);
+	}
+}
+
+static void vmx_check_guest_state(void)
+{
+	u32 vmentry_ctl = vmcs_read32(VM_ENTRY_CONTROLS);
+	u32 vmexit_ctl = vmcs_read32(VM_EXIT_CONTROLS);
+	u32 cpu_based_exec_ctrl = vmcs_read32(CPU_BASED_VM_EXEC_CONTROL);
+	u32 pin_based_exec_ctrl = vmcs_read32(PIN_BASED_VM_EXEC_CONTROL);
+	u32 secondary_exec_control = cpu_has_secondary_exec_ctrls() ?
+		vmcs_read32(SECONDARY_VM_EXEC_CONTROL) : 0;
+	u64 rflags = vmcs_readl(GUEST_RFLAGS);
+	bool vm86_active = rflags & X86_EFLAGS_VM;
+	bool long_mode_active = vmentry_ctl & VM_ENTRY_IA32E_MODE;
+	u32 vmentry_intr_info = vmcs_read32(VM_ENTRY_INTR_INFO_FIELD);
+	bool unrestricted = cpu_has_secondary_exec_ctrls() ?
+		secondary_exec_control & SECONDARY_EXEC_UNRESTRICTED_GUEST :
+		false;
+
+	u64 cr0_must_be_zeros = ~read_msr(MSR_IA32_VMX_CR0_FIXED1);
+	u64 host_cr0_must_be_ones = read_msr(MSR_IA32_VMX_CR0_FIXED0);
+	u64 guest_cr0_must_be_ones = host_cr0_must_be_ones &
+		~(unrestricted ? X86_CR0_PG | X86_CR0_PE : 0);
+	u64 cr4_must_be_zeros = ~read_msr(MSR_IA32_VMX_CR4_FIXED1);
+	u64 cr4_must_be_ones = read_msr(MSR_IA32_VMX_CR4_FIXED0);
+
+	u64 pin_based_ctrls = read_msr(MSR_IA32_VMX_TRUE_PINBASED_CTLS);
+	u64 proc_based_ctrls = read_msr(MSR_IA32_VMX_TRUE_PROCBASED_CTLS);
+
+	u64 debug_ctrl = vmcs_read64(GUEST_IA32_DEBUGCTL);
+	u32 activity_state = vmcs_read32(GUEST_ACTIVITY_STATE);
+	u32 interruptibility = vmcs_read32(GUEST_INTERRUPTIBILITY_INFO);
+	u64 pending_dbg_exceptions = vmcs_readl(GUEST_PENDING_DBG_EXCEPTIONS);
+
+	u8 seg;
+
+	found_issue = false;
+
+	// 26.2.1.1: VM-Execution Control Fields
+	CHECK_VMX_CTLS(pin_based_exec_ctrl, pin_based_ctrls);
+	CHECK_VMX_CTLS(cpu_based_exec_ctrl, proc_based_ctrls);
+
+	if (cpu_based_exec_ctrl & CPU_BASED_ACTIVATE_SECONDARY_CONTROLS) {
+		u64 secondary_ctrls = read_msr(MSR_IA32_VMX_PROCBASED_CTLS2);
+		CHECK_VMX_CTLS(secondary_exec_control, secondary_ctrls);
+	}
+
+	check(vmcs_read32(CR3_TARGET_COUNT) == 0);
+
+	check(!(cpu_based_exec_ctrl & CPU_BASED_USE_IO_BITMAPS));
+	check(!(cpu_based_exec_ctrl & CPU_BASED_TPR_SHADOW));
+	check(pin_based_exec_ctrl & PIN_BASED_NMI_EXITING ||
+	      !(pin_based_exec_ctrl & PIN_BASED_VIRTUAL_NMIS));
+	check(pin_based_exec_ctrl & PIN_BASED_VIRTUAL_NMIS ||
+	      !(cpu_based_exec_ctrl & CPU_BASED_NMI_WINDOW_EXITING));
+	check(!(pin_based_exec_ctrl & PIN_BASED_POSTED_INTR));
+
+	// 26.2.1.2: VM-Exit Control Fields
+	CHECK_VMX_CTLS(vmexit_ctl, read_msr(MSR_IA32_VMX_TRUE_EXIT_CTLS));
+	check(pin_based_exec_ctrl & PIN_BASED_VMX_PREEMPTION_TIMER ||
+	      !(vmexit_ctl & VM_EXIT_SAVE_VMX_PREEMPTION_TIMER));
+
+	// 26.2.1.3: VM-Entry Control Fields
+	CHECK_VMX_CTLS(vmentry_ctl, read_msr(MSR_IA32_VMX_TRUE_ENTRY_CTLS));
+	if (vmentry_intr_info & INTR_INFO_VALID_MASK) {
+		u32 type = vmentry_intr_info & INTR_INFO_INTR_TYPE_MASK;
+		u8 vector = vmentry_intr_info & INTR_INFO_VECTOR_MASK;
+		u32 insn_len = vmcs_read32(VM_ENTRY_INSTRUCTION_LEN);
+		bool has_error_code = vmentry_intr_info & INTR_INFO_DELIVER_CODE_MASK;
+
+		check(type != INTR_TYPE_RESERVED);
+		check((u32)(proc_based_ctrls >> 32) & CPU_BASED_MONITOR_TRAP_FLAG ||
+		      type != INTR_TYPE_OTHER_EVENT);
+		check(type != INTR_TYPE_NMI_INTR || vector == BP_VECTOR);
+		check(type != INTR_TYPE_HARD_EXCEPTION || vector <= 31);
+		check(type != INTR_TYPE_OTHER_EVENT || vector == DE_VECTOR);
+
+		check(has_error_code == (vmcs_readl(GUEST_CR0) & X86_CR0_PE &&
+					 (type == INTR_TYPE_HARD_EXCEPTION &&
+					  (vector == DF_VECTOR ||
+					   vector == TS_VECTOR ||
+					   vector == NP_VECTOR ||
+					   vector == SS_VECTOR ||
+					   vector == GP_VECTOR ||
+					   vector == PF_VECTOR ||
+					   vector == AC_VECTOR))));
+
+		check(!(vmentry_intr_info &
+			(INTR_INFO_RESVD_BITS_MASK | INTR_INFO_UNBLOCK_NMI)));
+
+		check(!has_error_code ||
+		      (vmcs_read32(VM_ENTRY_EXCEPTION_ERROR_CODE) & 0xffff8000) == 0);
+		check(!(type == INTR_TYPE_SOFT_INTR ||
+			type == INTR_TYPE_PRIV_SW_EXCEPTION ||
+			type == INTR_TYPE_SOFT_EXCEPTION) ||
+		      insn_len < MAX_INSN_SIZE);
+	}
+
+	check(!(vmentry_ctl & VM_ENTRY_SMM));
+	check(!(vmentry_ctl & VM_ENTRY_DEACT_DUAL_MONITOR));
+
+	// 26.2.2: Checks on Host Control Registers and MSRs
+	check((~vmcs_readl(HOST_CR0) & cr0_must_be_zeros) == cr0_must_be_zeros);
+	check((vmcs_readl(HOST_CR0) & host_cr0_must_be_ones) == host_cr0_must_be_ones);
+
+	check((~vmcs_readl(HOST_CR4) & cr4_must_be_zeros) == cr4_must_be_zeros);
+	check((vmcs_readl(HOST_CR4) & cr4_must_be_ones) == cr4_must_be_ones);
+
+	CHECK_IS_HOST_VMCS_FIELD_ADDR_CANONICAL(IA32_SYSENTER_ESP);
+	CHECK_IS_HOST_VMCS_FIELD_ADDR_CANONICAL(IA32_SYSENTER_EIP);
+
+	if (vmexit_ctl & VM_EXIT_LOAD_IA32_PAT) {
+		u64 pat = vmcs_read64(HOST_IA32_PAT);
+		unsigned i;
+		for (i = 0; i < 8; i++) {
+			u8 byte = pat & 0xff;
+			check(byte < 8 && byte != 2 && byte != 3);
+			pat >>= 8;
+		}
+	}
+
+	if (vmexit_ctl & VM_EXIT_LOAD_IA32_EFER) {
+		u64 efer = vmcs_read64(HOST_IA32_EFER);
+		check((efer & 0xffffffffffff0200ull) == 0);
+		if (vmexit_ctl & VM_EXIT_HOST_ADDR_SPACE_SIZE)
+			check((efer & (EFER_LMA | EFER_LME)) ==
+			      (EFER_LMA | EFER_LME));
+		else
+			check((efer & (EFER_LMA | EFER_LME)) == 0);
+	}
+
+	// 26.2.3: Checks on Host Segment and Descriptor-Table Registers
+	CHECK_HOST_SEG(ES);
+	CHECK_HOST_SEG(CS);
+	CHECK_HOST_SEG(SS);
+	CHECK_HOST_SEG(DS);
+	CHECK_HOST_SEG(FS);
+	CHECK_HOST_SEG(GS);
+	CHECK_HOST_SEG(TR);
+
+	check(vmcs_read16(HOST_CS_SELECTOR) != 0);
+	check(vmcs_read16(HOST_TR_SELECTOR) != 0);
+
+	CHECK_IS_HOST_TABLE_BASE_CANONICAL(FS);
+	CHECK_IS_HOST_TABLE_BASE_CANONICAL(GS);
+	CHECK_IS_HOST_TABLE_BASE_CANONICAL(GDTR);
+	CHECK_IS_HOST_TABLE_BASE_CANONICAL(IDTR);
+	CHECK_IS_HOST_TABLE_BASE_CANONICAL(TR);
+
+	// 26.2.4: Checks Related to Address-Space Size
+	check(vmexit_ctl & VM_EXIT_HOST_ADDR_SPACE_SIZE);
+
+	check(vmcs_readl(HOST_CR4) & X86_CR4_PAE);
+	CHECK_IS_HOST_VMCS_FIELD_ADDR_CANONICAL(RIP);
+
+	// 26.3.1.1: Checks on Guest Control Registers, Debug Registers, and MSRs
+	check((~vmcs_readl(GUEST_CR0) & cr0_must_be_zeros) ==
+	      cr0_must_be_zeros);
+	check((vmcs_readl(GUEST_CR0) & guest_cr0_must_be_ones) ==
+	      guest_cr0_must_be_ones);
+
+	check((~vmcs_readl(GUEST_CR4) & cr4_must_be_zeros) == cr4_must_be_zeros);
+	check((vmcs_readl(GUEST_CR4) & cr4_must_be_ones) == cr4_must_be_ones);
+
+	if (vmentry_ctl & VM_ENTRY_LOAD_DEBUG_CONTROLS) {
+		u64 debug_ctrl_reserved = 0xffffffffffff003cull;
+		check((debug_ctrl & debug_ctrl_reserved) == 0);
+	}
+
+	check(!long_mode_active || vmcs_readl(GUEST_CR4) & X86_CR4_PAE);
+
+	if (vmentry_ctl & VM_ENTRY_LOAD_DEBUG_CONTROLS)
+		check(!((u32)(vmcs_readl(GUEST_DR7) >> 32)));
+
+	CHECK_IS_GUEST_VMCS_FIELD_ADDR_CANONICAL(SYSENTER_ESP);
+	CHECK_IS_GUEST_VMCS_FIELD_ADDR_CANONICAL(SYSENTER_EIP);
+
+	if (vmentry_ctl & VM_ENTRY_LOAD_IA32_PAT) {
+		u64 pat = vmcs_read64(GUEST_IA32_PAT);
+		unsigned i;
+		for (i = 0; i < 8; i++) {
+			u8 byte = pat & 0xff;
+			check(byte < 8 && byte != 2 && byte != 3);
+			pat >>= 8;
+		}
+	}
+
+	if (vmentry_ctl & VM_ENTRY_LOAD_IA32_EFER) {
+		u64 efer = vmcs_read64(GUEST_IA32_EFER);
+		check((efer & 0xffffffffffff0200ull) == 0);
+		if ((vmcs_readl(GUEST_CR0) & X86_CR0_PG) != 0) {
+			if (long_mode_active)
+				check((efer & (EFER_LMA | EFER_LME)) ==
+				      (EFER_LMA | EFER_LME));
+			else
+				check((efer & (EFER_LMA | EFER_LME)) == 0);
+		}
+	}
+
+	if (vmentry_ctl & VM_ENTRY_LOAD_BNDCFGS) {
+		u64 bndcfgs = vmcs_read64(GUEST_BNDCFGS);
+		CHECK_IS_GUEST_ADDR_CANONICAL(bndcfgs & 0xfffffffffffff000ull);
+		check((bndcfgs & 0x00000ffc) == 0);
+	}
+
+	// 26.3.1.2: Checks on Guest Segment Registers
+	for (seg = VCPU_SREG_ES; seg <= VCPU_SREG_LDTR; seg++)
+		vmx_check_guest_segment(seg, vm86_active, long_mode_active, unrestricted);
+
+	// 26.3.1.3: Checks on Guest Descriptor-Table Registers
+	CHECK_IS_GUEST_TABLE_BASE_CANONICAL(GDTR);
+	CHECK_IS_GUEST_TABLE_BASE_CANONICAL(IDTR);
+
+	// 26.3.1.4: Checks on Guest RIP and RFLAGS
+	check((long_mode_active && vmcs_read32(GUEST_CS_AR_BYTES) & VMX_AR_L_MASK) ||
+	      (u32)(vmcs_readl(GUEST_RIP) >> 32) == 0);
+	CHECK_IS_GUEST_VMCS_FIELD_ADDR_CANONICAL(RIP);
+
+	check((rflags & ((-1ull << 22) | (1 << 15) | (1 << 5) | (1 << 3))) == 0);
+	check((rflags & X86_EFLAGS_FIXED) != 0);
+	check(!long_mode_active || !vm86_active);
+	check(((vmentry_intr_info & (INTR_INFO_VALID_MASK |
+				     INTR_INFO_INTR_TYPE_MASK)) !=
+	       (INTR_INFO_VALID_MASK | INTR_TYPE_EXT_INTR)) ||
+	      (rflags & X86_EFLAGS_IF));
+
+	// 26.3.1.5: Checks on Guest Non-Register State
+	check(activity_state <= GUEST_ACTIVITY_WAIT_SIPI);
+	check(activity_state != GUEST_ACTIVITY_HLT ||
+	      VMX_AR_DPL(vmcs_read32(GUEST_SS_AR_BYTES)) == 0);
+	check(!(interruptibility & (GUEST_INTR_STATE_STI | GUEST_INTR_STATE_MOV_SS)) ||
+	      activity_state == GUEST_ACTIVITY_ACTIVE);
+	if (vmentry_intr_info & INTR_INFO_VALID_MASK) {
+		u32 type = vmentry_intr_info & INTR_INFO_INTR_TYPE_MASK;
+		u8 vector = vmentry_intr_info & INTR_INFO_VECTOR_MASK;
+
+		check(activity_state != GUEST_ACTIVITY_HLT ||
+		      (type == INTR_TYPE_EXT_INTR ||
+		       type == INTR_TYPE_NMI_INTR ||
+		       (type == INTR_TYPE_HARD_EXCEPTION &&
+			(vector == DB_VECTOR || vector == MC_VECTOR)) ||
+		       (type == INTR_TYPE_OTHER_EVENT && vector == DB_VECTOR)));
+		check(activity_state != GUEST_ACTIVITY_SHUTDOWN ||
+		      (type == INTR_TYPE_NMI_INTR ||
+		       (type == INTR_TYPE_HARD_EXCEPTION && vector == MC_VECTOR)));
+		check(activity_state != GUEST_ACTIVITY_WAIT_SIPI);
+	}
+
+	check((interruptibility & 0xFFFFFFE0) == 0);
+	check((interruptibility & (GUEST_INTR_STATE_STI | GUEST_INTR_STATE_MOV_SS)) !=
+	      (GUEST_INTR_STATE_STI | GUEST_INTR_STATE_MOV_SS));
+	check(rflags & X86_EFLAGS_IF || !(interruptibility & GUEST_INTR_STATE_STI));
+	check(((vmentry_intr_info & (INTR_INFO_VALID_MASK | INTR_INFO_INTR_TYPE_MASK)) !=
+	       (INTR_INFO_VALID_MASK | INTR_TYPE_EXT_INTR)) ||
+	      (interruptibility & (GUEST_INTR_STATE_STI | GUEST_INTR_STATE_MOV_SS)) == 0);
+	check(((vmentry_intr_info & (INTR_INFO_VALID_MASK | INTR_INFO_INTR_TYPE_MASK)) !=
+	       (INTR_INFO_VALID_MASK | INTR_TYPE_NMI_INTR)) ||
+	      (interruptibility & GUEST_INTR_STATE_MOV_SS) == 0);
+	check((interruptibility & GUEST_INTR_STATE_SMI) == 0);
+	// Some processors require the following check; others do not.
+	check(((vmentry_intr_info & (INTR_INFO_VALID_MASK | INTR_INFO_INTR_TYPE_MASK)) !=
+	       (INTR_INFO_VALID_MASK | INTR_TYPE_NMI_INTR)) ||
+	      (interruptibility & GUEST_INTR_STATE_STI) == 0);
+	check(((vmentry_intr_info & (INTR_INFO_VALID_MASK | INTR_INFO_INTR_TYPE_MASK)) !=
+	       (INTR_INFO_VALID_MASK | INTR_TYPE_NMI_INTR)) ||
+	      (pin_based_exec_ctrl & PIN_BASED_VIRTUAL_NMIS) == 0 ||
+	      (interruptibility & GUEST_INTR_STATE_NMI) == 0);
+
+	check((u32)(pending_dbg_exceptions >> 32) == 0);
+	check((pending_dbg_exceptions & 0xfffeaff0) == 0);
+	if ((interruptibility & (GUEST_INTR_STATE_STI | GUEST_INTR_STATE_MOV_SS)) != 0 ||
+	    activity_state == GUEST_ACTIVITY_HLT) {
+		check(!((rflags & X86_EFLAGS_TF) && !(debug_ctrl & 0x2)) ||
+		      (pending_dbg_exceptions & 0x00004000));
+		check(!(!(rflags & X86_EFLAGS_TF) || (debug_ctrl & 0x2)) ||
+		      !(pending_dbg_exceptions & 0x00004000));
+	}
+
+	if (!found_issue)
+		pr_info("depriv: validated VMCS guest state\n");
+}
+
+static __init int adjust_vmx_controls(u32 ctl_min, u32 ctl_opt,
+				      u32 msr, u32 *result)
+{
+	u32 vmx_msr_low, vmx_msr_high;
+	u32 ctl = ctl_min | ctl_opt;
+
+	rdmsr(msr, vmx_msr_low, vmx_msr_high);
+
+	ctl &= vmx_msr_high; /* bit == 0 in high word ==> must be zero */
+	ctl |= vmx_msr_low;  /* bit == 1 in low word  ==> must be one  */
+
+	/* Ensure minimum (required) set of control bits are supported. */
+	if (ctl_min & ~ctl)
+		return -EIO;
+
+	*result = ctl;
+	return 0;
+}
+
+static int __init setup_depriv_vmcs_config(void)
+{
+	u32 min, opt, min2, opt2;
+	u32 _pin_based_exec_control = 0;
+	u32 _cpu_based_exec_control = 0;
+	u32 _cpu_based_2nd_exec_control = 0;
+	u32 _vmexit_control = 0;
+	u32 _vmentry_control = 0;
+
+	memset(&depriv_vmcs_conf, 0, sizeof(depriv_vmcs_conf));
+	min = 0;
+	opt = CPU_BASED_USE_MSR_BITMAPS |
+	      CPU_BASED_ACTIVATE_SECONDARY_CONTROLS;
+	if (adjust_vmx_controls(min, opt, MSR_IA32_VMX_PROCBASED_CTLS,
+				&_cpu_based_exec_control) < 0)
+		return -EIO;
+
+	if (_cpu_based_exec_control & CPU_BASED_ACTIVATE_SECONDARY_CONTROLS) {
+		min2 = 0;
+		opt2 = SECONDARY_EXEC_RDTSCP |
+		       SECONDARY_EXEC_ENABLE_INVPCID |
+		       SECONDARY_EXEC_XSAVES;
+		if (adjust_vmx_controls(min2, opt2,
+					MSR_IA32_VMX_PROCBASED_CTLS2,
+					&_cpu_based_2nd_exec_control) < 0)
+			return -EIO;
+	}
+
+	if (_cpu_based_exec_control & CPU_BASED_CR3_LOAD_EXITING) {
+		// if EPT is available, it's OK to disable this control
+		_cpu_based_exec_control &= ~CPU_BASED_CR3_LOAD_EXITING;
+		pr_info("depriv: disabled cr3 load exiting\n");
+	}
+
+	if (_cpu_based_exec_control & CPU_BASED_CR3_STORE_EXITING) {
+		// if EPT is available, it's OK to disable this control
+		_cpu_based_exec_control &= ~CPU_BASED_CR3_STORE_EXITING;
+		pr_info("depriv: disabled cr3 store exiting\n");
+	}
+
+	if (_cpu_based_exec_control & CPU_BASED_INVLPG_EXITING) {
+		pr_info("depriv: invlpg causes VM exits\n");
+	}
+
+	min = VM_EXIT_SAVE_DEBUG_CONTROLS |
+	      VM_EXIT_HOST_ADDR_SPACE_SIZE;
+	opt = VM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL |
+	      VM_EXIT_LOAD_IA32_PAT |
+	      VM_EXIT_LOAD_IA32_EFER;
+	if (adjust_vmx_controls(min, opt, MSR_IA32_VMX_EXIT_CTLS,
+				&_vmexit_control) < 0)
+		return -EIO;
+
+	min = 0;
+	opt = 0;
+	if (adjust_vmx_controls(min, opt, MSR_IA32_VMX_PINBASED_CTLS,
+				&_pin_based_exec_control) < 0)
+		return -EIO;
+
+	if (!(_cpu_based_2nd_exec_control &
+		SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY))
+		_pin_based_exec_control &= ~PIN_BASED_POSTED_INTR;
+
+	min = VM_ENTRY_LOAD_DEBUG_CONTROLS |
+	      VM_ENTRY_IA32E_MODE;
+	opt = VM_ENTRY_LOAD_IA32_PERF_GLOBAL_CTRL |
+	      VM_ENTRY_LOAD_IA32_PAT |
+	      VM_ENTRY_LOAD_IA32_EFER;
+	if (adjust_vmx_controls(min, opt, MSR_IA32_VMX_ENTRY_CTLS,
+				&_vmentry_control) < 0)
+		return -EIO;
+
+	depriv_vmcs_conf.pin_based_exec_ctrl = _pin_based_exec_control;
+	depriv_vmcs_conf.cpu_based_exec_ctrl = _cpu_based_exec_control;
+	depriv_vmcs_conf.cpu_based_2nd_exec_ctrl = _cpu_based_2nd_exec_control;
+	depriv_vmcs_conf.vmexit_ctrl         = _vmexit_control;
+	depriv_vmcs_conf.vmentry_ctrl        = _vmentry_control;
+
+	pr_info("depriv: pin based controls: %#x\n",
+		depriv_vmcs_conf.pin_based_exec_ctrl);
+	pr_info("depriv: processor based controls: %#x\n",
+		depriv_vmcs_conf.cpu_based_exec_ctrl);
+	pr_info("depriv: processor based 2nd controls: %#x\n",
+		depriv_vmcs_conf.cpu_based_2nd_exec_ctrl);
+	pr_info("depriv: vm exit controls: %#x\n",
+		depriv_vmcs_conf.vmexit_ctrl);
+	pr_info("depriv: vm entry controls: %#x\n",
+		depriv_vmcs_conf.vmentry_ctrl);
+
+	return 0;
+}
+
+static void __init vmx_depriv_cpu_controls(void)
+{
+	u32 eb;
+
+	vmcs_write32(PIN_BASED_VM_EXEC_CONTROL,
+		     depriv_vmcs_conf.pin_based_exec_ctrl);
+	vmcs_write32(CPU_BASED_VM_EXEC_CONTROL,
+		     depriv_vmcs_conf.cpu_based_exec_ctrl);
+
+	if (cpu_has_secondary_exec_ctrls()) {
+		vmcs_write32(SECONDARY_VM_EXEC_CONTROL,
+		     depriv_vmcs_conf.cpu_based_2nd_exec_ctrl);
+	}
+
+	vmcs_write32(VM_EXIT_CONTROLS,
+		     depriv_vmcs_conf.vmexit_ctrl |
+		     VM_EXIT_HOST_ADDR_SPACE_SIZE);
+	vmcs_write32(VM_ENTRY_CONTROLS,
+		     depriv_vmcs_conf.vmentry_ctrl);
+
+	eb = (1u << UD_VECTOR) | (1u << DF_VECTOR) | (1u << GP_VECTOR);
+	vmcs_write32(EXCEPTION_BITMAP, eb);
+	vmcs_write32(PAGE_FAULT_ERROR_CODE_MASK, 0);
+	vmcs_write32(PAGE_FAULT_ERROR_CODE_MATCH, 0);
+	vmcs_write32(CR3_TARGET_COUNT, 0);
+
+	vmcs_write32(VM_EXIT_MSR_STORE_COUNT, 0);
+	vmcs_write32(VM_EXIT_MSR_LOAD_COUNT, 0);
+	vmcs_write64(VM_EXIT_MSR_LOAD_ADDR, 0);
+	vmcs_write32(VM_ENTRY_MSR_LOAD_COUNT, 0);
+	vmcs_write64(VM_ENTRY_MSR_LOAD_ADDR, 0);
+}
+
+#define DEPRIV_CR4_NON_ROOT_OWNED_BITS				      \
+	(X86_CR4_PVI | X86_CR4_DE | X86_CR4_PCE | X86_CR4_OSFXSR      \
+	 | X86_CR4_OSXMMEXCPT | X86_CR4_LA57 | X86_CR4_TSD)
+
+static void __init vmx_depriv_cpu_crs(void)
+{
+	unsigned long cr0, cr3, cr4;
+	u64 pat, efer;
+
+	cr0 = read_cr0();
+	vmcs_writel(HOST_CR0, cr0);
+	vmcs_writel(CR0_READ_SHADOW, cr0);
+	vmcs_writel(GUEST_CR0, cr0);
+
+	cr3 = __read_cr3();
+	vmcs_writel(HOST_CR3, cr3);
+	vmcs_writel(GUEST_CR3, cr3);
+
+	cr4 = __read_cr4();
+	vmcs_writel(HOST_CR4, cr4);
+	vmcs_writel(CR4_READ_SHADOW, cr4);
+	vmcs_writel(GUEST_CR4, cr4 | (X86_CR4_PAE | X86_CR4_VMXE));
+
+	vmcs_writel(CR0_GUEST_HOST_MASK, ~X86_CR0_TS);
+	vmcs_writel(CR4_GUEST_HOST_MASK, ~DEPRIV_CR4_NON_ROOT_OWNED_BITS);
+
+	pat = read_msr(MSR_IA32_CR_PAT);
+	vmcs_write64(HOST_IA32_PAT, pat);
+	vmcs_write64(GUEST_IA32_PAT, pat);
+
+	efer = read_msr(MSR_EFER);
+	vmcs_write64(HOST_IA32_EFER, efer);
+	vmcs_write64(GUEST_IA32_EFER, efer);
+
+	if (efer & EFER_LMA && efer & EFER_SCE) {
+		pr_debug("STAR=%016lx", read_msr(MSR_STAR));
+		pr_debug("LSTAR=%016lx", read_msr(MSR_LSTAR));
+		pr_debug("CSTAR=%016lx", read_msr(MSR_CSTAR));
+		pr_debug("syscall mask=%016lx", read_msr(MSR_SYSCALL_MASK));
+	}
+
+	if (cpu_has_load_perf_global_ctrl()) {
+		u64 perf_global_ctrl;
+		rdmsrl_safe(MSR_CORE_PERF_GLOBAL_CTRL, &perf_global_ctrl);
+		vmcs_write64(GUEST_IA32_PERF_GLOBAL_CTRL, perf_global_ctrl);
+		vmcs_write64(HOST_IA32_PERF_GLOBAL_CTRL, perf_global_ctrl);
+	}
+
+	if (false) // true: test code path handling vmlaunch caused VM-entry fail
+		vmcs_write32(CR3_TARGET_COUNT, 0x100000);
+}
+
+static inline bool __init is_desc_16byte(struct desc_struct *dentry)
+{
+	// s = 0 : system descriptor
+	return dentry->p && !dentry->s;
+}
+
+static inline u32 __init get_desc_limit_in_byte(struct desc_struct *dentry)
+{
+	u32 limit = get_desc_limit(dentry);
+	if (dentry->g)
+		limit = (limit << PAGE_SHIFT) | (PAGE_SIZE - 1);
+	return limit;
+}
+
+static inline void __init dump_desc_entry(struct desc_struct *dentry)
+{
+	int cpu = smp_processor_id();
+	bool is_16byte = is_desc_16byte(dentry);
+	u16 *entry = (u16 *)dentry;
+	u32 limit = get_desc_limit_in_byte(dentry);
+	unsigned long base = get_desc_base(dentry);
+
+	if (is_16byte) {
+		pr_info("depriv: cpu%d %04x %04x %04x %04x %04x %04x %04x %04x\n",
+			cpu, entry[0], entry[1], entry[2], entry[3],
+			entry[4], entry[5], entry[6], entry[7]);
+		base += (u64)(*((u32 *)(dentry + 1))) << 32;
+	} else {
+		pr_info("depriv: cpu%d %04x %04x %04x %04x\n",
+			cpu, entry[0], entry[1], entry[2], entry[3]);
+	}
+
+	pr_info("depriv: cpu%d type %x, S %x, DPL %x, P %x, AVL %x, "
+		"L %x, D %x, G %x, limit %#x, base %#lx\n",
+		cpu, dentry->type, dentry->s, dentry->dpl, dentry->p,
+		dentry->avl, dentry->l, dentry->d, dentry->g, limit, base);
+}
+
+static inline __init struct desc_struct *get_gdt_entry(unsigned long addr)
+{
+	struct desc_struct *dentry = (struct desc_struct *)addr;
+	if (false)
+		dump_desc_entry(dentry);
+	return dentry;
+}
+
+static inline u32 __init get_desc_ar(struct desc_struct *dentry,
+				     bool is_null, bool is_segment)
+{
+	int cpu = smp_processor_id();
+	u32 unusable = is_null ? 1 : 0; // 0 = usable; 1 = unusable
+	/*
+	 * 26.3.1.2 Checks on Guest Segment Registers, AR bytes:
+	 */
+	bool s = (unusable ? dentry->s :
+			     (is_segment ? 1 : 0));
+	u32 ar = dentry->type |
+		 (s ? 1 : 0) << 4 |
+		 dentry->dpl << 5 |
+		 dentry->p << 7 |
+		 dentry->avl << 12 |
+		 dentry->l << 13 |
+		 dentry->d << 14 |
+		 dentry->g << 15 |
+		 unusable << 16;
+	pr_debug("depriv: cpu%d entry ar %#x\n", cpu, ar);
+	return ar;
+}
+
+#define DEPRIV_SELECTOR(name, sel) {						\
+	pr_debug("depriv: cpu%d " #name " %#x\n", cpu, sel);			\
+	dentry = get_gdt_entry(gdt_base + sel);					\
+	base = get_desc_base(dentry);						\
+	if (is_desc_16byte(dentry))						\
+		base += (u64)(*((u32 *)(dentry + 1))) << 32;			\
+	vmcs_write16(GUEST_##name##_SELECTOR, sel);				\
+	vmcs_writel(GUEST_##name##_BASE, base);					\
+	vmcs_write32(GUEST_##name##_LIMIT, get_desc_limit_in_byte(dentry));	\
+	vmcs_write32(GUEST_##name##_AR_BYTES,					\
+		     get_desc_ar(dentry, sel == 0, is_segment));		\
+}
+
+#define DEPRIV_SEGMENT(SEG, fix_null_selector) {				\
+	u16 seg;								\
+	savesegment(SEG, seg);							\
+	if (fix_null_selector && seg == 0) seg = __KERNEL_DS;			\
+	vmcs_write16(HOST_##SEG##_SELECTOR, seg);				\
+	DEPRIV_SELECTOR(SEG, seg);						\
+}
+
+static void __init vmx_depriv_cpu_segments(unsigned long gdt_base)
+{
+	int cpu = smp_processor_id();
+	struct desc_struct *dentry;
+	unsigned long base;
+	bool is_segment = true;
+
+	DEPRIV_SEGMENT(CS, false);
+	DEPRIV_SEGMENT(DS, false);
+	DEPRIV_SEGMENT(ES, false);
+	DEPRIV_SEGMENT(SS, false);
+	DEPRIV_SEGMENT(FS, false);
+	DEPRIV_SEGMENT(GS, false);
+
+	base = read_msr(MSR_FS_BASE);
+	pr_info("depriv: cpu%d FS base MSR %#lx\n", cpu, base);
+	vmcs_writel(HOST_FS_BASE, base);
+	vmcs_writel(GUEST_FS_BASE, base);
+
+	base = read_msr(MSR_GS_BASE);
+	pr_info("depriv: cpu%d GS base MSR %#lx\n", cpu, base);
+	vmcs_writel(HOST_GS_BASE, base);
+	vmcs_writel(GUEST_GS_BASE, base);
+}
+
+static void __init vmx_depriv_cpu_ldtr(unsigned long gdt_base)
+{
+	int cpu = smp_processor_id();
+	struct desc_struct *dentry;
+	unsigned long base;
+	u16 ldtr;
+	bool is_segment = false;
+
+	store_ldt(ldtr);
+	DEPRIV_SELECTOR(LDTR, ldtr);
+}
+
+static void __init vmx_depriv_cpu_tr(unsigned long gdt_base)
+{
+	int cpu = smp_processor_id();
+	struct desc_struct *dentry;
+	unsigned long base, tss_base;
+	u16 tr;
+	u32 ar;
+	bool is_segment = false;
+
+	store_tr(tr);
+	if (tr != GDT_ENTRY_TSS*8)
+		pr_err("depriv: cpu%d tr selector mismatch %#x : %#x\n",
+		       cpu, tr, GDT_ENTRY_TSS*8);
+	vmcs_write16(HOST_TR_SELECTOR, tr);
+	DEPRIV_SELECTOR(TR, tr);
+	vmcs_writel(HOST_TR_BASE, base);
+	tss_base = (unsigned long)&get_cpu_entry_area(cpu)->tss.x86_tss;
+	if (base != tss_base)
+		pr_err("depriv: cpu%d tr base mismatch %#lx : %#lx\n",
+		       cpu, base, tss_base);
+
+	ar = vmcs_read32(GUEST_TR_AR_BYTES);
+	if ((ar & VMX_AR_TYPE_MASK) != VMX_AR_TYPE_BUSY_64_TSS) {
+		pr_err("%s: tss fixup for long mode\n", __func__);
+		vmcs_write32(GUEST_TR_AR_BYTES,
+			     (ar & ~VMX_AR_TYPE_MASK) |
+			     VMX_AR_TYPE_BUSY_64_TSS);
+	}
+}
+
+static void __init vmx_depriv_cpu_desc_tables(void)
+{
+	int cpu = smp_processor_id();
+	struct desc_ptr gdt, idt;
+	unsigned long gdt_base;
+
+	store_gdt(&gdt);
+	gdt_base = gdt.address;
+	if (gdt_base != (unsigned long)get_current_gdt_ro())
+		pr_err("depriv: cpu%d gdt base mismatch %#lx : %#lx\n",
+		       cpu, gdt_base, (unsigned long)get_current_gdt_ro());
+	vmcs_writel(HOST_GDTR_BASE, gdt_base);
+	vmcs_writel(GUEST_GDTR_BASE, gdt_base);
+	/* there is no host gdt limit */
+	vmcs_write32(GUEST_GDTR_LIMIT, gdt.size);
+
+	store_idt(&idt);
+	/* host should never handle interrupts */
+	vmcs_writel(HOST_IDTR_BASE, idt.address);
+	vmcs_writel(GUEST_IDTR_BASE, idt.address);
+	/* there is no host idt limit */
+	vmcs_write32(GUEST_IDTR_LIMIT, idt.size);
+
+	vmx_depriv_cpu_segments(gdt_base);
+	vmx_depriv_cpu_ldtr(gdt_base);
+	vmx_depriv_cpu_tr(gdt_base);
+}
+
+static void __init vmx_depriv_cpu_sysenter_msrs(void)
+{
+	u32 low32, high32;
+	unsigned long msr;
+
+	msr = read_msr(MSR_IA32_SYSENTER_ESP);
+	vmcs_writel(HOST_IA32_SYSENTER_ESP, msr);
+	vmcs_writel(GUEST_SYSENTER_ESP, msr);
+
+	rdmsr(MSR_IA32_SYSENTER_CS, low32, high32);
+	vmcs_write32(HOST_IA32_SYSENTER_CS, low32);
+	vmcs_write32(GUEST_SYSENTER_CS, low32);
+
+	msr = read_msr(MSR_IA32_SYSENTER_EIP);
+	vmcs_writel(HOST_IA32_SYSENTER_EIP, msr);
+	vmcs_writel(GUEST_SYSENTER_EIP, msr);
+}
+
+static void __init vmx_depriv_cpu_misc(void)
+{
+	unsigned long dr7;
+	u64 dbg_ctrl;
+
+	get_debugreg(dr7, 7);
+	vmcs_writel(GUEST_DR7, dr7);
+
+	dbg_ctrl = read_msr(MSR_IA32_DEBUGCTLMSR);
+	vmcs_write64(GUEST_IA32_DEBUGCTL, dbg_ctrl);
+}
+
+/*
+ * sync host states to guest states
+ */
+static void __init vmx_depriv_cpu_state(void)
+{
+	vmx_depriv_cpu_controls();
+	vmx_depriv_cpu_crs();
+	vmx_depriv_cpu_desc_tables();
+	vmx_depriv_cpu_sysenter_msrs();
+	vmx_depriv_cpu_misc();
+}
+
+static void vmx_repriv_cpu_release_resources(void)
+{
+	int cpu = smp_processor_id();
+	void *host_cpu_state = per_cpu(depriv_cpu_state, cpu);
+	struct vmcs *vmcs = per_cpu(depriv_vmcs, cpu);
+
+	pr_info("depriv: reprivilege cpu%d releasing resources\n", cpu);
+
+	if (host_cpu_state) {
+		per_cpu(depriv_cpu_state, cpu) = NULL;
+		memset(host_cpu_state, 0, DEPRIV_CPU_STATE_BUFFER_SIZE);
+		free_pages((unsigned long)host_cpu_state, DEPRIV_CPU_STATE_PAGE_ORDER);
+	}
+
+	if (vmcs) {
+		per_cpu(depriv_vmcs, cpu) = NULL;
+		vmcs_clear(vmcs);
+		free_vmcs(vmcs);
+	}
+}
+
+void vmx_depriv_vmexit(void);
+int vmx_depriv(unsigned long root_rsp);
+void vmx_depriv_rip(void);
+void vmx_depriv_vmcall(void);
+
+static void vmx_depriv_cpu_intercept_msr(u32 msr, bool enable)
+{
+	u32 orig_msr = msr;
+	int cpu = smp_processor_id();
+	void *msr_bitmap = per_cpu(depriv_cpu_state, cpu) +
+			   DEPRIV_CPU_STATE_VMCS_MSR_BITMAP;
+	int f = sizeof(unsigned long);
+	bool set_result = false;
+
+	if (msr <= 0x1fff) {
+		if (enable)
+			__set_bit(msr, msr_bitmap + 0x800 / f);
+		else
+			__clear_bit(msr, msr_bitmap + 0x800 / f);
+		set_result = !!test_bit(msr, msr_bitmap + 0x800 / f);
+	} else if (msr >= 0xc0000000 && msr <= 0xc0001fff) {
+		msr &= 0x1fff;
+		if (enable)
+			__set_bit(msr, msr_bitmap + 0xc00 / f);
+		else
+			__clear_bit(msr, msr_bitmap + 0xc00 / f);
+		set_result = !!test_bit(msr, msr_bitmap + 0xc00 / f);
+	} else {
+		pr_err("depriv: cpu%d set MSR bitmap @ %#x out of scope\n",
+		       cpu, orig_msr);
+		return;
+	}
+
+	if (set_result != enable)
+		pr_err("depriv: cpu%d set MSR bitmap @ %#x failed\n",
+		       cpu, orig_msr);
+}
+
+#define DEPRIV_SET_ROOT_MODE_TRAMPOLINE_STACK(ip_off) do {			\
+	*(unsigned long *)(root_rsp + 0x0) = rip + (ip_off);			\
+	*(unsigned long *)(root_rsp + 0x8) = vmcs_read16(GUEST_CS_SELECTOR);	\
+	*(unsigned long *)(root_rsp + 0x10) = vmcs_readl(GUEST_RFLAGS);		\
+	*(unsigned long *)(root_rsp + 0x18) = vmcs_readl(GUEST_RSP);		\
+	*(unsigned long *)(root_rsp + 0x20) = vmcs_read16(GUEST_SS_SELECTOR);	\
+	*(unsigned long *)(root_rsp + 0x28) = vmcs_readl(GUEST_CR3);		\
+	*(unsigned long *)(root_rsp + 0x30) = vmcs_readl(GUEST_GS_BASE);	\
+	*(unsigned long *)(root_rsp + 0x38) = vmcs_readl(GUEST_FS_BASE);	\
+} while (0)
+
+static void __init vmx_depriv_cpu(void *info)
+{
+	int cpu = smp_processor_id();
+	int node = cpu_to_node(cpu);
+	struct vmcs *vmcs = NULL;
+	struct page *page = NULL;
+	void *host_cpu_state = NULL;
+	void *msr_bitmap = NULL;
+	unsigned long root_rsp, rip, rsp, rflags;
+	int vmx_depriv_result;
+
+	if (!(depriv_vmcs_conf.cpu_based_exec_ctrl & CPU_BASED_USE_MSR_BITMAPS)) {
+		pr_err("depriv: MSR bitmap not available on cpu%d\n", cpu);
+		goto error;
+	}
+
+	vmcs = alloc_vmcs_cpu(false, cpu, GFP_KERNEL);
+	if (!vmcs) {
+		pr_err("depriv: unable to allocate VMCS for cpu%d\n", cpu);
+		goto error;
+	}
+
+	vmcs_clear(vmcs);
+	vmcs_load(vmcs);
+	vmcs_write64(VMCS_LINK_POINTER, ~0ull);
+	indirect_branch_prediction_barrier();
+	per_cpu(depriv_vmcs, cpu) = vmcs;
+
+	// memory for root mode VM exit handler
+	page = __alloc_pages_node(node, GFP_KERNEL, DEPRIV_CPU_STATE_PAGE_ORDER);
+	if (!page) {
+		pr_err("depriv: unable to allocate host state buffer for cpu%d\n", cpu);
+		goto error;
+	}
+
+	host_cpu_state = page_address(page);
+	memset(host_cpu_state, 0, DEPRIV_CPU_STATE_BUFFER_SIZE);
+	per_cpu(depriv_cpu_state, cpu) = host_cpu_state;
+
+	// last page of host state
+	msr_bitmap = host_cpu_state + DEPRIV_CPU_STATE_VMCS_MSR_BITMAP;
+	memset(msr_bitmap, 0xff, PAGE_SIZE);
+	vmx_depriv_cpu_intercept_msr(MSR_FS_BASE, true);
+	vmx_depriv_cpu_intercept_msr(MSR_GS_BASE, true);
+	vmx_depriv_cpu_intercept_msr(MSR_KERNEL_GS_BASE, true);
+	vmcs_write64(MSR_BITMAP, __pa(msr_bitmap));
+
+	vmx_depriv_cpu_state();
+
+	vmcs_writel(HOST_RIP, (unsigned long)vmx_depriv_vmexit);
+	// reserve extra DEPRIV_HOST_STACK_RESERVED_BYTES bytes for reprivileging host
+	root_rsp = (unsigned long)msr_bitmap - DEPRIV_HOST_STACK_RESERVED_BYTES;
+	vmcs_writel(HOST_RSP, root_rsp);
+
+	/* switching to non-root mode */
+	rip = (unsigned long)vmx_depriv_rip;
+	vmcs_writel(GUEST_RIP, rip);
+	asm volatile("mov %%rsp,%0" : "=m"(rsp));
+	// reserve extra 8 bytes for RIP pushed to stack when calling vmx_depriv
+	rsp -= 8;
+	vmcs_writel(GUEST_RSP, rsp);
+
+	asm volatile("xor %%rax,%%rax\n\t"
+		     "pushf\n\t"
+		     "pop %%rax\n\t"
+		     "mov %%rax,%0"
+		     : "=m"(rflags) :: "%rax");
+	vmcs_writel(GUEST_RFLAGS, rflags & ~X86_EFLAGS_IF);
+
+	DEPRIV_SET_ROOT_MODE_TRAMPOLINE_STACK(0);
+
+	pr_info("depriv: deprivileging cpu%d: rip=%#lx rsp=%#lx\n",
+		cpu, rip, rsp);
+
+	if (false) // true: test code path handling vmresume caused VM-entry fail
+		vmcs_write32(GUEST_TR_AR_BYTES, 0x009b);
+
+	/*
+	 * Should we save/restore general purpose registers around vmx_depriv?
+	 * Yes, but only restore them when there was a successful vmentry.
+	 */
+	vmx_depriv_result = vmx_depriv(root_rsp);
+	if (!vmx_depriv_result) {
+		// continue in non-root mode...
+		asm volatile("mov %%rsp,%0" : "=m"(rsp));
+		asm volatile("xor %%rax,%%rax\n\t"
+			     "pushf\n\t"
+			     "pop %%rax\n\t"
+			     "mov %%rax,%0"
+			     : "=m"(rflags) :: "%rax");
+		pr_info("depriv: cpu%d deprivileged: rsp=%#lx  rflags=%#lx\n",
+			cpu, rsp, rflags);
+
+		wrmsrl(MSR_FS_BASE, read_msr(MSR_FS_BASE));
+		wrmsrl(MSR_GS_BASE, read_msr(MSR_GS_BASE));
+		wrmsrl(MSR_KERNEL_GS_BASE, read_msr(MSR_KERNEL_GS_BASE));
+		vmx_depriv_vmcall();
+		return;
+	}
+
+	// still in root mode
+	if (vmx_depriv_result == 1)
+		pr_err("depriv: launch failed on cpu%d\n", cpu);
+	else if (vmx_depriv_result == 2)
+		pr_err("depriv: resume failed on cpu%d\n", cpu);
+
+error:
+	vmx_repriv_cpu_release_resources();
+}
+
+static void vmx_repriv_cpu_crs(void)
+{
+	int cpu = smp_processor_id();
+	unsigned long host_cr0 = read_cr0();
+	unsigned long host_cr3 = __read_cr3();
+	unsigned long host_cr4 = __read_cr4();
+	unsigned long guest_cr0 = vmcs_readl(GUEST_CR0);
+	unsigned long guest_cr3 = vmcs_readl(GUEST_CR3);
+	unsigned long guest_cr4 = vmcs_readl(GUEST_CR4);
+
+	if (host_cr0 != guest_cr0) {
+		pr_info("depriv: repriv cpu%d cr0 %#lx : %#lx : %#lx\n",
+			cpu, host_cr0, vmcs_readl(HOST_CR0), guest_cr0);
+		write_cr0(guest_cr0);
+		vmcs_writel(HOST_CR0, guest_cr0);
+	}
+
+	if (host_cr3 != guest_cr3) {
+		pr_info("depriv: repriv cpu%d cr3 %#lx : %#lx\n",
+			cpu, host_cr3, guest_cr3);
+	}
+
+	if (host_cr4 != guest_cr4) {
+		pr_info("depriv: repriv cpu%d cr4 %#lx : %#lx : %#lx\n",
+			cpu, host_cr4, vmcs_readl(HOST_CR4), guest_cr4);
+		vmcs_writel(HOST_CR4, guest_cr4);
+	}
+}
+
+static inline void vmx_repriv_cpu_sysenter_msrs(void)
+{
+	wrmsrl(MSR_IA32_SYSENTER_ESP, vmcs_readl(GUEST_SYSENTER_ESP));
+	wrmsr(MSR_IA32_SYSENTER_CS, vmcs_read32(GUEST_SYSENTER_CS), 0);
+	wrmsrl(MSR_IA32_SYSENTER_EIP, vmcs_readl(GUEST_SYSENTER_EIP));
+}
+
+static inline void vmx_repriv_cpu_misc(void)
+{
+	set_debugreg(vmcs_readl(GUEST_DR7), 7);
+	wrmsrl(MSR_IA32_DEBUGCTLMSR, vmcs_read64(GUEST_IA32_DEBUGCTL));
+}
+
+#define REPRIV_SEGMENT(tag, TAG) do {					\
+	ar = vmcs_read32(GUEST_##TAG##S_AR_BYTES);			\
+	if (ar & VMX_AR_UNUSABLE_MASK) {				\
+		pr_info("depriv: cpu%d " #TAG "S unusable\n", cpu);	\
+		break;							\
+	}								\
+	sel = vmcs_read16(GUEST_##TAG##S_SELECTOR);			\
+	loadsegment(tag##s, sel);					\
+	pr_debug("depriv: cpu%d " #TAG "S %#x\n", cpu, sel);		\
+} while (0)
+
+static inline void vmx_repriv_cpu_segments(void)
+{
+	int cpu = smp_processor_id();
+	unsigned long host_base, guest_base;
+	u32 ar;
+	u16 sel;
+
+	REPRIV_SEGMENT(d, D);
+	REPRIV_SEGMENT(e, E);
+
+	ar = vmcs_read32(GUEST_FS_AR_BYTES);
+	if ((ar >> 16) & 0x1) {
+		pr_info("depriv: repriv cpu%d FS unusable\n", cpu);
+	} else {
+		sel = vmcs_read16(GUEST_FS_SELECTOR);
+		loadsegment(fs, sel);
+		pr_info("depriv: repriv cpu%d FS = %#x\n", cpu, sel);
+	}
+
+	host_base = read_msr(MSR_FS_BASE);
+	guest_base = vmcs_readl(GUEST_FS_BASE);
+	pr_info("depriv: repriv cpu%d FS base %#lx : %#lx\n",
+		cpu, host_base, guest_base);
+
+	// never change GS BASE, which points to kernel mode per-CPU data
+	ar = vmcs_read32(GUEST_GS_AR_BYTES);
+	if ((ar >> 16) & 0x1) {
+		pr_info("depriv: repriv cpu%d GS unusable\n", cpu);
+	} else {
+		sel = vmcs_read16(GUEST_FS_SELECTOR);
+		load_gs_index(sel);
+		pr_info("depriv: repriv cpu%d FS %#x\n", cpu, sel);
+	}
+
+	host_base = read_msr(MSR_GS_BASE);
+	guest_base = vmcs_readl(GUEST_GS_BASE);
+	pr_info("depriv: repriv cpu%d GS base %#lx : %#lx\n",
+		cpu, host_base, guest_base);
+}
+
+static inline void vmx_repriv_cpu_ldtr(void)
+{
+	int cpu = smp_processor_id();
+	u16 guest_ldtr = vmcs_read16(GUEST_LDTR_SELECTOR), host_ldtr;
+
+	store_ldt(host_ldtr);
+	if (host_ldtr != guest_ldtr) {
+		pr_info("depriv: repriv cpu%d LDTR mismatch %#x : %#x\n",
+			cpu, host_ldtr, guest_ldtr);
+		load_ldt(guest_ldtr);
+	}
+}
+
+static inline void vmx_repriv_cpu_tr(void)
+{
+	int cpu = smp_processor_id();
+	u16 guest_tr = vmcs_read16(GUEST_TR_SELECTOR), host_tr;
+
+	store_tr(host_tr);
+	if (host_tr != guest_tr) {
+		pr_info("depriv: repriv cpu%d TR mismatch %#x : %#x\n",
+			cpu, host_tr, guest_tr);
+		if (guest_tr == 0)
+			return;
+		load_tr(guest_tr);
+		vmcs_write16(HOST_TR_SELECTOR, guest_tr);
+	}
+}
+
+#define REPRIV_DESC_TABLE(tag, TAG) do {						\
+	store_##tag##dt(&host_dt);							\
+	guest_dt_base = vmcs_readl(GUEST_##TAG##DTR_BASE);				\
+	if (host_dt.address != guest_dt_base)						\
+		pr_err("depriv: repriv cpu%d " #tag "dt base mismatch %#lx : %#lx\n",	\
+		       cpu, host_dt.address, guest_dt_base);				\
+	vmcs_writel(HOST_##TAG##DTR_BASE, guest_dt_base);				\
+	guest_dt_limit = vmcs_read32(GUEST_##TAG##DTR_LIMIT);				\
+	if (host_dt.size != guest_dt_limit) {						\
+		pr_debug("depriv: repriv cpu%d " #tag "dt limit mismatch %#x : %#x\n",	\
+			 cpu, host_dt.size , guest_dt_limit);				\
+		host_dt.size = guest_dt_limit;						\
+		load_##tag##dt(&host_dt);						\
+	}										\
+} while (0)
+
+static inline void vmx_repriv_cpu_desc_tables(void)
+{
+	int cpu = smp_processor_id();
+	struct desc_ptr host_dt;
+	unsigned long guest_dt_base;
+	u32 guest_dt_limit;
+
+	REPRIV_DESC_TABLE(g, G);
+	REPRIV_DESC_TABLE(i, I);
+
+	vmx_repriv_cpu_segments();
+	vmx_repriv_cpu_ldtr();
+	vmx_repriv_cpu_tr();
+}
+
+static void vmx_repriv_cpu(void *info)
+{
+	/* trigger a vmcall vmexit to reprivilege */
+#if 0
+	asm volatile("push %%rbp; vmcall; pop %%rbp" : : :
+		     "rax", "rcx", "rdx", "rbx",
+		     "rsi", "rdi", "r8", "r9", "r10",
+		     "r11", "r12", "r13", "r14", "r15");
+#endif
+
+	vmx_repriv_cpu_release_resources();
+}
+
+void vmx_repriv_host(void)
+{
+	on_each_cpu(vmx_repriv_cpu, NULL, 0);
+	pr_info("depriv: reprivileged host\n");
+}
+
+/*
+ * sync guest state to host w/o changing guest state
+ */
+static void vmx_repriv_cpu_state(void)
+{
+	vmx_repriv_cpu_crs();
+	vmx_repriv_cpu_misc();
+	vmx_repriv_cpu_sysenter_msrs();
+	vmx_repriv_cpu_desc_tables();
+}
+
+#define DEPRIV_CONTINUE_IN_NON_ROOT_MODE(ins_len) do {				\
+	DEPRIV_SET_ROOT_MODE_TRAMPOLINE_STACK(ins_len);				\
+	vmcs_writel(GUEST_RIP, rip + ins_len);					\
+	return true;								\
+} while (0)
+
+#define DEPRIV_CONTINUE_IN_ROOT_MODE(ins_len) do {				\
+	vmx_repriv_cpu_state();							\
+	dump_vmcs();								\
+	pr_info("depriv: cpu%d exit reason %d skip %d bytes instruction and "	\
+		"continue in root mode\n",					\
+		cpu, reason, ins_len);						\
+	DEPRIV_SET_ROOT_MODE_TRAMPOLINE_STACK(ins_len);				\
+	return false;								\
+} while (0)
+
+#define DEPRIV_DUMP_GPRS do {							\
+	int i;									\
+	pr_info("depriv: cpu%d guest GPRs:\n", cpu);				\
+	for (i = __VCPU_REGS_RAX; i <= __VCPU_REGS_R15; i += 4) {		\
+		unsigned long *r = &regs[i];					\
+		pr_info("\tcpu%d\t%016lx %016lx %016lx %016lx\n",		\
+			cpu, *r, *(r + 1), *(r + 2), *(r + 3));			\
+	}									\
+} while (0)
+
+/*
+ * if guest_rip is a user level virtual address then it's mostly not valid in
+ * root mode, because it is mapped using non-root mode cr3 and page tables.
+ */
+static void dump_guest_insn(unsigned long guest_rip, int insn_len, char *insn)
+{
+	int i;
+
+	// don't try to access user level virtual address
+	if (!(guest_rip & 0xf000000000000000ul)) {
+		memset(insn, 0, MAX_INSN_SIZE * 3 + 1);
+		return;
+	}
+
+	if (insn_len == 0)
+		insn_len = MAX_INSN_SIZE;
+
+	for (i = 0; i < insn_len; i++)
+		sprintf(insn + 3 * i, " %02x", *(u8 *)(guest_rip + i));
+	insn[3 * i] = '\0';
+}
+
+static unsigned long cnt = 0;
+
+/*
+ * the following fs base sync logic is confusing, but it happens on nested
+ */
+static void dump_fsgs_base(unsigned long root_rsp)
+{
+	int cpu = smp_processor_id();
+	unsigned long base, last_base;
+
+	base = vmcs_readl(GUEST_FS_BASE);
+	last_base = *(unsigned long *)(root_rsp + 0x38);
+	if (base != last_base)
+		pr_info("depriv: cpu%d (%ld) FS base %#lx <= %#lx\n",
+			cpu, cnt, base, last_base);
+
+	base = vmcs_readl(GUEST_GS_BASE);
+	last_base = *(unsigned long *)(root_rsp + 0x30);
+	if (base != last_base)
+		pr_info("depriv: cpu%d (%ld) GS base %#lx <= %#lx\n",
+			cpu, cnt, base, last_base);
+}
+
+bool vmx_depriv_vmexit_handler(unsigned long *regs)
+{
+	unsigned long root_rsp = vmcs_readl(HOST_RSP);
+	unsigned long rip = vmcs_readl(GUEST_RIP);
+	unsigned long rsp = vmcs_readl(GUEST_RSP);
+	char insn[64];
+	int cpu = smp_processor_id();
+	u32 reason = vmcs_read32(VM_EXIT_REASON), insn_len = 0;
+	static bool enable_log = false;
+
+	++cnt;
+
+	regs[__VCPU_REGS_RSP] = rsp;
+
+	dump_fsgs_base(root_rsp);
+
+	pr_debug("depriv: cpu%d exit reason:%#x rip:%#lx rsp:%#lx\n",
+		 cpu, reason, rip, rsp);
+	if (enable_log) {
+		pr_info("depriv: cpu%d (%ld) exit reason:%#x rip:%#lx rsp:%#lx\n",
+			cpu, cnt, reason, rip, rsp);
+		enable_log = false;
+	}
+
+	if (reason & VMX_EXIT_REASONS_FAILED_VMENTRY) {
+		dump_vmcs();
+		vmx_check_guest_state();
+		// no need to sync guest state to host if we never enter guest
+		DEPRIV_CONTINUE_IN_ROOT_MODE(0);
+	}
+
+	switch (reason) {
+	case EXIT_REASON_EXCEPTION_NMI: {
+		bool continue_in_root_mode = true;
+		u32 intr_info = vmcs_read32(VM_EXIT_INTR_INFO);
+		u8 vector = intr_info & INTR_INFO_VECTOR_MASK;
+		u32 error_code = 0;
+
+		if (intr_info & INTR_INFO_DELIVER_CODE_MASK)
+			error_code = vmcs_read32(VM_EXIT_INTR_ERROR_CODE);
+
+		pr_info("depriv: cpu%d intr info: %#x error code %#x\n",
+			cpu, intr_info, error_code);
+
+		insn_len = vmcs_read32(VM_EXIT_INSTRUCTION_LEN);
+		dump_guest_insn(rip, insn_len, insn);
+
+		pr_info("depriv: cpu%d hit exception %d @ rip %#lx insn: %s\n",
+			cpu, vector, rip, insn);
+
+		if (is_invalid_opcode(intr_info)) {
+			pr_info("depriv: cpu%d hit UD @ rip %#lx insn: %s\n",
+				cpu, rip, insn);
+		} else if (is_page_fault(intr_info)) {
+			pr_info("depriv: cpu%d page fault @ %#lx with error code %#x\n",
+				cpu, vmcs_readl(EXIT_QUALIFICATION), error_code);
+			continue_in_root_mode = false;
+		} else if (is_gp_fault(intr_info)) {
+			continue_in_root_mode = false;
+		} else if (is_machine_check(intr_info))
+			pr_info("depriv: cpu%d to handle machine check in root mode\n", cpu);
+		else if (is_machine_check(intr_info) || is_nmi(intr_info))
+			pr_info("depriv: cpu%d to handle NMI in root mode\n", cpu);
+
+		if (continue_in_root_mode) {
+			pr_info("depriv: cpu%d hit exception %d, continue in root mode\n",
+				cpu, vector);
+			DEPRIV_CONTINUE_IN_ROOT_MODE(0);
+		} else {
+			u32 intr;
+
+			intr = vector | INTR_INFO_VALID_MASK;
+			if (vector == DF_VECTOR || vector == GP_VECTOR || vector == PF_VECTOR)
+				intr |= INTR_TYPE_HARD_EXCEPTION;
+			if (vector == GP_VECTOR || vector == PF_VECTOR)
+				intr |= INTR_INFO_DELIVER_CODE_MASK;
+			vmcs_write32(VM_ENTRY_INTR_INFO_FIELD, intr);
+			vmcs_write32(VM_ENTRY_INSTRUCTION_LEN, insn_len);
+			vmcs_write32(VM_ENTRY_EXCEPTION_ERROR_CODE, error_code);
+			pr_info("depriv: cpu%d injecting exception %d, continue in non-root mode\n",
+				cpu, vector);
+
+			pr_info("depriv: cpu%d (%ld) FS base %#lx : %#lx : %#lx\n",
+				cpu, cnt, read_msr(MSR_FS_BASE),
+				vmcs_readl(HOST_FS_BASE), vmcs_readl(GUEST_FS_BASE));
+
+			pr_info("depriv: cpu%d (%ld) GS base %#lx : %#lx : %#lx : %#lx\n",
+				cpu, cnt, read_msr(MSR_GS_BASE), vmcs_readl(HOST_GS_BASE),
+				vmcs_readl(GUEST_GS_BASE), read_msr(MSR_KERNEL_GS_BASE));
+
+			DEPRIV_DUMP_GPRS;
+			enable_log = true;
+			//dump_vmcs();
+			vmx_check_guest_state();
+
+			DEPRIV_CONTINUE_IN_NON_ROOT_MODE(0);
+		}
+	}
+
+	case EXIT_REASON_CR_ACCESS: {
+		unsigned long qualification = vmcs_readl(EXIT_QUALIFICATION);
+		int cr = qualification & 0xf;
+		int reg = (qualification >> 8) & 0xf;
+		unsigned long cr4 = vmcs_readl(GUEST_CR4);
+
+		if (cr != 3)
+			DEPRIV_CONTINUE_IN_ROOT_MODE(0);
+
+		if (cr4 & X86_CR4_PCIDE)
+			pr_info("depriv: cpu%d PCID enabled\n", cpu);
+		else
+			pr_info("depriv: cpu%d PCID disabled\n", cpu);
+
+		switch ((qualification >> 4) & 3) {
+		case 0: { /* mov to cr */
+			unsigned long cr3 = regs[reg];
+			unsigned long cr4 = vmcs_readl(GUEST_CR4);
+
+			if (!(cr4 & X86_CR4_PCIDE)) {
+				pr_info("depriv: cpu%d PCID disabled, fixing cr3\n", cpu);
+				cr3 &= ~X86_CR3_PCID_NOFLUSH;
+			} else
+				pr_info("depriv: cpu%d PCID enabled\n", cpu);
+
+			vmcs_writel(GUEST_CR3, cr3);
+
+			pr_info("depriv: cpu%d wrote cr3 from reg%d: %#lx (%#lx)\n",
+				cpu, cr, cr3, regs[reg]);
+			break;
+		}
+		case 1: /*mov from cr*/
+			regs[reg] = vmcs_readl(GUEST_CR3);
+
+			/* XXX RSP in regs won't be loaded into non-root mode */
+			if (reg == __VCPU_REGS_RSP)
+				vmcs_writel(GUEST_RSP, regs[reg]);
+
+			pr_debug("depriv: cpu%d read cr3 to reg%d: %#lx\n",
+				 cpu, cr, regs[reg]);
+			break;
+		}
+
+		insn_len = vmcs_read32(VM_EXIT_INSTRUCTION_LEN);
+		if (true) {
+			pr_debug("depriv: cpu%d accessed cr3 and continue in non-root mode\n", cpu);
+			DEPRIV_CONTINUE_IN_NON_ROOT_MODE(insn_len);
+		} else {
+			pr_info("depriv: cpu%d accessed cr3 and continue in root mode\n", cpu);
+			DEPRIV_CONTINUE_IN_ROOT_MODE(insn_len);
+		}
+	}
+
+	case EXIT_REASON_CPUID: {
+		static int cnt_cpuid_0x2 = 0;
+		pr_info("depriv: cpu%d cpuid[%#x]\n", cpu, (u32)regs[__VCPU_REGS_RAX]);
+
+		if ((u32)regs[__VCPU_REGS_RAX] == 0x2) {
+			cnt_cpuid_0x2++;
+			if (cnt_cpuid_0x2 == 20) {
+				//vmcs_write32(EXCEPTION_BITMAP, 0xffffffff);
+				pr_info("depriv: cpu%d all fault VM-exit enabeld\n", cpu);
+			}
+		}
+
+		native_cpuid((unsigned int *)&regs[__VCPU_REGS_RAX],
+			     (unsigned int *)&regs[__VCPU_REGS_RBX],
+			     (unsigned int *)&regs[__VCPU_REGS_RCX],
+			     (unsigned int *)&regs[__VCPU_REGS_RDX]);
+
+		insn_len = vmcs_read32(VM_EXIT_INSTRUCTION_LEN);
+		if (true) {
+			pr_debug("depriv: cpu%d executed cpuid and continue in non-root mode\n", cpu);
+			DEPRIV_CONTINUE_IN_NON_ROOT_MODE(insn_len);
+		} else {
+			pr_info("depriv: cpu%d executed cpuid and continue in root mode\n", cpu);
+			DEPRIV_CONTINUE_IN_ROOT_MODE(insn_len);
+		}
+	}
+
+	case EXIT_REASON_VMCALL:
+		insn_len = vmcs_read32(VM_EXIT_INSTRUCTION_LEN);
+
+		if (true) {
+			pr_info("depriv: cpu%d vmcall @ %#lx, continue in non-root mode\n",
+				cpu, rip);
+			DEPRIV_CONTINUE_IN_NON_ROOT_MODE(insn_len);
+		}
+
+		DEPRIV_CONTINUE_IN_ROOT_MODE(insn_len);
+		break;
+
+	case EXIT_REASON_MSR_READ: {
+		u32 ecx = (u32)regs[__VCPU_REGS_RCX];
+		unsigned long val;
+		static int cnt_0x3b = 0;
+
+		if (ecx == MSR_FS_BASE) {
+			val = vmcs_readl(GUEST_FS_BASE);
+			pr_info("depriv: cpu%d FS base MSR: %#lx\n", cpu, val);
+		} else if (ecx == MSR_GS_BASE) {
+			/*
+			 * never read GS base MSR directly when running in root mode,
+			 * which now points to kernel mode per-CPU data.
+			 */
+			val = vmcs_readl(GUEST_GS_BASE);
+			pr_info("depriv: cpu%d GS base MSR: %#lx\n", cpu, val);
+		} else if (rdmsrl_safe(ecx, (unsigned long long *)&val))
+			pr_info("depriv: cpu%d msr[%#x]: %#lx failed\n", cpu, ecx, val);
+
+		pr_info("depriv: cpu%d msr[%#x]: %#lx\n", cpu, ecx, val);
+#if 1
+		pr_info("depriv: cpu%d (%ld) FS base %#lx : %#lx : %#lx\n",
+			cpu, cnt, read_msr(MSR_FS_BASE),
+			vmcs_readl(HOST_FS_BASE), vmcs_readl(GUEST_FS_BASE));
+#endif
+		if (ecx == 0x3b) {
+			cnt_0x3b++;
+			if (cnt_0x3b == 2) {
+				//vmcs_write32(EXCEPTION_BITMAP, 0xffffffff);
+				pr_info("depriv: cpu%d all fault VM-exit enabeld\n", cpu);
+			}
+		}
+
+		*((u32 *)&regs[__VCPU_REGS_RAX]) = (u32)val;
+		*((u32 *)&regs[__VCPU_REGS_RDX]) = (u32)(val >> 32);
+
+		pr_info("depriv: cpu%d executed rdmsr and continue in non-root mode\n", cpu);
+		insn_len = vmcs_read32(VM_EXIT_INSTRUCTION_LEN);
+		DEPRIV_CONTINUE_IN_NON_ROOT_MODE(insn_len);
+	}
+
+	case EXIT_REASON_MSR_WRITE: {
+		bool continue_in_root_mode = false;
+		u32 ecx = (u32)regs[__VCPU_REGS_RCX];
+		unsigned long val = (unsigned long)(u32)regs[__VCPU_REGS_RAX] |
+				    ((unsigned long)(u32)regs[__VCPU_REGS_RDX] << 32);
+
+		if (ecx == MSR_GS_BASE) {
+			/*
+			 * never write GS base MSR directly when running in root mode,
+			 * which now points to kernel mode per-CPU data.
+			 */
+			pr_info("depriv: cpu%d GS base MSR = %#lx\n", cpu, val);
+		} else if (wrmsrl_safe(ecx, val))
+			continue_in_root_mode = true;
+
+		switch (ecx) {
+		case MSR_IA32_SPEC_CTRL:
+			pr_debug("depriv: cpu%d speculation control MSR = %#lx\n", cpu, val);
+			vmx_depriv_cpu_intercept_msr(MSR_IA32_PRED_CMD, true);
+			break;
+		case MSR_IA32_PRED_CMD:
+			pr_debug("depriv: cpu%d prediction command MSR = %#lx\n", cpu, val);
+			vmx_depriv_cpu_intercept_msr(MSR_IA32_PRED_CMD, true);
+			break;
+		case MSR_FS_BASE:
+#if 0
+			pr_info("depriv: cpu%d (%ld) before write FS base %#lx : %#lx : %#lx\n",
+				cpu, cnt, read_msr(MSR_FS_BASE),
+				vmcs_readl(HOST_FS_BASE), vmcs_readl(GUEST_FS_BASE));
+#endif
+			//pr_info("depriv: cpu%d FS base MSR = %#lx\n", cpu, val);
+			/*
+			 * guest FS base needs to be syned up with MSR_FS_BASE, thus we will
+			 * have correct FS base value in non-root mode after all future VM-entries.
+			 */
+			vmcs_writel(GUEST_FS_BASE, val);
+			/*
+			 * host FS base needs to be syned up with MSR_FS_BASE, thus we will
+			 * have correct FS base value in root mode after all future VM-exits.
+			 */
+			vmcs_writel(HOST_FS_BASE, val);
+#if 0
+			pr_info("depriv: cpu%d (%ld) after write FS base %#lx : %#lx : %#lx\n",
+				cpu, cnt, read_msr(MSR_FS_BASE),
+				vmcs_readl(HOST_FS_BASE), vmcs_readl(GUEST_FS_BASE));
+#endif
+			break;
+		case MSR_GS_BASE:
+			pr_info("depriv: cpu%d GS base MSR = %#lx\n", cpu, val);
+			/*
+			 * guest GS base needs to be syned up with MSR_GS_BASE, thus we will
+			 * have correct GS base value in non-root mode after all future VM-entries.
+			 */
+			vmcs_writel(GUEST_GS_BASE, val);
+			// never change HOST_GS_BASE, which points to kernel mode per-CPU data
+			break;
+		case MSR_KERNEL_GS_BASE:
+			pr_info("depriv: cpu%d kernel GS base MSR = %#lx\n", cpu, val);
+			break;
+		case MSR_IA32_TSCDEADLINE:
+			pr_debug("depriv: cpu%d TSC deadline timer MSR = %#lx\n", cpu, val);
+			break;
+		case 0x80b: // EOI virtualization msr
+			pr_debug("depriv: cpu%d EOI MSR = %#lx\n", cpu, val);
+			break;
+		default:
+			pr_info("depriv: cpu%d msr[%#x] = %#lx\n", cpu, ecx, val);
+			break;
+		}
+
+		if (unlikely(continue_in_root_mode)) {
+			pr_info("depriv: cpu%d msr[%#x] = %#lx failed, continue in root mode\n",
+				cpu, ecx, val);
+			DEPRIV_CONTINUE_IN_ROOT_MODE(0);
+		} else {
+			pr_debug("depriv: cpu%d executed wrmsr and continue in non-root mode\n", cpu);
+			insn_len = vmcs_read32(VM_EXIT_INSTRUCTION_LEN);
+			DEPRIV_CONTINUE_IN_NON_ROOT_MODE(insn_len);
+		}
+	}
+
+	default: /* continue in root mode */
+		pr_info("depriv: cpu%d exit reason=%#x\n", cpu, reason);
+		DEPRIV_CONTINUE_IN_ROOT_MODE(0);
+	}
+}
+
+void __init vmx_depriv_host(void)
+{
+	if (setup_depriv_vmcs_config()) {
+		pr_err("depriv: error setting up deprivilege VMCS config\n");
+		return;
+	}
+
+	on_each_cpu(vmx_depriv_cpu, NULL, 0);
+}
diff --git a/arch/x86/kvm/vmx/depriv.h b/arch/x86/kvm/vmx/depriv.h
new file mode 100644
index 000000000000..9fb2120e7217
--- /dev/null
+++ b/arch/x86/kvm/vmx/depriv.h
@@ -0,0 +1,8 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef _X86_VMX_DEPRIV_H
+#define _X86_VMX_DEPRIV_H
+
+void __init vmx_depriv_host(void);
+void vmx_repriv_host(void);
+
+#endif /* _X86_VMX_DEPRIV_H */
diff --git a/arch/x86/kvm/vmx/depriv_entry.S b/arch/x86/kvm/vmx/depriv_entry.S
new file mode 100644
index 000000000000..934d551364ca
--- /dev/null
+++ b/arch/x86/kvm/vmx/depriv_entry.S
@@ -0,0 +1,122 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#include <linux/linkage.h>
+#include <asm/asm.h>
+#include <asm/kvm_vcpu_regs.h>
+
+#define WORD_SIZE (BITS_PER_LONG / 8)
+
+#define VCPU_RAX	__VCPU_REGS_RAX * WORD_SIZE
+#define VCPU_RCX	__VCPU_REGS_RCX * WORD_SIZE
+#define VCPU_RDX	__VCPU_REGS_RDX * WORD_SIZE
+#define VCPU_RBX	__VCPU_REGS_RBX * WORD_SIZE
+/* Intentionally omit RSP as it's context switched by hardware */
+#define VCPU_RBP	__VCPU_REGS_RBP * WORD_SIZE
+#define VCPU_RSI	__VCPU_REGS_RSI * WORD_SIZE
+#define VCPU_RDI	__VCPU_REGS_RDI * WORD_SIZE
+
+#define VCPU_R8		__VCPU_REGS_R8  * WORD_SIZE
+#define VCPU_R9		__VCPU_REGS_R9  * WORD_SIZE
+#define VCPU_R10	__VCPU_REGS_R10 * WORD_SIZE
+#define VCPU_R11	__VCPU_REGS_R11 * WORD_SIZE
+#define VCPU_R12	__VCPU_REGS_R12 * WORD_SIZE
+#define VCPU_R13	__VCPU_REGS_R13 * WORD_SIZE
+#define VCPU_R14	__VCPU_REGS_R14 * WORD_SIZE
+#define VCPU_R15	__VCPU_REGS_R15 * WORD_SIZE
+
+	.text
+
+SYM_FUNC_START(vmx_depriv)
+	/* assuming vmlaunch will succeed */
+	xor %eax, %eax
+	/* Enter non-root mode */
+	vmlaunch
+
+	/* vmlaunch failed, switch to root mode stask */
+	mov %_ASM_ARG1, %rsp
+	mov $1, %eax
+	jmp vmx_depriv_continue_in_root_mode
+SYM_FUNC_END(vmx_depriv)
+
+/* vmlaunch succeeded */
+SYM_FUNC_START(vmx_depriv_rip)
+	/* to instruction immediately after vmx_depriv */
+	ret
+SYM_FUNC_END(vmx_depriv_rip)
+
+SYM_FUNC_START(vmx_depriv_vmcall)
+	/* vmcall */
+	.byte 0x0f, 0x01, 0xc1
+	ret
+SYM_FUNC_END(vmx_depriv_vmcall)
+
+SYM_FUNC_START(vmx_depriv_vmexit)
+	push %r15
+	push %r14
+	push %r13
+	push %r12
+	push %r11
+	push %r10
+	push %r9
+	push %r8
+	push %rdi
+	push %rsi
+	push %rbp
+	push %rsp
+	push %rbx
+	push %rdx
+	push %rcx
+	push %rax
+
+	xor %rax, %rax
+	xor %rcx, %rcx
+	xor %rdx, %rdx
+	xor %rbx, %rbx
+	xor %rbp, %rbp
+	xor %rsi, %rsi
+	xor %rdi, %rdi
+	xor %r8,  %r8
+	xor %r9,  %r9
+	xor %r10, %r10
+	xor %r11, %r11
+	xor %r12, %r12
+	xor %r13, %r13
+	xor %r14, %r14
+	xor %r15, %r15
+
+	mov %rsp, %_ASM_ARG1
+	call vmx_depriv_vmexit_handler
+	cmpb $1, %al
+
+	pop %rax
+	pop %rcx
+	pop %rdx
+	pop %rbx
+	pop %r8
+	pop %rbp
+	pop %rsi
+	pop %rdi
+	pop %r8
+	pop %r9
+	pop %r10
+	pop %r11
+	pop %r12
+	pop %r13
+	pop %r14
+	pop %r15
+
+	/*
+	 * upon vmx_depriv_vmexit_handler returning false, switch back to root
+	 * mode with guest stack
+	 */
+	jne vmx_depriv_continue_in_root_mode
+
+	/*
+	 * upon vmx_depriv_vmexit_handler returning true, continue non-root mode
+	 */
+	vmresume
+
+	/*
+	 * VM resume failed, switch back to root mode with guest stack
+	 */
+	jmp vmx_depriv_continue_in_root_mode
+SYM_FUNC_END(vmx_depriv_vmexit)
diff --git a/arch/x86/kvm/vmx/vmenter.S b/arch/x86/kvm/vmx/vmenter.S
index 39479af40390..696d1445635c 100644
--- a/arch/x86/kvm/vmx/vmenter.S
+++ b/arch/x86/kvm/vmx/vmenter.S
@@ -92,109 +92,6 @@ SYM_FUNC_START(vmx_vmexit)
 	RET
 SYM_FUNC_END(vmx_vmexit)
 
-SYM_FUNC_START(vmx_depriv)
-	/* assuming vmlaunch will succeed */
-	xor %eax, %eax
-	/* Enter non-root mode */
-	vmlaunch
-
-	/* vmlaunch failed, switch to root mode stask */
-	mov %_ASM_ARG1, %rsp
-	mov $1, %eax
-	jmp vmx_depriv_continue_in_root_mode
-SYM_FUNC_END(vmx_depriv)
-
-/* vmlaunch succeeded */
-SYM_FUNC_START(vmx_depriv_rip)
-	ret
-SYM_FUNC_END(vmx_depriv_rip)
-
-SYM_FUNC_START(vmx_depriv_vmcall)
-	/* vmcall */
-	.byte 0x0f, 0x01, 0xc1
-	ret
-SYM_FUNC_END(vmx_depriv_vmcall)
-
-SYM_FUNC_START(vmx_depriv_vmexit)
-	push %r15
-	push %r14
-	push %r13
-	push %r12
-	push %r11
-	push %r10
-	push %r9
-	push %r8
-	push %rdi
-	push %rsi
-	push %rbp
-	push %rsp
-	push %rbx
-	push %rdx
-	push %rcx
-	push %rax
-
-	xor %rax, %rax
-	xor %rcx, %rcx
-	xor %rdx, %rdx
-	xor %rbx, %rbx
-	xor %rbp, %rbp
-	xor %rsi, %rsi
-	xor %rdi, %rdi
-	xor %r8,  %r8
-	xor %r9,  %r9
-	xor %r10, %r10
-	xor %r11, %r11
-	xor %r12, %r12
-	xor %r13, %r13
-	xor %r14, %r14
-	xor %r15, %r15
-
-	mov %rsp, %_ASM_ARG1
-	call vmx_depriv_vmexit_handler
-	cmpb $1, %al
-
-	pop %rax
-	pop %rcx
-	pop %rdx
-	pop %rbx
-	pop %r8
-	pop %rbp
-	pop %rsi
-	pop %rdi
-	pop %r8
-	pop %r9
-	pop %r10
-	pop %r11
-	pop %r12
-	pop %r13
-	pop %r14
-	pop %r15
-
-	jne vmx_depriv_continue_in_root_mode
-
-	/*
-	 * upon vmx_depriv_vmexit_handler returning true, continue non-root mode
-	 */
-	vmresume
-
-	jmp vmx_depriv_continue_in_root_mode
-
-	/*
-	 * VM resume failed, switch back to root mode with guest stack
-	 */
-	mov (%rsp), %rax
-	mov %rax, %rsp
-	xor %eax, %eax
-	mov $2, %eax
-	/* to instruction immediately after vmx_depriv */
-	ret
-
-	/*
-	 * upon vmx_depriv_vmexit_handler returning false, switch back to root
-	 * mode with guest stack
-	 */
-SYM_FUNC_END(vmx_depriv_vmexit)
-
 /**
  * __vmx_vcpu_run - Run a vCPU via a transition to VMX guest mode
  * @vmx:	struct vcpu_vmx * (forwarded to vmx_update_host_rsp)
diff --git a/arch/x86/kvm/vmx/vmx.c b/arch/x86/kvm/vmx/vmx.c
index 0e8b15819b48..419fe96280c4 100644
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@ -54,6 +54,9 @@
 
 #include "capabilities.h"
 #include "cpuid.h"
+#if IS_ENABLED(CONFIG_KVM_INTEL_DEPRIV_HOST)
+#include "depriv.h"
+#endif
 #include "evmcs.h"
 #include "hyperv.h"
 #include "kvm_onhyperv.h"
@@ -5793,7 +5796,7 @@ void dump_vmcs(struct kvm_vcpu *vcpu)
 	unsigned long cr4;
 	int efer_slot;
 
-	if (false && !dump_invalid_vmcs) {
+	if (!dump_invalid_vmcs) {
 		pr_warn_ratelimited("set kvm_intel.dump_invalid_vmcs=1 to dump internal KVM state.\n");
 		return;
 	}
@@ -5920,9 +5923,8 @@ void dump_vmcs(struct kvm_vcpu *vcpu)
 	       vmcs_read32(VM_EXIT_INTR_INFO),
 	       vmcs_read32(VM_EXIT_INTR_ERROR_CODE),
 	       vmcs_read32(VM_EXIT_INSTRUCTION_LEN));
-	pr_err("        reason=%08x qualification=%016lx VM instr error=%08x\n",
-	       vmcs_read32(VM_EXIT_REASON), vmcs_readl(EXIT_QUALIFICATION),
-	       vmcs_read32(VM_INSTRUCTION_ERROR));
+	pr_err("        reason=%08x qualification=%016lx\n",
+	       vmcs_read32(VM_EXIT_REASON), vmcs_readl(EXIT_QUALIFICATION));
 	pr_err("IDTVectoring: info=%08x errcode=%08x\n",
 	       vmcs_read32(IDT_VECTORING_INFO_FIELD),
 	       vmcs_read32(IDT_VECTORING_ERROR_CODE));
@@ -8042,1193 +8044,6 @@ static void vmx_cleanup_l1d_flush(void)
 	l1tf_vmx_mitigation = VMENTER_L1D_FLUSH_AUTO;
 }
 
-#if IS_ENABLED(CONFIG_KVM_INTEL_DEPRIV_HOST)
-/*
- * host state memory buffer page order
- */
-#define DEPRIV_CPU_STATE_PAGE_ORDER		1
-#define DEPRIV_CPU_STATE_BUFFER_SIZE		(PAGE_SIZE << DEPRIV_CPU_STATE_PAGE_ORDER)
-#define DEPRIV_CPU_STATE_VMCS_MSR_BITMAP	(DEPRIV_CPU_STATE_BUFFER_SIZE - PAGE_SIZE)
-
-/*
- * needed to iret to root mode kernel or user space when the VM exit happened
- */
-#define DEPRIV_HOST_STACK_RESERVED_BYTES (16 * 8)
-
-static struct vmcs_config depriv_vmcs_conf;
-static DEFINE_PER_CPU(struct vmcs *, depriv_vmcs);
-static DEFINE_PER_CPU(void *, depriv_cpu_state);
-
-static void __init vmx_depriv_cpu_controls(void)
-{
-	u32 eb;
-
-	vmcs_write32(PIN_BASED_VM_EXEC_CONTROL,
-		     depriv_vmcs_conf.pin_based_exec_ctrl);
-	vmcs_write32(CPU_BASED_VM_EXEC_CONTROL,
-		     depriv_vmcs_conf.cpu_based_exec_ctrl);
-
-	if (cpu_has_secondary_exec_ctrls()) {
-		vmcs_write32(SECONDARY_VM_EXEC_CONTROL,
-		     depriv_vmcs_conf.cpu_based_2nd_exec_ctrl);
-	}
-
-	vmcs_write32(VM_EXIT_CONTROLS,
-		     depriv_vmcs_conf.vmexit_ctrl |
-		     VM_EXIT_HOST_ADDR_SPACE_SIZE);
-	vmcs_write32(VM_ENTRY_CONTROLS,
-		     depriv_vmcs_conf.vmentry_ctrl);
-
-	eb = (1u << UD_VECTOR) | (1u << DF_VECTOR) | (1u << GP_VECTOR);
-	vmcs_write32(EXCEPTION_BITMAP, eb);
-	vmcs_write32(PAGE_FAULT_ERROR_CODE_MASK, 0);
-	vmcs_write32(PAGE_FAULT_ERROR_CODE_MATCH, 0);
-	vmcs_write32(CR3_TARGET_COUNT, 0);
-
-	vmcs_write32(VM_EXIT_MSR_STORE_COUNT, 0);
-	vmcs_write32(VM_EXIT_MSR_LOAD_COUNT, 0);
-	vmcs_write64(VM_EXIT_MSR_LOAD_ADDR, 0);
-	vmcs_write32(VM_ENTRY_MSR_LOAD_COUNT, 0);
-	vmcs_write64(VM_ENTRY_MSR_LOAD_ADDR, 0);
-}
-
-static void __init vmx_depriv_cpu_crs(void)
-{
-	unsigned long cr0, cr3, cr4;
-	u64 pat, efer;
-
-	cr0 = read_cr0();
-	vmcs_writel(HOST_CR0, cr0);
-	vmcs_writel(CR0_READ_SHADOW, cr0);
-	vmcs_writel(GUEST_CR0, cr0);
-
-	cr3 = __read_cr3();
-	vmcs_writel(HOST_CR3, cr3);
-	vmcs_writel(GUEST_CR3, cr3);
-
-	cr4 = __read_cr4();
-	vmcs_writel(HOST_CR4, cr4);
-	vmcs_writel(CR4_READ_SHADOW, cr4);
-	vmcs_writel(GUEST_CR4, cr4 | KVM_PMODE_VM_CR4_ALWAYS_ON);
-
-	vmcs_writel(CR0_GUEST_HOST_MASK, ~X86_CR0_TS);
-	vmcs_writel(CR4_GUEST_HOST_MASK, ~KVM_CR4_GUEST_OWNED_BITS);
-
-	pat = read_msr(MSR_IA32_CR_PAT);
-	vmcs_write64(HOST_IA32_PAT, pat);
-	vmcs_write64(GUEST_IA32_PAT, pat);
-
-	efer = read_msr(MSR_EFER);
-	vmcs_write64(HOST_IA32_EFER, efer);
-	vmcs_write64(GUEST_IA32_EFER, efer);
-
-	if (efer & EFER_LMA && efer & EFER_SCE) {
-		pr_debug("STAR=%016lx", read_msr(MSR_STAR));
-		pr_debug("LSTAR=%016lx", read_msr(MSR_LSTAR));
-		pr_debug("CSTAR=%016lx", read_msr(MSR_CSTAR));
-		pr_debug("syscall mask=%016lx", read_msr(MSR_SYSCALL_MASK));
-	}
-
-	if (cpu_has_load_perf_global_ctrl()) {
-		u64 perf_global_ctrl;
-		rdmsrl_safe(MSR_CORE_PERF_GLOBAL_CTRL, &perf_global_ctrl);
-		vmcs_write64(GUEST_IA32_PERF_GLOBAL_CTRL, perf_global_ctrl);
-		vmcs_write64(HOST_IA32_PERF_GLOBAL_CTRL, perf_global_ctrl);
-	}
-
-	if (false) // true: test code path handling vmlaunch caused VM-entry fail
-		vmcs_write32(CR3_TARGET_COUNT, 0x100000);
-}
-
-static inline bool __init is_desc_16byte(struct desc_struct *dentry)
-{
-	// s = 0 : system descriptor
-	return dentry->p && !dentry->s;
-}
-
-static inline u32 __init get_desc_limit_in_byte(struct desc_struct *dentry)
-{
-	u32 limit = get_desc_limit(dentry);
-	if (dentry->g)
-		limit = (limit << PAGE_SHIFT) | (PAGE_SIZE - 1);
-	return limit;
-}
-
-static inline void __init dump_desc_entry(struct desc_struct *dentry)
-{
-	int cpu = smp_processor_id();
-	bool is_16byte = is_desc_16byte(dentry);
-	u16 *entry = (u16 *)dentry;
-	u32 limit = get_desc_limit_in_byte(dentry);
-	unsigned long base = get_desc_base(dentry);
-
-	if (is_16byte) {
-		pr_info("depriv: cpu%d %04x %04x %04x %04x %04x %04x %04x %04x\n",
-			cpu, entry[0], entry[1], entry[2], entry[3],
-			entry[4], entry[5], entry[6], entry[7]);
-		base += (u64)(*((u32 *)(dentry + 1))) << 32;
-	} else {
-		pr_info("depriv: cpu%d %04x %04x %04x %04x\n",
-			cpu, entry[0], entry[1], entry[2], entry[3]);
-	}
-
-	pr_info("depriv: cpu%d type %x, S %x, DPL %x, P %x, AVL %x, "
-		"L %x, D %x, G %x, limit %#x, base %#lx\n",
-		cpu, dentry->type, dentry->s, dentry->dpl, dentry->p,
-		dentry->avl, dentry->l, dentry->d, dentry->g, limit, base);
-}
-
-static inline __init struct desc_struct *get_gdt_entry(unsigned long addr)
-{
-	struct desc_struct *dentry = (struct desc_struct *)addr;
-	if (false)
-		dump_desc_entry(dentry);
-	return dentry;
-}
-
-static inline u32 __init get_desc_ar(struct desc_struct *dentry,
-				     bool is_null, bool is_segment)
-{
-	int cpu = smp_processor_id();
-	u32 unusable = is_null ? 1 : 0; // 0 = usable; 1 = unusable
-	/*
-	 * 26.3.1.2 Checks on Guest Segment Registers, AR bytes:
-	 */
-	bool s = (unusable ? dentry->s :
-			     (is_segment ? 1 : 0));
-	u32 ar = dentry->type |
-		 (s ? 1 : 0) << 4 |
-		 dentry->dpl << 5 |
-		 dentry->p << 7 |
-		 dentry->avl << 12 |
-		 dentry->l << 13 |
-		 dentry->d << 14 |
-		 dentry->g << 15 |
-		 unusable << 16;
-	pr_debug("depriv: cpu%d entry ar %#x\n", cpu, ar);
-	return ar;
-}
-
-#define DEPRIV_SELECTOR(name, sel) {						\
-	pr_debug("depriv: cpu%d " #name " %#x\n", cpu, sel);			\
-	dentry = get_gdt_entry(gdt_base + sel);					\
-	base = get_desc_base(dentry);						\
-	if (is_desc_16byte(dentry))						\
-		base += (u64)(*((u32 *)(dentry + 1))) << 32;			\
-	vmcs_write16(GUEST_##name##_SELECTOR, sel);				\
-	vmcs_writel(GUEST_##name##_BASE, base);					\
-	vmcs_write32(GUEST_##name##_LIMIT, get_desc_limit_in_byte(dentry));	\
-	vmcs_write32(GUEST_##name##_AR_BYTES,					\
-		     get_desc_ar(dentry, sel == 0, is_segment));		\
-}
-
-#define DEPRIV_SEGMENT(SEG, fix_null_selector) {				\
-	u16 seg;								\
-	savesegment(SEG, seg);							\
-	if (fix_null_selector && seg == 0) seg = __KERNEL_DS;			\
-	vmcs_write16(HOST_##SEG##_SELECTOR, seg);				\
-	DEPRIV_SELECTOR(SEG, seg);						\
-}
-
-static void __init vmx_depriv_cpu_segments(unsigned long gdt_base)
-{
-	int cpu = smp_processor_id();
-	struct desc_struct *dentry;
-	unsigned long base;
-	bool is_segment = true;
-
-	DEPRIV_SEGMENT(CS, false);
-	DEPRIV_SEGMENT(DS, false);
-	DEPRIV_SEGMENT(ES, false);
-	DEPRIV_SEGMENT(SS, false);
-	DEPRIV_SEGMENT(FS, false);
-	DEPRIV_SEGMENT(GS, false);
-
-	base = read_msr(MSR_FS_BASE);
-	pr_info("depriv: cpu%d FS base MSR %#lx\n", cpu, base);
-	vmcs_writel(HOST_FS_BASE, base);
-	vmcs_writel(GUEST_FS_BASE, base);
-
-	base = read_msr(MSR_GS_BASE);
-	pr_info("depriv: cpu%d GS base MSR %#lx\n", cpu, base);
-	vmcs_writel(HOST_GS_BASE, base);
-	vmcs_writel(GUEST_GS_BASE, base);
-}
-
-static void __init vmx_depriv_cpu_ldtr(unsigned long gdt_base)
-{
-	int cpu = smp_processor_id();
-	struct desc_struct *dentry;
-	unsigned long base;
-	u16 ldtr;
-	bool is_segment = false;
-
-	store_ldt(ldtr);
-	DEPRIV_SELECTOR(LDTR, ldtr);
-}
-
-static void __init vmx_depriv_cpu_tr(unsigned long gdt_base)
-{
-	int cpu = smp_processor_id();
-	struct desc_struct *dentry;
-	unsigned long base, tss_base;
-	u16 tr;
-	u32 ar;
-	bool is_segment = false;
-
-	store_tr(tr);
-	if (tr != GDT_ENTRY_TSS*8)
-		pr_err("depriv: cpu%d tr selector mismatch %#x : %#x\n",
-		       cpu, tr, GDT_ENTRY_TSS*8);
-	vmcs_write16(HOST_TR_SELECTOR, tr);
-	DEPRIV_SELECTOR(TR, tr);
-	vmcs_writel(HOST_TR_BASE, base);
-	tss_base = (unsigned long)&get_cpu_entry_area(cpu)->tss.x86_tss;
-	if (base != tss_base)
-		pr_err("depriv: cpu%d tr base mismatch %#lx : %#lx\n",
-		       cpu, base, tss_base);
-
-	ar = vmcs_read32(GUEST_TR_AR_BYTES);
-	if ((ar & VMX_AR_TYPE_MASK) != VMX_AR_TYPE_BUSY_64_TSS) {
-		pr_err("%s: tss fixup for long mode\n", __func__);
-		vmcs_write32(GUEST_TR_AR_BYTES,
-			     (ar & ~VMX_AR_TYPE_MASK) |
-			     VMX_AR_TYPE_BUSY_64_TSS);
-	}
-}
-
-static void __init vmx_depriv_cpu_desc_tables(void)
-{
-	int cpu = smp_processor_id();
-	struct desc_ptr gdt, idt;
-	unsigned long gdt_base;
-
-	store_gdt(&gdt);
-	gdt_base = gdt.address;
-	if (gdt_base != (unsigned long)get_current_gdt_ro())
-		pr_err("depriv: cpu%d gdt base mismatch %#lx : %#lx\n",
-		       cpu, gdt_base, (unsigned long)get_current_gdt_ro());
-	vmcs_writel(HOST_GDTR_BASE, gdt_base);
-	vmcs_writel(GUEST_GDTR_BASE, gdt_base);
-	/* there is no host gdt limit */
-	vmcs_write32(GUEST_GDTR_LIMIT, gdt.size);
-
-	store_idt(&idt);
-	/* host should never handle interrupts */
-	vmcs_writel(HOST_IDTR_BASE, idt.address);
-	vmcs_writel(GUEST_IDTR_BASE, idt.address);
-	/* there is no host idt limit */
-	vmcs_write32(GUEST_IDTR_LIMIT, idt.size);
-
-	vmx_depriv_cpu_segments(gdt_base);
-	vmx_depriv_cpu_ldtr(gdt_base);
-	vmx_depriv_cpu_tr(gdt_base);
-}
-
-static void __init vmx_depriv_cpu_sysenter_msrs(void)
-{
-	u32 low32, high32;
-	unsigned long msr;
-
-	msr = read_msr(MSR_IA32_SYSENTER_ESP);
-	vmcs_writel(HOST_IA32_SYSENTER_ESP, msr);
-	vmcs_writel(GUEST_SYSENTER_ESP, msr);
-
-	rdmsr(MSR_IA32_SYSENTER_CS, low32, high32);
-	vmcs_write32(HOST_IA32_SYSENTER_CS, low32);
-	vmcs_write32(GUEST_SYSENTER_CS, low32);
-
-	msr = read_msr(MSR_IA32_SYSENTER_EIP);
-	vmcs_writel(HOST_IA32_SYSENTER_EIP, msr);
-	vmcs_writel(GUEST_SYSENTER_EIP, msr);
-}
-
-static void __init vmx_depriv_cpu_misc(void)
-{
-	unsigned long dr7;
-	u64 dbg_ctrl;
-
-	get_debugreg(dr7, 7);
-	vmcs_writel(GUEST_DR7, dr7);
-
-	dbg_ctrl = read_msr(MSR_IA32_DEBUGCTLMSR);
-	vmcs_write64(GUEST_IA32_DEBUGCTL, dbg_ctrl);
-}
-
-/*
- * sync host states to guest states
- */
-static void __init vmx_depriv_cpu_state(void)
-{
-	vmx_depriv_cpu_controls();
-	vmx_depriv_cpu_crs();
-	vmx_depriv_cpu_desc_tables();
-	vmx_depriv_cpu_sysenter_msrs();
-	vmx_depriv_cpu_misc();
-}
-
-static void vmx_repriv_cpu_release_resources(void)
-{
-	int cpu = smp_processor_id();
-	void *host_cpu_state = per_cpu(depriv_cpu_state, cpu);
-	struct vmcs *vmcs = per_cpu(depriv_vmcs, cpu);
-
-	pr_info("depriv: reprivilege cpu%d releasing resources\n", cpu);
-
-	if (host_cpu_state) {
-		per_cpu(depriv_cpu_state, cpu) = NULL;
-		memset(host_cpu_state, 0, DEPRIV_CPU_STATE_BUFFER_SIZE);
-		free_pages((unsigned long)host_cpu_state, DEPRIV_CPU_STATE_PAGE_ORDER);
-	}
-
-	if (vmcs) {
-		per_cpu(depriv_vmcs, cpu) = NULL;
-		vmcs_clear(vmcs);
-		free_vmcs(vmcs);
-	}
-}
-
-void vmx_depriv_vmexit(void);
-int vmx_depriv(unsigned long root_rsp);
-void vmx_depriv_rip(void);
-void vmx_depriv_vmcall(void);
-
-static void vmx_depriv_cpu_intercept_msr(u32 msr, bool enable)
-{
-	u32 orig_msr = msr;
-	int cpu = smp_processor_id();
-	void *msr_bitmap = per_cpu(depriv_cpu_state, cpu) +
-			   DEPRIV_CPU_STATE_VMCS_MSR_BITMAP;
-	int f = sizeof(unsigned long);
-	bool set_result = false;
-
-	if (msr <= 0x1fff) {
-		if (enable)
-			__set_bit(msr, msr_bitmap + 0x800 / f);
-		else
-			__clear_bit(msr, msr_bitmap + 0x800 / f);
-		set_result = !!test_bit(msr, msr_bitmap + 0x800 / f);
-	} else if (msr >= 0xc0000000 && msr <= 0xc0001fff) {
-		msr &= 0x1fff;
-		if (enable)
-			__set_bit(msr, msr_bitmap + 0xc00 / f);
-		else
-			__clear_bit(msr, msr_bitmap + 0xc00 / f);
-		set_result = !!test_bit(msr, msr_bitmap + 0xc00 / f);
-	} else {
-		pr_err("depriv: cpu%d set MSR bitmap @ %#x out of scope\n",
-		       cpu, orig_msr);
-		return;
-	}
-
-	if (set_result != enable)
-		pr_err("depriv: cpu%d set MSR bitmap @ %#x failed\n",
-		       cpu, orig_msr);
-}
-
-#define DEPRIV_SET_ROOT_MODE_TRAMPOLINE_STACK(ip_off) do {			\
-	*(unsigned long *)(root_rsp + 0x0) = rip + (ip_off);			\
-	*(unsigned long *)(root_rsp + 0x8) = vmcs_read16(GUEST_CS_SELECTOR);	\
-	*(unsigned long *)(root_rsp + 0x10) = vmcs_readl(GUEST_RFLAGS);		\
-	*(unsigned long *)(root_rsp + 0x18) = vmcs_readl(GUEST_RSP);		\
-	*(unsigned long *)(root_rsp + 0x20) = vmcs_read16(GUEST_SS_SELECTOR);	\
-	*(unsigned long *)(root_rsp + 0x28) = vmcs_readl(GUEST_CR3);		\
-	*(unsigned long *)(root_rsp + 0x30) = vmcs_readl(GUEST_GS_BASE);	\
-	*(unsigned long *)(root_rsp + 0x38) = vmcs_readl(GUEST_FS_BASE);	\
-} while (0)
-
-static void __init vmx_depriv_cpu(void *info)
-{
-	int cpu = smp_processor_id();
-	int node = cpu_to_node(cpu);
-	struct vmcs *vmcs = NULL;
-	struct page *page = NULL;
-	void *host_cpu_state = NULL;
-	void *msr_bitmap = NULL;
-	unsigned long root_rsp, rip, rsp, rflags;
-	int vmx_depriv_result;
-
-	if (!(depriv_vmcs_conf.cpu_based_exec_ctrl & CPU_BASED_USE_MSR_BITMAPS)) {
-		pr_err("depriv: MSR bitmap not available on cpu%d\n", cpu);
-		goto error;
-	}
-
-	vmcs = alloc_vmcs_cpu(false, cpu, GFP_KERNEL);
-	if (!vmcs) {
-		pr_err("depriv: unable to allocate VMCS for cpu%d\n", cpu);
-		goto error;
-	}
-
-	vmcs_clear(vmcs);
-	vmcs_load(vmcs);
-	vmcs_write64(VMCS_LINK_POINTER, ~0ull);
-	indirect_branch_prediction_barrier();
-	per_cpu(depriv_vmcs, cpu) = vmcs;
-
-	// memory for root mode VM exit handler
-	page = __alloc_pages_node(node, GFP_KERNEL, DEPRIV_CPU_STATE_PAGE_ORDER);
-	if (!page) {
-		pr_err("depriv: unable to allocate host state buffer for cpu%d\n", cpu);
-		goto error;
-	}
-
-	host_cpu_state = page_address(page);
-	memset(host_cpu_state, 0, DEPRIV_CPU_STATE_BUFFER_SIZE);
-	per_cpu(depriv_cpu_state, cpu) = host_cpu_state;
-
-	// last page of host state
-	msr_bitmap = host_cpu_state + DEPRIV_CPU_STATE_VMCS_MSR_BITMAP;
-	memset(msr_bitmap, 0xff, PAGE_SIZE);
-	vmx_depriv_cpu_intercept_msr(MSR_FS_BASE, true);
-	vmx_depriv_cpu_intercept_msr(MSR_GS_BASE, true);
-	vmx_depriv_cpu_intercept_msr(MSR_KERNEL_GS_BASE, true);
-	vmcs_write64(MSR_BITMAP, __pa(msr_bitmap));
-
-	vmx_depriv_cpu_state();
-
-	vmcs_writel(HOST_RIP, (unsigned long)vmx_depriv_vmexit);
-	// reserve extra DEPRIV_HOST_STACK_RESERVED_BYTES bytes for reprivileging host
-	root_rsp = (unsigned long)msr_bitmap - DEPRIV_HOST_STACK_RESERVED_BYTES;
-	vmcs_writel(HOST_RSP, root_rsp);
-
-	/* switching to non-root mode */
-	rip = (unsigned long)vmx_depriv_rip;
-	vmcs_writel(GUEST_RIP, rip);
-	asm volatile("mov %%rsp,%0" : "=m"(rsp));
-	// reserve extra 8 bytes for RIP pushed to stack when calling vmx_depriv
-	rsp -= 8;
-	vmcs_writel(GUEST_RSP, rsp);
-
-	asm volatile("xor %%rax,%%rax\n\t"
-		     "pushf\n\t"
-		     "pop %%rax\n\t"
-		     "mov %%rax,%0"
-		     : "=m"(rflags) :: "%rax");
-	vmcs_writel(GUEST_RFLAGS, rflags & ~X86_EFLAGS_IF);
-
-	DEPRIV_SET_ROOT_MODE_TRAMPOLINE_STACK(0);
-
-	pr_info("depriv: deprivileging cpu%d: rip=%#lx rsp=%#lx\n",
-		cpu, rip, rsp);
-
-	if (false) // true: test code path handling vmresume caused VM-entry fail
-		vmcs_write32(GUEST_TR_AR_BYTES, 0x009b);
-
-	/*
-	 * Should we save/restore general purpose registers around vmx_depriv?
-	 * Yes, but only restore them when there was a successful vmentry.
-	 */
-	vmx_depriv_result = vmx_depriv(root_rsp);
-	if (!vmx_depriv_result) {
-		// continue in non-root mode...
-		asm volatile("mov %%rsp,%0" : "=m"(rsp));
-		asm volatile("xor %%rax,%%rax\n\t"
-			     "pushf\n\t"
-			     "pop %%rax\n\t"
-			     "mov %%rax,%0"
-			     : "=m"(rflags) :: "%rax");
-		pr_info("depriv: cpu%d deprivileged: rsp=%#lx  rflags=%#lx\n",
-			cpu, rsp, rflags);
-
-		wrmsrl(MSR_FS_BASE, read_msr(MSR_FS_BASE));
-		wrmsrl(MSR_GS_BASE, read_msr(MSR_GS_BASE));
-		wrmsrl(MSR_KERNEL_GS_BASE, read_msr(MSR_KERNEL_GS_BASE));
-		vmx_depriv_vmcall();
-		return;
-	}
-
-	// still in root mode
-	if (vmx_depriv_result == 1)
-		pr_err("depriv: launch failed on cpu%d\n", cpu);
-	else if (vmx_depriv_result == 2)
-		pr_err("depriv: resume failed on cpu%d\n", cpu);
-
-error:
-	vmx_repriv_cpu_release_resources();
-}
-
-static void vmx_repriv_cpu_crs(void)
-{
-	int cpu = smp_processor_id();
-	unsigned long host_cr0 = read_cr0();
-	unsigned long host_cr3 = __read_cr3();
-	unsigned long host_cr4 = __read_cr4();
-	unsigned long guest_cr0 = vmcs_readl(GUEST_CR0);
-	unsigned long guest_cr3 = vmcs_readl(GUEST_CR3);
-	unsigned long guest_cr4 = vmcs_readl(GUEST_CR4);
-
-	if (host_cr0 != guest_cr0) {
-		pr_info("depriv: repriv cpu%d cr0 %#lx : %#lx : %#lx\n",
-			cpu, host_cr0, vmcs_readl(HOST_CR0), guest_cr0);
-		write_cr0(guest_cr0);
-		vmcs_writel(HOST_CR0, guest_cr0);
-	}
-
-	if (host_cr3 != guest_cr3) {
-		pr_info("depriv: repriv cpu%d cr3 %#lx : %#lx\n",
-			cpu, host_cr3, guest_cr3);
-	}
-
-	if (host_cr4 != guest_cr4) {
-		pr_info("depriv: repriv cpu%d cr4 %#lx : %#lx : %#lx\n",
-			cpu, host_cr4, vmcs_readl(HOST_CR4), guest_cr4);
-		vmcs_writel(HOST_CR4, guest_cr4);
-	}
-}
-
-static inline void vmx_repriv_cpu_sysenter_msrs(void)
-{
-	wrmsrl(MSR_IA32_SYSENTER_ESP, vmcs_readl(GUEST_SYSENTER_ESP));
-	wrmsr(MSR_IA32_SYSENTER_CS, vmcs_read32(GUEST_SYSENTER_CS), 0);
-	wrmsrl(MSR_IA32_SYSENTER_EIP, vmcs_readl(GUEST_SYSENTER_EIP));
-}
-
-static inline void vmx_repriv_cpu_misc(void)
-{
-	set_debugreg(vmcs_readl(GUEST_DR7), 7);
-	wrmsrl(MSR_IA32_DEBUGCTLMSR, vmcs_read64(GUEST_IA32_DEBUGCTL));
-}
-
-#define REPRIV_SEGMENT(tag, TAG) do {					\
-	ar = vmcs_read32(GUEST_##TAG##S_AR_BYTES);			\
-	if (ar & VMX_AR_UNUSABLE_MASK) {				\
-		pr_info("depriv: cpu%d " #TAG "S unusable\n", cpu);	\
-		break;							\
-	}								\
-	sel = vmcs_read16(GUEST_##TAG##S_SELECTOR);			\
-	loadsegment(tag##s, sel);					\
-	pr_debug("depriv: cpu%d " #TAG "S %#x\n", cpu, sel);		\
-} while (0)
-
-static inline void vmx_repriv_cpu_segments(void)
-{
-	int cpu = smp_processor_id();
-	unsigned long host_base, guest_base;
-	u32 ar;
-	u16 sel;
-
-	REPRIV_SEGMENT(d, D);
-	REPRIV_SEGMENT(e, E);
-
-	ar = vmcs_read32(GUEST_FS_AR_BYTES);
-	if ((ar >> 16) & 0x1) {
-		pr_info("depriv: repriv cpu%d FS unusable\n", cpu);
-	} else {
-		sel = vmcs_read16(GUEST_FS_SELECTOR);
-		loadsegment(fs, sel);
-		pr_info("depriv: repriv cpu%d FS = %#x\n", cpu, sel);
-	}
-
-	host_base = read_msr(MSR_FS_BASE);
-	guest_base = vmcs_readl(GUEST_FS_BASE);
-	pr_info("depriv: repriv cpu%d FS base %#lx : %#lx\n",
-		cpu, host_base, guest_base);
-
-	// never change GS BASE, which points to kernel mode per-CPU data
-	ar = vmcs_read32(GUEST_GS_AR_BYTES);
-	if ((ar >> 16) & 0x1) {
-		pr_info("depriv: repriv cpu%d GS unusable\n", cpu);
-	} else {
-		sel = vmcs_read16(GUEST_FS_SELECTOR);
-		load_gs_index(sel);
-		pr_info("depriv: repriv cpu%d FS %#x\n", cpu, sel);
-	}
-
-	host_base = read_msr(MSR_GS_BASE);
-	guest_base = vmcs_readl(GUEST_GS_BASE);
-	pr_info("depriv: repriv cpu%d GS base %#lx : %#lx\n",
-		cpu, host_base, guest_base);
-}
-
-static inline void vmx_repriv_cpu_ldtr(void)
-{
-	int cpu = smp_processor_id();
-	u16 guest_ldtr = vmcs_read16(GUEST_LDTR_SELECTOR), host_ldtr;
-
-	store_ldt(host_ldtr);
-	if (host_ldtr != guest_ldtr) {
-		pr_info("depriv: repriv cpu%d LDTR mismatch %#x : %#x\n",
-			cpu, host_ldtr, guest_ldtr);
-		load_ldt(guest_ldtr);
-	}
-}
-
-static inline void vmx_repriv_cpu_tr(void)
-{
-	int cpu = smp_processor_id();
-	u16 guest_tr = vmcs_read16(GUEST_TR_SELECTOR), host_tr;
-
-	store_tr(host_tr);
-	if (host_tr != guest_tr) {
-		pr_info("depriv: repriv cpu%d TR mismatch %#x : %#x\n",
-			cpu, host_tr, guest_tr);
-		if (guest_tr == 0)
-			return;
-		load_tr(guest_tr);
-		vmcs_write16(HOST_TR_SELECTOR, guest_tr);
-	}
-}
-
-#define REPRIV_DESC_TABLE(tag, TAG) do {						\
-	store_##tag##dt(&host_dt);							\
-	guest_dt_base = vmcs_readl(GUEST_##TAG##DTR_BASE);				\
-	if (host_dt.address != guest_dt_base)						\
-		pr_err("depriv: repriv cpu%d " #tag "dt base mismatch %#lx : %#lx\n",	\
-		       cpu, host_dt.address, guest_dt_base);				\
-	vmcs_writel(HOST_##TAG##DTR_BASE, guest_dt_base);				\
-	guest_dt_limit = vmcs_read32(GUEST_##TAG##DTR_LIMIT);				\
-	if (host_dt.size != guest_dt_limit) {						\
-		pr_debug("depriv: repriv cpu%d " #tag "dt limit mismatch %#x : %#x\n",	\
-			 cpu, host_dt.size , guest_dt_limit);				\
-		host_dt.size = guest_dt_limit;						\
-		load_##tag##dt(&host_dt);						\
-	}										\
-} while (0)
-
-static inline void vmx_repriv_cpu_desc_tables(void)
-{
-	int cpu = smp_processor_id();
-	struct desc_ptr host_dt;
-	unsigned long guest_dt_base;
-	u32 guest_dt_limit;
-
-	REPRIV_DESC_TABLE(g, G);
-	REPRIV_DESC_TABLE(i, I);
-
-	vmx_repriv_cpu_segments();
-	vmx_repriv_cpu_ldtr();
-	vmx_repriv_cpu_tr();
-}
-
-static void vmx_repriv_cpu(void *info)
-{
-	/* trigger a vmcall vmexit to reprivilege */
-#if 0
-	asm volatile("push %%rbp; vmcall; pop %%rbp" : : :
-		     "rax", "rcx", "rdx", "rbx",
-		     "rsi", "rdi", "r8", "r9", "r10",
-		     "r11", "r12", "r13", "r14", "r15");
-#endif
-
-	vmx_repriv_cpu_release_resources();
-}
-
-static void vmx_repriv_host(void)
-{
-	on_each_cpu(vmx_repriv_cpu, NULL, 0);
-	pr_info("depriv: reprivileged host\n");
-}
-
-/*
- * sync guest state to host w/o changing guest state
- */
-static void vmx_repriv_cpu_state(void)
-{
-	vmx_repriv_cpu_crs();
-	vmx_repriv_cpu_misc();
-	vmx_repriv_cpu_sysenter_msrs();
-	vmx_repriv_cpu_desc_tables();
-}
-
-#define DEPRIV_CONTINUE_IN_NON_ROOT_MODE(ins_len) do {				\
-	DEPRIV_SET_ROOT_MODE_TRAMPOLINE_STACK(ins_len);				\
-	vmcs_writel(GUEST_RIP, rip + ins_len);					\
-	return true;								\
-} while (0)
-
-#define DEPRIV_CONTINUE_IN_ROOT_MODE(ins_len) do {				\
-	vmx_repriv_cpu_state();							\
-	dump_vmcs();								\
-	pr_info("depriv: cpu%d exit reason %d skip %d bytes instruction and "	\
-		"continue in root mode\n",					\
-		cpu, reason, ins_len);						\
-	DEPRIV_SET_ROOT_MODE_TRAMPOLINE_STACK(ins_len);				\
-	return false;								\
-} while (0)
-
-#define DEPRIV_DUMP_GPRS do {							\
-	int i;									\
-	pr_info("depriv: cpu%d guest GPRs:\n", cpu);				\
-	for (i = __VCPU_REGS_RAX; i <= __VCPU_REGS_R15; i += 4) {		\
-		unsigned long *r = &regs[i];					\
-		pr_info("\tcpu%d\t%016lx %016lx %016lx %016lx\n",		\
-			cpu, *r, *(r + 1), *(r + 2), *(r + 3));			\
-	}									\
-} while (0)
-
-static inline bool is_canonical_address(u64 la, u64 cr4)
-{
-	return get_canonical(la, cr4 & X86_CR4_LA57 ? 57 : 48) == la;
-}
-
-/*
- * if guest_rip is a user level virtual address then it's mostly not valid in
- * root mode, because it is mapped using non-root mode cr3 and page tables.
- */
-static void dump_guest_insn(unsigned long guest_rip, int insn_len, char *insn)
-{
-	int i;
-
-	// don't try to access user level virtual address
-	if (!(guest_rip & 0xf000000000000000ul)) {
-		memset(insn, 0, MAX_INSN_SIZE * 3 + 1);
-		return;
-	}
-
-	if (insn_len == 0)
-		insn_len = MAX_INSN_SIZE;
-
-	for (i = 0; i < insn_len; i++)
-		sprintf(insn + 3 * i, " %02x", *(u8 *)(guest_rip + i));
-	insn[3 * i] = '\0';
-}
-
-static unsigned long cnt = 0;
-
-/*
- * the following fs base sync logic is confusing, but it happens on nested
- */
-static void dump_fsgs_base(unsigned long root_rsp)
-{
-	int cpu = smp_processor_id();
-	unsigned long base, last_base;
-
-	base = vmcs_readl(GUEST_FS_BASE);
-	last_base = *(unsigned long *)(root_rsp + 0x38);
-	if (base != last_base)
-		pr_info("depriv: cpu%d (%ld) FS base %#lx <== %#lx\n",
-			cpu, cnt, base, last_base);
-
-	base = vmcs_readl(GUEST_GS_BASE);
-	last_base = *(unsigned long *)(root_rsp + 0x30);
-	if (base != last_base)
-		pr_info("depriv: cpu%d (%ld) GS base %#lx <== %#lx\n",
-			cpu, cnt, base, last_base);
-}
-
-bool vmx_depriv_vmexit_handler(unsigned long *regs)
-{
-	unsigned long root_rsp = vmcs_readl(HOST_RSP);
-	unsigned long rip = vmcs_readl(GUEST_RIP);
-	unsigned long rsp = vmcs_readl(GUEST_RSP);
-	char insn[64];
-	int cpu = smp_processor_id();
-	u32 reason = vmcs_read32(VM_EXIT_REASON), insn_len = 0;
-	static bool enable_log = false;
-
-	++cnt;
-
-	regs[__VCPU_REGS_RSP] = rsp;
-
-	dump_fsgs_base(root_rsp);
-
-	pr_debug("depriv: cpu%d exit reason:%#x rip:%#lx rsp:%#lx\n",
-		 cpu, reason, rip, rsp);
-	if (enable_log) {
-		pr_info("depriv: cpu%d (%ld) exit reason:%#x rip:%#lx rsp:%#lx\n",
-			cpu, cnt, reason, rip, rsp);
-		enable_log = false;
-	}
-
-	if (reason & VMX_EXIT_REASONS_FAILED_VMENTRY) {
-		dump_vmcs();
-		vmx_check_guest_state();
-		// no need to sync guest state to host if we never enter guest
-		DEPRIV_CONTINUE_IN_ROOT_MODE(0);
-	}
-
-	switch (reason) {
-	case EXIT_REASON_EXCEPTION_NMI: {
-		bool continue_in_root_mode = true;
-		u32 intr_info = vmcs_read32(VM_EXIT_INTR_INFO);
-		u8 vector = intr_info & INTR_INFO_VECTOR_MASK;
-		u32 error_code = 0;
-
-		if (intr_info & INTR_INFO_DELIVER_CODE_MASK)
-			error_code = vmcs_read32(VM_EXIT_INTR_ERROR_CODE);
-
-		pr_info("depriv: cpu%d intr info: %#x error code %#x\n",
-			cpu, intr_info, error_code);
-
-		insn_len = vmcs_read32(VM_EXIT_INSTRUCTION_LEN);
-		dump_guest_insn(rip, insn_len, insn);
-
-		pr_info("depriv: cpu%d hit exception %d @ rip %#lx insn: %s\n",
-			cpu, vector, rip, insn);
-
-		if (is_invalid_opcode(intr_info)) {
-			pr_info("depriv: cpu%d hit UD @ rip %#lx insn: %s\n",
-				cpu, rip, insn);
-		} else if (is_page_fault(intr_info)) {
-			pr_info("depriv: cpu%d page fault @ %#lx with error code %#x\n",
-				cpu, vmcs_readl(EXIT_QUALIFICATION), error_code);
-			continue_in_root_mode = false;
-		} else if (is_gp_fault(intr_info)) {
-			continue_in_root_mode = false;
-		} else if (is_machine_check(intr_info))
-			pr_info("depriv: cpu%d to handle machine check in root mode\n", cpu);
-		else if (is_machine_check(intr_info) || is_nmi(intr_info))
-			pr_info("depriv: cpu%d to handle NMI in root mode\n", cpu);
-
-		if (continue_in_root_mode) {
-			pr_info("depriv: cpu%d hit exception %d, continue in root mode\n",
-				cpu, vector);
-			DEPRIV_CONTINUE_IN_ROOT_MODE(0);
-		} else {
-			u32 intr;
-
-			intr = vector | INTR_INFO_VALID_MASK;
-			if (vector == DF_VECTOR || vector == GP_VECTOR || vector == PF_VECTOR)
-				intr |= INTR_TYPE_HARD_EXCEPTION;
-			if (vector == GP_VECTOR || vector == PF_VECTOR)
-				intr |= INTR_INFO_DELIVER_CODE_MASK;
-			vmcs_write32(VM_ENTRY_INTR_INFO_FIELD, intr);
-			vmcs_write32(VM_ENTRY_INSTRUCTION_LEN, insn_len);
-			vmcs_write32(VM_ENTRY_EXCEPTION_ERROR_CODE, error_code);
-			pr_info("depriv: cpu%d injecting exception %d, continue in non-root mode\n",
-				cpu, vector);
-
-			pr_info("depriv: cpu%d (%ld) FS base %#lx : %#lx : %#lx\n",
-				cpu, cnt, read_msr(MSR_FS_BASE),
-				vmcs_readl(HOST_FS_BASE), vmcs_readl(GUEST_FS_BASE));
-
-			pr_info("depriv: cpu%d (%ld) GS base %#lx : %#lx : %#lx : %#lx\n",
-				cpu, cnt, read_msr(MSR_GS_BASE), vmcs_readl(HOST_GS_BASE),
-				vmcs_readl(GUEST_GS_BASE), read_msr(MSR_KERNEL_GS_BASE));
-
-			DEPRIV_DUMP_GPRS;
-			enable_log = true;
-			//dump_vmcs();
-			vmx_check_guest_state();
-
-			DEPRIV_CONTINUE_IN_NON_ROOT_MODE(0);
-		}
-	}
-
-	case EXIT_REASON_CR_ACCESS: {
-		unsigned long qualification = vmcs_readl(EXIT_QUALIFICATION);
-		int cr = qualification & 0xf;
-		int reg = (qualification >> 8) & 0xf;
-		unsigned long cr4 = vmcs_readl(GUEST_CR4);
-
-		if (cr != 3)
-			DEPRIV_CONTINUE_IN_ROOT_MODE(0);
-
-		if (cr4 & X86_CR4_PCIDE)
-			pr_info("depriv: cpu%d PCID enabled\n", cpu);
-		else
-			pr_info("depriv: cpu%d PCID disabled\n", cpu);
-
-		switch ((qualification >> 4) & 3) {
-		case 0: { /* mov to cr */
-			unsigned long cr3 = regs[reg];
-			unsigned long cr4 = vmcs_readl(GUEST_CR4);
-
-			if (!(cr4 & X86_CR4_PCIDE)) {
-				pr_info("depriv: cpu%d PCID disabled, fixing cr3\n", cpu);
-				cr3 &= ~X86_CR3_PCID_NOFLUSH;
-			} else
-				pr_info("depriv: cpu%d PCID enabled\n", cpu);
-
-			vmcs_writel(GUEST_CR3, cr3);
-
-			pr_info("depriv: cpu%d wrote cr3 from reg%d: %#lx (%#lx)\n",
-				cpu, cr, cr3, regs[reg]);
-			break;
-		}
-		case 1: /*mov from cr*/
-			regs[reg] = vmcs_readl(GUEST_CR3);
-
-			/* XXX RSP in regs won't be loaded into non-root mode */
-			if (reg == __VCPU_REGS_RSP)
-				vmcs_writel(GUEST_RSP, regs[reg]);
-
-			pr_debug("depriv: cpu%d read cr3 to reg%d: %#lx\n",
-				 cpu, cr, regs[reg]);
-			break;
-		}
-
-		insn_len = vmcs_read32(VM_EXIT_INSTRUCTION_LEN);
-		if (true) {
-			pr_debug("depriv: cpu%d accessed cr3 and continue in non-root mode\n", cpu);
-			DEPRIV_CONTINUE_IN_NON_ROOT_MODE(insn_len);
-		} else {
-			pr_info("depriv: cpu%d accessed cr3 and continue in root mode\n", cpu);
-			DEPRIV_CONTINUE_IN_ROOT_MODE(insn_len);
-		}
-	}
-
-	case EXIT_REASON_CPUID: {
-		static int cnt_cpuid_0x2 = 0;
-		pr_info("depriv: cpu%d cpuid[%#x]\n", cpu, (u32)regs[__VCPU_REGS_RAX]);
-
-		if ((u32)regs[__VCPU_REGS_RAX] == 0x2) {
-			cnt_cpuid_0x2++;
-			if (cnt_cpuid_0x2 == 20) {
-				//vmcs_write32(EXCEPTION_BITMAP, 0xffffffff);
-				pr_info("depriv: cpu%d all fault VM-exit enabeld\n", cpu);
-			}
-		}
-
-		native_cpuid((unsigned int *)&regs[__VCPU_REGS_RAX],
-			     (unsigned int *)&regs[__VCPU_REGS_RBX],
-			     (unsigned int *)&regs[__VCPU_REGS_RCX],
-			     (unsigned int *)&regs[__VCPU_REGS_RDX]);
-
-		insn_len = vmcs_read32(VM_EXIT_INSTRUCTION_LEN);
-		if (true) {
-			pr_debug("depriv: cpu%d executed cpuid and continue in non-root mode\n", cpu);
-			DEPRIV_CONTINUE_IN_NON_ROOT_MODE(insn_len);
-		} else {
-			pr_info("depriv: cpu%d executed cpuid and continue in root mode\n", cpu);
-			DEPRIV_CONTINUE_IN_ROOT_MODE(insn_len);
-		}
-	}
-
-	case EXIT_REASON_VMCALL:
-		insn_len = vmcs_read32(VM_EXIT_INSTRUCTION_LEN);
-
-		if (true) {
-			pr_info("depriv: cpu%d vmcall @ %#lx, continue in non-root mode\n",
-				cpu, rip);
-			DEPRIV_CONTINUE_IN_NON_ROOT_MODE(insn_len);
-		}
-
-		DEPRIV_CONTINUE_IN_ROOT_MODE(insn_len);
-		break;
-
-	case EXIT_REASON_MSR_READ: {
-		u32 ecx = (u32)regs[__VCPU_REGS_RCX];
-		unsigned long val;
-		static int cnt_0x3b = 0;
-
-		if (ecx == MSR_FS_BASE) {
-			val = vmcs_readl(GUEST_FS_BASE);
-			pr_info("depriv: cpu%d FS base MSR: %#lx\n", cpu, val);
-		} else if (ecx == MSR_GS_BASE) {
-			/*
-			 * never read GS base MSR directly when running in root mode,
-			 * which now points to kernel mode per-CPU data.
-			 */
-			val = vmcs_readl(GUEST_GS_BASE);
-			pr_info("depriv: cpu%d GS base MSR: %#lx\n", cpu, val);
-		} else if (rdmsrl_safe(ecx, (unsigned long long *)&val))
-			pr_info("depriv: cpu%d msr[%#x]: %#lx failed\n", cpu, ecx, val);
-
-		pr_info("depriv: cpu%d msr[%#x]: %#lx\n", cpu, ecx, val);
-#if 1
-		pr_info("depriv: cpu%d (%ld) FS base %#lx : %#lx : %#lx\n",
-			cpu, cnt, read_msr(MSR_FS_BASE),
-			vmcs_readl(HOST_FS_BASE), vmcs_readl(GUEST_FS_BASE));
-#endif
-		if (ecx == 0x3b) {
-			cnt_0x3b++;
-			if (cnt_0x3b == 2) {
-				//vmcs_write32(EXCEPTION_BITMAP, 0xffffffff);
-				pr_info("depriv: cpu%d all fault VM-exit enabeld\n", cpu);
-			}
-		}
-
-		*((u32 *)&regs[__VCPU_REGS_RAX]) = (u32)val;
-		*((u32 *)&regs[__VCPU_REGS_RDX]) = (u32)(val >> 32);
-
-		pr_info("depriv: cpu%d executed rdmsr and continue in non-root mode\n", cpu);
-		insn_len = vmcs_read32(VM_EXIT_INSTRUCTION_LEN);
-		DEPRIV_CONTINUE_IN_NON_ROOT_MODE(insn_len);
-	}
-
-	case EXIT_REASON_MSR_WRITE: {
-		bool continue_in_root_mode = false;
-		u32 ecx = (u32)regs[__VCPU_REGS_RCX];
-		unsigned long val = (unsigned long)(u32)regs[__VCPU_REGS_RAX] |
-				    ((unsigned long)(u32)regs[__VCPU_REGS_RDX] << 32);
-
-		if (ecx == MSR_GS_BASE) {
-			/*
-			 * never write GS base MSR directly when running in root mode,
-			 * which now points to kernel mode per-CPU data.
-			 */
-			pr_info("depriv: cpu%d GS base MSR = %#lx\n", cpu, val);
-		} else if (wrmsrl_safe(ecx, val))
-			continue_in_root_mode = true;
-
-		switch (ecx) {
-		case MSR_IA32_SPEC_CTRL:
-			pr_debug("depriv: cpu%d speculation control MSR = %#lx\n", cpu, val);
-			vmx_depriv_cpu_intercept_msr(MSR_IA32_PRED_CMD, true);
-			break;
-		case MSR_IA32_PRED_CMD:
-			pr_debug("depriv: cpu%d prediction command MSR = %#lx\n", cpu, val);
-			vmx_depriv_cpu_intercept_msr(MSR_IA32_PRED_CMD, true);
-			break;
-		case MSR_FS_BASE:
-#if 0
-			pr_info("depriv: cpu%d (%ld) before write FS base %#lx : %#lx : %#lx\n",
-				cpu, cnt, read_msr(MSR_FS_BASE),
-				vmcs_readl(HOST_FS_BASE), vmcs_readl(GUEST_FS_BASE));
-#endif
-			//pr_info("depriv: cpu%d FS base MSR = %#lx\n", cpu, val);
-			/*
-			 * guest FS base needs to be syned up with MSR_FS_BASE, thus we will
-			 * have correct FS base value in non-root mode after all future VM-entries.
-			 */
-			vmcs_writel(GUEST_FS_BASE, val);
-			/*
-			 * host FS base needs to be syned up with MSR_FS_BASE, thus we will
-			 * have correct FS base value in root mode after all future VM-exits.
-			 */
-			vmcs_writel(HOST_FS_BASE, val);
-#if 0
-			pr_info("depriv: cpu%d (%ld) after write FS base %#lx : %#lx : %#lx\n",
-				cpu, cnt, read_msr(MSR_FS_BASE),
-				vmcs_readl(HOST_FS_BASE), vmcs_readl(GUEST_FS_BASE));
-#endif
-			break;
-		case MSR_GS_BASE:
-			pr_info("depriv: cpu%d GS base MSR = %#lx\n", cpu, val);
-			/*
-			 * guest GS base needs to be syned up with MSR_GS_BASE, thus we will
-			 * have correct GS base value in non-root mode after all future VM-entries.
-			 */
-			vmcs_writel(GUEST_GS_BASE, val);
-			// never change HOST_GS_BASE, which points to kernel mode per-CPU data
-			break;
-		case MSR_KERNEL_GS_BASE:
-			pr_info("depriv: cpu%d kernel GS base MSR = %#lx\n", cpu, val);
-			break;
-		case MSR_IA32_TSCDEADLINE:
-			pr_debug("depriv: cpu%d TSC deadline timer MSR = %#lx\n", cpu, val);
-			break;
-		case 0x80b: // EOI virtualization msr
-			pr_debug("depriv: cpu%d EOI MSR = %#lx\n", cpu, val);
-			break;
-		default:
-			pr_info("depriv: cpu%d msr[%#x] = %#lx\n", cpu, ecx, val);
-			break;
-		}
-
-		if (unlikely(continue_in_root_mode)) {
-			pr_info("depriv: cpu%d msr[%#x] = %#lx failed, continue in root mode\n",
-				cpu, ecx, val);
-			DEPRIV_CONTINUE_IN_ROOT_MODE(0);
-		} else {
-			pr_debug("depriv: cpu%d executed wrmsr and continue in non-root mode\n", cpu);
-			insn_len = vmcs_read32(VM_EXIT_INSTRUCTION_LEN);
-			DEPRIV_CONTINUE_IN_NON_ROOT_MODE(insn_len);
-		}
-	}
-
-	default: /* continue in root mode */
-		pr_info("depriv: cpu%d exit reason=%#x\n", cpu, reason);
-		DEPRIV_CONTINUE_IN_ROOT_MODE(0);
-	}
-}
-
-static int __init setup_depriv_vmcs_config(void)
-{
-	u32 min, opt, min2, opt2;
-	u32 _pin_based_exec_control = 0;
-	u32 _cpu_based_exec_control = 0;
-	u32 _cpu_based_2nd_exec_control = 0;
-	u32 _vmexit_control = 0;
-	u32 _vmentry_control = 0;
-
-	memset(&depriv_vmcs_conf, 0, sizeof(depriv_vmcs_conf));
-	min = 0;
-	opt = CPU_BASED_USE_MSR_BITMAPS |
-	      CPU_BASED_ACTIVATE_SECONDARY_CONTROLS;
-	if (adjust_vmx_controls(min, opt, MSR_IA32_VMX_PROCBASED_CTLS,
-				&_cpu_based_exec_control) < 0)
-		return -EIO;
-
-	if (_cpu_based_exec_control & CPU_BASED_ACTIVATE_SECONDARY_CONTROLS) {
-		min2 = 0;
-		opt2 = SECONDARY_EXEC_RDTSCP |
-		       SECONDARY_EXEC_ENABLE_INVPCID |
-		       SECONDARY_EXEC_XSAVES;
-		if (adjust_vmx_controls(min2, opt2,
-					MSR_IA32_VMX_PROCBASED_CTLS2,
-					&_cpu_based_2nd_exec_control) < 0)
-			return -EIO;
-	}
-
-	if (_cpu_based_exec_control & CPU_BASED_CR3_LOAD_EXITING) {
-		// if EPT is available, it's OK to disable this control
-		_cpu_based_exec_control &= ~CPU_BASED_CR3_LOAD_EXITING;
-		pr_info("depriv: disabled cr3 load exiting\n");
-	}
-
-	if (_cpu_based_exec_control & CPU_BASED_CR3_STORE_EXITING) {
-		// if EPT is available, it's OK to disable this control
-		_cpu_based_exec_control &= ~CPU_BASED_CR3_STORE_EXITING;
-		pr_info("depriv: disabled cr3 store exiting\n");
-	}
-
-	if (_cpu_based_exec_control & CPU_BASED_INVLPG_EXITING) {
-		pr_info("depriv: invlpg causes VM exits\n");
-	}
-
-	min = VM_EXIT_SAVE_DEBUG_CONTROLS |
-	      VM_EXIT_HOST_ADDR_SPACE_SIZE;
-	opt = VM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL |
-	      VM_EXIT_LOAD_IA32_PAT |
-	      VM_EXIT_LOAD_IA32_EFER;
-	if (adjust_vmx_controls(min, opt, MSR_IA32_VMX_EXIT_CTLS,
-				&_vmexit_control) < 0)
-		return -EIO;
-
-	min = 0;
-	opt = 0;
-	if (adjust_vmx_controls(min, opt, MSR_IA32_VMX_PINBASED_CTLS,
-				&_pin_based_exec_control) < 0)
-		return -EIO;
-
-	if (cpu_has_broken_vmx_preemption_timer())
-		_pin_based_exec_control &= ~PIN_BASED_VMX_PREEMPTION_TIMER;
-	if (!(_cpu_based_2nd_exec_control &
-		SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY))
-		_pin_based_exec_control &= ~PIN_BASED_POSTED_INTR;
-
-	min = VM_ENTRY_LOAD_DEBUG_CONTROLS |
-	      VM_ENTRY_IA32E_MODE;
-	opt = VM_ENTRY_LOAD_IA32_PERF_GLOBAL_CTRL |
-	      VM_ENTRY_LOAD_IA32_PAT |
-	      VM_ENTRY_LOAD_IA32_EFER;
-	if (adjust_vmx_controls(min, opt, MSR_IA32_VMX_ENTRY_CTLS,
-				&_vmentry_control) < 0)
-		return -EIO;
-
-	depriv_vmcs_conf.pin_based_exec_ctrl = _pin_based_exec_control;
-	depriv_vmcs_conf.cpu_based_exec_ctrl = _cpu_based_exec_control;
-	depriv_vmcs_conf.cpu_based_2nd_exec_ctrl = _cpu_based_2nd_exec_control;
-	depriv_vmcs_conf.vmexit_ctrl         = _vmexit_control;
-	depriv_vmcs_conf.vmentry_ctrl        = _vmentry_control;
-
-	pr_info("depriv: pin based controls: %#x\n",
-		depriv_vmcs_conf.pin_based_exec_ctrl);
-	pr_info("depriv: processor based controls: %#x\n",
-		depriv_vmcs_conf.cpu_based_exec_ctrl);
-	pr_info("depriv: processor based 2nd controls: %#x\n",
-		depriv_vmcs_conf.cpu_based_2nd_exec_ctrl);
-	pr_info("depriv: vm exit controls: %#x\n",
-		depriv_vmcs_conf.vmexit_ctrl);
-	pr_info("depriv: vm entry controls: %#x\n",
-		depriv_vmcs_conf.vmentry_ctrl);
-
-	return 0;
-}
-
-static void __init vmx_depriv_host(void)
-{
-	if (setup_depriv_vmcs_config()) {
-		pr_err("depriv: error setting up deprivilege VMCS config\n");
-		return;
-	}
-
-	on_each_cpu(vmx_depriv_cpu, NULL, 0);
-}
-#endif
-
 static void vmx_exit(void)
 {
 #ifdef CONFIG_KEXEC_CORE
@@ -9357,438 +8172,3 @@ static int __init vmx_init(void)
 	return 0;
 }
 module_init(vmx_init);
-
-#if IS_ENABLED(CONFIG_KVM_INTEL_DEPRIV_HOST)
-static bool found_issue = false;
-
-#define CHECK(_c) do {							\
-	if (!(_c)) {							\
-		pr_err("depriv: guest state check failed (%d): %s\n",	\
-		       __LINE__, #_c);					\
-		found_issue = true;					\
-	}								\
-} while (0)
-
-#define CHECK_VMX_CTLS(ctls, val)					\
-	CHECK((~(ctls) & ((u32)(val))) == 0 &&				\
-	      ((ctls) & ~((u32)((val) >> 32))) == 0)
-
-#define CHECK_HOST_SEG(seg)						\
-	CHECK((vmcs_read16(HOST_##seg##_SELECTOR) & 0x7) == 0)
-
-#define CHECK_IS_ADDR_CANONICAL(a, gh)					\
-	CHECK(is_canonical_address(a, vmcs_readl(gh##_CR4)))
-
-#define CHECK_IS_HOST_ADDR_CANONICAL(a)					\
-	CHECK_IS_ADDR_CANONICAL(a, HOST)
-
-#define CHECK_IS_GUEST_ADDR_CANONICAL(a)				\
-	CHECK_IS_ADDR_CANONICAL(a, GUEST)
-
-#define CHECK_IS_VMCS_FIELD_ADDR_CANONICAL(vmcs_field, gh)		\
-	CHECK_IS_ADDR_CANONICAL(vmcs_readl(vmcs_field), gh)
-
-#define CHECK_IS_HOST_VMCS_FIELD_ADDR_CANONICAL(addr)			\
-	CHECK_IS_VMCS_FIELD_ADDR_CANONICAL(HOST_##addr, HOST)
-
-#define CHECK_IS_GUEST_VMCS_FIELD_ADDR_CANONICAL(addr)			\
-	CHECK_IS_VMCS_FIELD_ADDR_CANONICAL(GUEST_##addr, GUEST)
-
-#define CHECK_IS_HOST_TABLE_BASE_CANONICAL(tab)				\
-	CHECK_IS_HOST_VMCS_FIELD_ADDR_CANONICAL(tab##_BASE)
-
-#define CHECK_IS_GUEST_TABLE_BASE_CANONICAL(tab)			\
-	CHECK_IS_GUEST_VMCS_FIELD_ADDR_CANONICAL(tab##_BASE)
-
-#define PAGE_M (PAGE_SIZE - 1)
-
-static void vmx_check_guest_segment(u8 s, bool vm86_active,
-				    bool long_mode_active, bool unrestricted)
-{
-	u16 selector = vmcs_read16(kvm_vmx_segment_fields[s].selector);
-	unsigned long base = vmcs_readl(kvm_vmx_segment_fields[s].base);
-	u32 limit = vmcs_read32(kvm_vmx_segment_fields[s].limit);
-	u32 ar = vmcs_read32(kvm_vmx_segment_fields[s].ar_bytes);
-	bool unusable = !!(ar & VMX_AR_UNUSABLE_MASK);
-	bool present = !!(ar & VMX_AR_P_MASK);
-	unsigned rpl = selector & SEGMENT_RPL_MASK;
-	unsigned ti = selector & SEGMENT_TI_MASK;
-	unsigned type = ar & VMX_AR_TYPE_MASK;
-	unsigned dpl = VMX_AR_DPL(ar);
-
-	if (unusable) {
-		pr_err("depriv: seg%d sel=0x%04x, attr=0x%05x, limit=0x%08x, base=0x%016lx\n",
-		       s, selector, ar, limit, base);
-		return;
-	}
-
-	if (s == VCPU_SREG_TR)
-		CHECK(ti == 0);
-	else if (s == VCPU_SREG_LDTR)
-		CHECK(!present || ti == 0);
-	else if (s == VCPU_SREG_SS)
-		CHECK(vm86_active ||
-		      unrestricted ||
-		      rpl == (vmcs_read16(GUEST_CS_SELECTOR) & SEGMENT_RPL_MASK));
-
-	if (s < VCPU_SREG_LDTR && vm86_active)
-		CHECK(base == (unsigned long)(selector << 4));
-	if (s == VCPU_SREG_TR || s == VCPU_SREG_FS || s == VCPU_SREG_GS)
-		CHECK_IS_GUEST_ADDR_CANONICAL(base);
-	else if (s == VCPU_SREG_LDTR)
-		CHECK(!present || is_canonical_address(base, vmcs_readl(GUEST_CR4)));
-	else if (s == VCPU_SREG_CS)
-		CHECK(!((u32)(base >> 32)));
-	else
-		CHECK(!present || !((u32)(base >> 32)));
-
-	if (s < VCPU_SREG_LDTR && vm86_active)
-		CHECK(limit == 0xffff);
-
-	if (s < VCPU_SREG_LDTR)
-		if (vm86_active)
-			CHECK(ar == 0xF3);
-		else {
-			// Type
-			if (s == VCPU_SREG_CS)
-				CHECK((unrestricted && type == 3) ||
-				      (type & 9) == 9);
-			else if (s == VCPU_SREG_SS)
-				CHECK(!present || type == 3 || type == 7);
-			else if (present)
-				CHECK((type & 1) == 1 &&
-				      ((type & 8) == 0 || (type & 2) == 2));
-			// S
-			if (s == VCPU_SREG_TR)
-				CHECK(!(ar & VMX_AR_S_MASK));
-			if (s != VCPU_SREG_TR && present) {
-				CHECK(ar & VMX_AR_S_MASK);
-			}
-			// DPL
-			if (s == VCPU_SREG_CS)
-				if (type == 3) /* data segment => real mode */
-					CHECK(dpl == 0);
-				else if ((type & 4) == 0) /* non-conforming code segment */
-					CHECK(dpl == VMX_AR_DPL(vmcs_read32(GUEST_SS_AR_BYTES)));
-				else /* conforming code segment */
-					CHECK(dpl <= VMX_AR_DPL(vmcs_read32(GUEST_SS_AR_BYTES)));
-			else if (s == VCPU_SREG_SS)
-				if (!(vmcs_readl(GUEST_CR0) & X86_CR0_PE) ||
-				    (vmcs_read32(GUEST_CS_AR_BYTES) & VMX_AR_TYPE_MASK) == 3)
-					CHECK(dpl == 0);
-				else
-					CHECK(dpl == rpl);
-			else if (!unrestricted && present && type <= 11)
-				/* not a conforming code segment */
-				CHECK(dpl >= rpl);
-			// P
-			if (s == VCPU_SREG_CS || present)
-				CHECK(present);
-			// reserved bits
-			if (s == VCPU_SREG_CS || present)
-				CHECK((ar & 0xfffe0f00) == 0);
-			// D/B
-			if (s == VCPU_SREG_CS)
-				CHECK(!long_mode_active || !(ar & VMX_AR_L_MASK) || !(ar & VMX_AR_DB_MASK));
-			// G
-			if (s == VCPU_SREG_CS || present) {
-				CHECK((limit & PAGE_M) == PAGE_M || !(ar & VMX_AR_G_MASK));
-				CHECK(limit >> 20 == 0 || ar & VMX_AR_G_MASK);
-			}
-		}
-	else if (s == VCPU_SREG_TR) {
-		CHECK((!long_mode_active && type == 3) || type == 11);
-		CHECK(!(ar & VMX_AR_S_MASK));
-		CHECK(present);
-		CHECK((ar & 0xfffe0f00) == 0);
-		CHECK((limit & PAGE_M) == PAGE_M || !(ar & VMX_AR_G_MASK));
-		CHECK(limit >> 20 == 0 || ar & VMX_AR_G_MASK);
-	} else if (s == VCPU_SREG_LDTR && present) {
-		CHECK(type == 2);
-		CHECK(!(ar & VMX_AR_S_MASK));
-		CHECK(present);
-		CHECK((ar & 0xfffe0f00) == 0);
-		CHECK((limit & PAGE_M) == PAGE_M || !(ar & VMX_AR_G_MASK));
-		CHECK(limit >> 20 == 0 || ar & VMX_AR_G_MASK);
-	}
-}
-
-void vmx_check_guest_state(void)
-{
-	u32 vmentry_ctl = vmcs_read32(VM_ENTRY_CONTROLS);
-	u32 vmexit_ctl = vmcs_read32(VM_EXIT_CONTROLS);
-	u32 cpu_based_exec_ctrl = vmcs_read32(CPU_BASED_VM_EXEC_CONTROL);
-	u32 pin_based_exec_ctrl = vmcs_read32(PIN_BASED_VM_EXEC_CONTROL);
-	u32 secondary_exec_control = cpu_has_secondary_exec_ctrls() ?
-		vmcs_read32(SECONDARY_VM_EXEC_CONTROL) : 0;
-	u64 rflags = vmcs_readl(GUEST_RFLAGS);
-	bool vm86_active = rflags & X86_EFLAGS_VM;
-	bool long_mode_active = vmentry_ctl & VM_ENTRY_IA32E_MODE;
-	u32 vmentry_intr_info = vmcs_read32(VM_ENTRY_INTR_INFO_FIELD);
-	bool unrestricted = cpu_has_secondary_exec_ctrls() ?
-		secondary_exec_control & SECONDARY_EXEC_UNRESTRICTED_GUEST :
-		false;
-
-	u64 cr0_must_be_zeros = ~read_msr(MSR_IA32_VMX_CR0_FIXED1);
-	u64 host_cr0_must_be_ones = read_msr(MSR_IA32_VMX_CR0_FIXED0);
-	u64 guest_cr0_must_be_ones = host_cr0_must_be_ones &
-		~(unrestricted ? X86_CR0_PG | X86_CR0_PE : 0);
-	u64 cr4_must_be_zeros = ~read_msr(MSR_IA32_VMX_CR4_FIXED1);
-	u64 cr4_must_be_ones = read_msr(MSR_IA32_VMX_CR4_FIXED0);
-
-	u64 pin_based_ctrls = read_msr(MSR_IA32_VMX_TRUE_PINBASED_CTLS);
-	u64 proc_based_ctrls = read_msr(MSR_IA32_VMX_TRUE_PROCBASED_CTLS);
-
-	u64 debug_ctrl = vmcs_read64(GUEST_IA32_DEBUGCTL);
-	u32 activity_state = vmcs_read32(GUEST_ACTIVITY_STATE);
-	u32 interruptibility = vmcs_read32(GUEST_INTERRUPTIBILITY_INFO);
-	u64 pending_dbg_exceptions = vmcs_readl(GUEST_PENDING_DBG_EXCEPTIONS);
-
-	u8 s;
-
-	found_issue = false;
-
-	// 26.2.1.1: VM-Execution Control Fields
-	CHECK_VMX_CTLS(pin_based_exec_ctrl, pin_based_ctrls);
-	CHECK_VMX_CTLS(cpu_based_exec_ctrl, proc_based_ctrls);
-
-	if (cpu_based_exec_ctrl & CPU_BASED_ACTIVATE_SECONDARY_CONTROLS) {
-		u64 secondary_ctrls = read_msr(MSR_IA32_VMX_PROCBASED_CTLS2);
-		CHECK_VMX_CTLS(secondary_exec_control, secondary_ctrls);
-	}
-
-	CHECK(vmcs_read32(CR3_TARGET_COUNT) == 0);
-
-	CHECK(!(cpu_based_exec_ctrl & CPU_BASED_USE_IO_BITMAPS));
-	CHECK(!(cpu_based_exec_ctrl & CPU_BASED_TPR_SHADOW));
-	CHECK(pin_based_exec_ctrl & PIN_BASED_NMI_EXITING ||
-	      !(pin_based_exec_ctrl & PIN_BASED_VIRTUAL_NMIS));
-	CHECK(pin_based_exec_ctrl & PIN_BASED_VIRTUAL_NMIS ||
-	      !(cpu_based_exec_ctrl & CPU_BASED_NMI_WINDOW_EXITING));
-	CHECK(!(pin_based_exec_ctrl & PIN_BASED_POSTED_INTR));
-
-	// 26.2.1.2: VM-Exit Control Fields
-	CHECK_VMX_CTLS(vmexit_ctl, read_msr(MSR_IA32_VMX_TRUE_EXIT_CTLS));
-	CHECK(pin_based_exec_ctrl & PIN_BASED_VMX_PREEMPTION_TIMER ||
-	      !(vmexit_ctl & VM_EXIT_SAVE_VMX_PREEMPTION_TIMER));
-
-	// 26.2.1.3: VM-Entry Control Fields
-	CHECK_VMX_CTLS(vmentry_ctl, read_msr(MSR_IA32_VMX_TRUE_ENTRY_CTLS));
-	if (vmentry_intr_info & INTR_INFO_VALID_MASK) {
-		u32 type = vmentry_intr_info & INTR_INFO_INTR_TYPE_MASK;
-		u8 vector = vmentry_intr_info & INTR_INFO_VECTOR_MASK;
-		u32 insn_len = vmcs_read32(VM_ENTRY_INSTRUCTION_LEN);
-		bool has_error_code = vmentry_intr_info & INTR_INFO_DELIVER_CODE_MASK;
-
-		CHECK(type != INTR_TYPE_RESERVED);
-		CHECK((u32)(proc_based_ctrls >> 32) & CPU_BASED_MONITOR_TRAP_FLAG ||
-		      type != INTR_TYPE_OTHER_EVENT);
-		CHECK(type != INTR_TYPE_NMI_INTR || vector == BP_VECTOR);
-		CHECK(type != INTR_TYPE_HARD_EXCEPTION || vector <= 31);
-		CHECK(type != INTR_TYPE_OTHER_EVENT || vector == DE_VECTOR);
-
-		CHECK(has_error_code == (vmcs_readl(GUEST_CR0) & X86_CR0_PE &&
-					 (type == INTR_TYPE_HARD_EXCEPTION &&
-					  (vector == DF_VECTOR ||
-					   vector == TS_VECTOR ||
-					   vector == NP_VECTOR ||
-					   vector == SS_VECTOR ||
-					   vector == GP_VECTOR ||
-					   vector == PF_VECTOR ||
-					   vector == AC_VECTOR))));
-
-		CHECK(!(vmentry_intr_info &
-			(INTR_INFO_RESVD_BITS_MASK | INTR_INFO_UNBLOCK_NMI)));
-
-		CHECK(!has_error_code ||
-		      (vmcs_read32(VM_ENTRY_EXCEPTION_ERROR_CODE) & 0xffff8000) == 0);
-		CHECK(!(type == INTR_TYPE_SOFT_INTR ||
-			type == INTR_TYPE_PRIV_SW_EXCEPTION ||
-			type == INTR_TYPE_SOFT_EXCEPTION) ||
-		      insn_len < MAX_INSN_SIZE);
-	}
-
-	CHECK(!(vmentry_ctl & VM_ENTRY_SMM));
-	CHECK(!(vmentry_ctl & VM_ENTRY_DEACT_DUAL_MONITOR));
-
-	// 26.2.2: Checks on Host Control Registers and MSRs
-	CHECK((~vmcs_readl(HOST_CR0) & cr0_must_be_zeros) == cr0_must_be_zeros);
-	CHECK((vmcs_readl(HOST_CR0) & host_cr0_must_be_ones) == host_cr0_must_be_ones);
-
-	CHECK((~vmcs_readl(HOST_CR4) & cr4_must_be_zeros) == cr4_must_be_zeros);
-	CHECK((vmcs_readl(HOST_CR4) & cr4_must_be_ones) == cr4_must_be_ones);
-
-	CHECK_IS_HOST_VMCS_FIELD_ADDR_CANONICAL(IA32_SYSENTER_ESP);
-	CHECK_IS_HOST_VMCS_FIELD_ADDR_CANONICAL(IA32_SYSENTER_EIP);
-
-	if (vmexit_ctl & VM_EXIT_LOAD_IA32_PAT) {
-		u64 pat = vmcs_read64(HOST_IA32_PAT);
-		unsigned i;
-		for (i = 0; i < 8; i++) {
-			u8 byte = pat & 0xff;
-			CHECK(byte < 8 && byte != 2 && byte != 3);
-			pat >>= 8;
-		}
-	}
-
-	if (vmexit_ctl & VM_EXIT_LOAD_IA32_EFER) {
-		u64 efer = vmcs_read64(HOST_IA32_EFER);
-		CHECK((efer & 0xffffffffffff0200ull) == 0);
-		if (vmexit_ctl & VM_EXIT_HOST_ADDR_SPACE_SIZE)
-			CHECK((efer & (EFER_LMA | EFER_LME)) ==
-			      (EFER_LMA | EFER_LME));
-		else
-			CHECK((efer & (EFER_LMA | EFER_LME)) == 0);
-	}
-
-	// 26.2.3: Checks on Host Segment and Descriptor-Table Registers
-	CHECK_HOST_SEG(ES);
-	CHECK_HOST_SEG(CS);
-	CHECK_HOST_SEG(SS);
-	CHECK_HOST_SEG(DS);
-	CHECK_HOST_SEG(FS);
-	CHECK_HOST_SEG(GS);
-	CHECK_HOST_SEG(TR);
-
-	CHECK(vmcs_read16(HOST_CS_SELECTOR) != 0);
-	CHECK(vmcs_read16(HOST_TR_SELECTOR) != 0);
-
-	CHECK_IS_HOST_TABLE_BASE_CANONICAL(FS);
-	CHECK_IS_HOST_TABLE_BASE_CANONICAL(GS);
-	CHECK_IS_HOST_TABLE_BASE_CANONICAL(GDTR);
-	CHECK_IS_HOST_TABLE_BASE_CANONICAL(IDTR);
-	CHECK_IS_HOST_TABLE_BASE_CANONICAL(TR);
-
-	// 26.2.4: Checks Related to Address-Space Size
-	CHECK(vmexit_ctl & VM_EXIT_HOST_ADDR_SPACE_SIZE);
-
-	CHECK(vmcs_readl(HOST_CR4) & X86_CR4_PAE);
-	CHECK_IS_HOST_VMCS_FIELD_ADDR_CANONICAL(RIP);
-
-	// 26.3.1.1: Checks on Guest Control Registers, Debug Registers, and MSRs
-	CHECK((~vmcs_readl(GUEST_CR0) & cr0_must_be_zeros) ==
-	      cr0_must_be_zeros);
-	CHECK((vmcs_readl(GUEST_CR0) & guest_cr0_must_be_ones) ==
-	      guest_cr0_must_be_ones);
-
-	CHECK((~vmcs_readl(GUEST_CR4) & cr4_must_be_zeros) == cr4_must_be_zeros);
-	CHECK((vmcs_readl(GUEST_CR4) & cr4_must_be_ones) == cr4_must_be_ones);
-
-	if (vmentry_ctl & VM_ENTRY_LOAD_DEBUG_CONTROLS) {
-		u64 debug_ctrl_reserved = 0xffffffffffff003cull;
-		CHECK((debug_ctrl & debug_ctrl_reserved) == 0);
-	}
-
-	CHECK(!long_mode_active || vmcs_readl(GUEST_CR4) & X86_CR4_PAE);
-
-	if (vmentry_ctl & VM_ENTRY_LOAD_DEBUG_CONTROLS)
-		CHECK(!((u32)(vmcs_readl(GUEST_DR7) >> 32)));
-
-	CHECK_IS_GUEST_VMCS_FIELD_ADDR_CANONICAL(SYSENTER_ESP);
-	CHECK_IS_GUEST_VMCS_FIELD_ADDR_CANONICAL(SYSENTER_EIP);
-
-	if (vmentry_ctl & VM_ENTRY_LOAD_IA32_PAT) {
-		u64 pat = vmcs_read64(GUEST_IA32_PAT);
-		unsigned i;
-		for (i = 0; i < 8; i++) {
-			u8 byte = pat & 0xff;
-			CHECK(byte < 8 && byte != 2 && byte != 3);
-			pat >>= 8;
-		}
-	}
-
-	if (vmentry_ctl & VM_ENTRY_LOAD_IA32_EFER) {
-		u64 efer = vmcs_read64(GUEST_IA32_EFER);
-		CHECK((efer & 0xffffffffffff0200ull) == 0);
-		if ((vmcs_readl(GUEST_CR0) & X86_CR0_PG) != 0) {
-			if (long_mode_active)
-				CHECK((efer & (EFER_LMA | EFER_LME)) ==
-				      (EFER_LMA | EFER_LME));
-			else
-				CHECK((efer & (EFER_LMA | EFER_LME)) == 0);
-		}
-	}
-
-	if (vmentry_ctl & VM_ENTRY_LOAD_BNDCFGS) {
-		u64 bndcfgs = vmcs_read64(GUEST_BNDCFGS);
-		CHECK_IS_GUEST_ADDR_CANONICAL(bndcfgs & 0xfffffffffffff000ull);
-		CHECK((bndcfgs & 0x00000ffc) == 0);
-	}
-
-	// 26.3.1.2: Checks on Guest Segment Registers
-	for (s = VCPU_SREG_ES; s <= VCPU_SREG_LDTR; s++)
-		vmx_check_guest_segment(s, vm86_active, long_mode_active, unrestricted);
-
-	// 26.3.1.3: Checks on Guest Descriptor-Table Registers
-	CHECK_IS_GUEST_TABLE_BASE_CANONICAL(GDTR);
-	CHECK_IS_GUEST_TABLE_BASE_CANONICAL(IDTR);
-
-	// 26.3.1.4: Checks on Guest RIP and RFLAGS
-	CHECK((long_mode_active && vmcs_read32(kvm_vmx_segment_fields[VCPU_SREG_CS].ar_bytes) & VMX_AR_L_MASK) ||
-	      (u32)(vmcs_readl(GUEST_RIP) >> 32) == 0);
-	CHECK_IS_GUEST_VMCS_FIELD_ADDR_CANONICAL(RIP);
-
-	CHECK((rflags & ((-1ull << 22) | (1 << 15) | (1 << 5) | (1 << 3))) == 0);
-	CHECK((rflags & X86_EFLAGS_FIXED) != 0);
-	CHECK(!long_mode_active || !vm86_active);
-	CHECK(((vmentry_intr_info & (INTR_INFO_VALID_MASK |
-				     INTR_INFO_INTR_TYPE_MASK)) !=
-	       (INTR_INFO_VALID_MASK | INTR_TYPE_EXT_INTR)) ||
-	      (rflags & X86_EFLAGS_IF));
-
-	// 26.3.1.5: Checks on Guest Non-Register State
-	CHECK(activity_state <= GUEST_ACTIVITY_WAIT_SIPI);
-	CHECK(activity_state != GUEST_ACTIVITY_HLT||
-	      ((vmcs_read32(kvm_vmx_segment_fields[VCPU_SREG_SS].ar_bytes) >> 5) & 0x3) == 0);
-	CHECK(!(interruptibility & (GUEST_INTR_STATE_STI | GUEST_INTR_STATE_MOV_SS)) ||
-	      activity_state == GUEST_ACTIVITY_ACTIVE);
-	if (vmentry_intr_info & INTR_INFO_VALID_MASK) {
-		u32 type = vmentry_intr_info & INTR_INFO_INTR_TYPE_MASK;
-		u8 vector = vmentry_intr_info & INTR_INFO_VECTOR_MASK;
-
-		CHECK(activity_state != GUEST_ACTIVITY_HLT ||
-		      (type == INTR_TYPE_EXT_INTR ||
-		       type == INTR_TYPE_NMI_INTR ||
-		       (type == INTR_TYPE_HARD_EXCEPTION &&
-			(vector == DB_VECTOR || vector == MC_VECTOR)) ||
-		       (type == INTR_TYPE_OTHER_EVENT && vector == DB_VECTOR)));
-		CHECK(activity_state != GUEST_ACTIVITY_SHUTDOWN ||
-		      (type == INTR_TYPE_NMI_INTR ||
-		       (type == INTR_TYPE_HARD_EXCEPTION && vector == MC_VECTOR)));
-		CHECK(activity_state != GUEST_ACTIVITY_WAIT_SIPI);
-	}
-
-	CHECK((interruptibility & 0xFFFFFFE0) == 0);
-	CHECK((interruptibility & (GUEST_INTR_STATE_STI | GUEST_INTR_STATE_MOV_SS)) !=
-	      (GUEST_INTR_STATE_STI | GUEST_INTR_STATE_MOV_SS));
-	CHECK(rflags & X86_EFLAGS_IF || !(interruptibility & GUEST_INTR_STATE_STI));
-	CHECK(((vmentry_intr_info & (INTR_INFO_VALID_MASK | INTR_INFO_INTR_TYPE_MASK)) !=
-	       (INTR_INFO_VALID_MASK | INTR_TYPE_EXT_INTR)) ||
-	      (interruptibility & (GUEST_INTR_STATE_STI | GUEST_INTR_STATE_MOV_SS)) == 0);
-	CHECK(((vmentry_intr_info & (INTR_INFO_VALID_MASK | INTR_INFO_INTR_TYPE_MASK)) !=
-	       (INTR_INFO_VALID_MASK | INTR_TYPE_NMI_INTR)) ||
-	      (interruptibility & GUEST_INTR_STATE_MOV_SS) == 0);
-	CHECK((interruptibility & GUEST_INTR_STATE_SMI) == 0);
-	// Some processors require the following check; others do not.
-	CHECK(((vmentry_intr_info & (INTR_INFO_VALID_MASK | INTR_INFO_INTR_TYPE_MASK)) !=
-	       (INTR_INFO_VALID_MASK | INTR_TYPE_NMI_INTR)) ||
-	      (interruptibility & GUEST_INTR_STATE_STI) == 0);
-	CHECK(((vmentry_intr_info & (INTR_INFO_VALID_MASK | INTR_INFO_INTR_TYPE_MASK)) !=
-	       (INTR_INFO_VALID_MASK | INTR_TYPE_NMI_INTR)) ||
-	      (pin_based_exec_ctrl & PIN_BASED_VIRTUAL_NMIS) == 0 ||
-	      (interruptibility & GUEST_INTR_STATE_NMI) == 0);
-
-	CHECK((u32)(pending_dbg_exceptions >> 32) == 0);
-	CHECK((pending_dbg_exceptions & 0xfffeaff0) == 0);
-	if ((interruptibility & (GUEST_INTR_STATE_STI | GUEST_INTR_STATE_MOV_SS)) != 0 ||
-	    activity_state == GUEST_ACTIVITY_HLT) {
-		CHECK(!((rflags & X86_EFLAGS_TF) && !(debug_ctrl & 0x2)) ||
-		      (pending_dbg_exceptions & 0x00004000));
-		CHECK(!(!(rflags & X86_EFLAGS_TF) || (debug_ctrl & 0x2)) ||
-		      !(pending_dbg_exceptions & 0x00004000));
-	}
-
-	if (!found_issue)
-		pr_info("depriv: validated VMCS guest state\n");
-}
-#endif
-- 
2.34.1

