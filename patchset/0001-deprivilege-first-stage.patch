From 336a622a4c97e74e92766deca4a6e67d011c23a3 Mon Sep 17 00:00:00 2001
From: Xin Li <fantry@msn.com>
Date: Tue, 9 Jun 2020 16:51:18 -0700
Subject: [PATCH 001/140] deprivilege first stage

---
 arch/x86/entry/entry_64.S  |    9 +
 arch/x86/include/asm/vmx.h |    2 +
 arch/x86/kvm/Kconfig       |    8 +
 arch/x86/kvm/vmx/vmenter.S |  199 +++++
 arch/x86/kvm/vmx/vmx.c     | 1521 +++++++++++++++++++++++++++++++++++-
 arch/x86/kvm/vmx/vmx.h     |    5 +
 virt/kvm/kvm_main.c        |    9 +
 7 files changed, 1750 insertions(+), 3 deletions(-)

diff --git a/arch/x86/entry/entry_64.S b/arch/x86/entry/entry_64.S
index 466df3e50276..84c41092e84b 100644
--- a/arch/x86/entry/entry_64.S
+++ b/arch/x86/entry/entry_64.S
@@ -1414,6 +1414,15 @@ nmi_restore:
 	iretq
 SYM_CODE_END(asm_exc_nmi)
 
+SYM_CODE_START(vmx_depriv_continue_in_root_mode)
+	push	%rax
+	mov	0x30(%rsp), %rax
+	mov	%rax, %cr3
+	pop	%rax
+	iretq
+SYM_CODE_END(vmx_depriv_continue_in_root_mode)
+EXPORT_SYMBOL(vmx_depriv_continue_in_root_mode)
+
 #ifndef CONFIG_IA32_EMULATION
 /*
  * This handles SYSCALL from 32-bit code.  There is no way to program
diff --git a/arch/x86/include/asm/vmx.h b/arch/x86/include/asm/vmx.h
index 0ffaa3156a4e..b9522d02e571 100644
--- a/arch/x86/include/asm/vmx.h
+++ b/arch/x86/include/asm/vmx.h
@@ -611,4 +611,6 @@ enum vmx_l1d_flush_state {
 
 extern enum vmx_l1d_flush_state l1tf_vmx_mitigation;
 
+extern asmlinkage void vmx_depriv_continue_in_root_mode(void);
+
 #endif
diff --git a/arch/x86/kvm/Kconfig b/arch/x86/kvm/Kconfig
index 2b1548da00eb..533ba008322c 100644
--- a/arch/x86/kvm/Kconfig
+++ b/arch/x86/kvm/Kconfig
@@ -86,6 +86,14 @@ config KVM_INTEL
 	  To compile this as a module, choose M here: the module
 	  will be called kvm-intel.
 
+config KVM_INTEL_DEPRIV_HOST
+	tristate "KVM host deprivilege for Intel (and compatible) processors support"
+	depends on KVM_INTEL
+	help
+	  Provides support for KVM host deprivilege.
+
+	  If unsure, say N.
+
 config X86_SGX_KVM
 	bool "Software Guard eXtensions (SGX) Virtualization"
 	depends on X86_SGX && KVM_INTEL
diff --git a/arch/x86/kvm/vmx/vmenter.S b/arch/x86/kvm/vmx/vmenter.S
index 435c187927c4..23f70b795c38 100644
--- a/arch/x86/kvm/vmx/vmenter.S
+++ b/arch/x86/kvm/vmx/vmenter.S
@@ -92,6 +92,191 @@ SYM_FUNC_START(vmx_vmexit)
 	RET
 SYM_FUNC_END(vmx_vmexit)
 
+SYM_FUNC_START(vmx_depriv_test_vmcall)
+	/* vmcall */
+	.byte 0x0f, 0x01, 0xc1
+	ret
+SYM_FUNC_END(vmx_depriv_test_vmcall)
+
+SYM_FUNC_START(vmx_depriv_execute_special_insn_in_root_mode)
+	push %rax
+	mov %_ASM_ARG1, %_ASM_AX
+
+	push %r15
+	push %r14
+	push %r13
+	push %r12
+	push %r11
+	push %r10
+	push %r9
+	push %r8
+	push %rdi
+	push %rsi
+	push %rbp
+	push %rsp
+	push %rbx
+	push %rdx
+	push %rcx
+	push %rax
+
+	mov VCPU_RCX(%_ASM_AX), %_ASM_CX
+	mov VCPU_RDX(%_ASM_AX), %_ASM_DX
+	mov VCPU_RBX(%_ASM_AX), %_ASM_BX
+	mov VCPU_RBP(%_ASM_AX), %_ASM_BP
+	mov VCPU_RSI(%_ASM_AX), %_ASM_SI
+	mov VCPU_RDI(%_ASM_AX), %_ASM_DI
+	mov VCPU_R8 (%_ASM_AX),  %r8
+	mov VCPU_R9 (%_ASM_AX),  %r9
+	mov VCPU_R10(%_ASM_AX), %r10
+	mov VCPU_R11(%_ASM_AX), %r11
+	mov VCPU_R12(%_ASM_AX), %r12
+	mov VCPU_R13(%_ASM_AX), %r13
+	mov VCPU_R14(%_ASM_AX), %r14
+	mov VCPU_R15(%_ASM_AX), %r15
+	// overwirte RAX in last step
+	mov VCPU_RAX(%_ASM_AX), %_ASM_AX
+
+SYM_CODE_START(vmx_depriv_special_insn)
+	.byte 0x48, 0x0f, 0xc7, 0x1f
+	nop
+	nop
+	nop
+	nop
+	nop
+	nop
+	nop
+	nop
+	nop
+	nop
+	nop
+	nop
+
+	pop %rax
+	mov %_ASM_CX,   VCPU_RCX(%_ASM_AX)
+	mov %_ASM_DX,   VCPU_RDX(%_ASM_AX)
+	mov %_ASM_BX,   VCPU_RBX(%_ASM_AX)
+	mov %_ASM_BP,   VCPU_RBP(%_ASM_AX)
+	mov %_ASM_SI,   VCPU_RSI(%_ASM_AX)
+	mov %_ASM_DI,   VCPU_RDI(%_ASM_AX)
+	mov %r8,  VCPU_R8 (%_ASM_AX)
+	mov %r9,  VCPU_R9 (%_ASM_AX)
+	mov %r10, VCPU_R10(%_ASM_AX)
+	mov %r11, VCPU_R11(%_ASM_AX)
+	mov %r12, VCPU_R12(%_ASM_AX)
+	mov %r13, VCPU_R13(%_ASM_AX)
+	mov %r14, VCPU_R14(%_ASM_AX)
+	mov %r15, VCPU_R15(%_ASM_AX)
+
+	pop %rcx
+	pop %rdx
+	pop %rbx
+	pop %rbp
+	pop %rbp
+	pop %rsi
+	pop %rdi
+	pop %r8
+	pop %r9
+	pop %r10
+	pop %r11
+	pop %r12
+	pop %r13
+	pop %r14
+	pop %r15
+
+	pop %rax
+	ret
+SYM_FUNC_END(vmx_depriv_execute_special_insn_in_root_mode)
+
+SYM_FUNC_START(vmx_depriv_vmexit)
+	push %r15
+	push %r14
+	push %r13
+	push %r12
+	push %r11
+	push %r10
+	push %r9
+	push %r8
+	push %rdi
+	push %rsi
+	push %rbp
+	push %rsp
+	push %rbx
+	push %rdx
+	push %rcx
+	push %rax
+
+	xor %rax, %rax
+	xor %rcx, %rcx
+	xor %rdx, %rdx
+	xor %rbx, %rbx
+	xor %rbp, %rbp
+	xor %rsi, %rsi
+	xor %rdi, %rdi
+	xor %r8,  %r8
+	xor %r9,  %r9
+	xor %r10, %r10
+	xor %r11, %r11
+	xor %r12, %r12
+	xor %r13, %r13
+	xor %r14, %r14
+	xor %r15, %r15
+
+	mov %rsp, %_ASM_ARG1
+	call vmx_depriv_vmexit_handler
+	cmpb $1, %al
+
+	pop %rax
+	pop %rcx
+	pop %rdx
+	pop %rbx
+	pop %r8
+	pop %rbp
+	pop %rsi
+	pop %rdi
+	pop %r8
+	pop %r9
+	pop %r10
+	pop %r11
+	pop %r12
+	pop %r13
+	pop %r14
+	pop %r15
+
+	jne 1f
+	/*
+	 * upon vmx_depriv_vmexit_handler returning true, continue non-root mode
+	 */
+	vmresume
+
+	/*
+	 * VM resume failed, switch back to root mode with guest stack
+	 */
+	mov (%rsp), %rax
+	mov %rax, %rsp
+	xor %eax, %eax
+	mov $2, %eax
+	/* to instruction immediately after vmx_depriv */
+	ret
+
+	/*
+	 * upon vmx_depriv_vmexit_handler returning false, switch back to root
+	 * mode with guest stack
+	 */
+1:	jmp vmx_depriv_continue_in_root_mode
+
+2:	mov (%rsp), %rax
+	mov %rax, %rsp
+	pop %rax
+	/*
+	 * sync up flags, external interrupts could be allowed
+	 */
+	popf
+	/*
+	 * execute or skip the instruction just causing the VM exit in root mode
+	 */
+	ret
+SYM_FUNC_END(vmx_depriv_vmexit)
+
 /**
  * __vmx_vcpu_run - Run a vCPU via a transition to VMX guest mode
  * @vmx:	struct vcpu_vmx * (forwarded to vmx_update_host_rsp)
@@ -237,6 +422,20 @@ SYM_FUNC_END(__vmx_vcpu_run)
 
 
 .section .text, "ax"
+SYM_FUNC_START(vmx_depriv)
+	/* Enter non-root mode */
+	vmlaunch
+	/* Jump on VM-Fail. */
+	jmp 1f
+
+/* vmlaunch succeeded */
+SYM_CODE_START(vmx_depriv_guest_rip)
+	xor %eax, %eax
+	ret
+
+1:	mov $1, %eax
+	ret
+SYM_FUNC_END(vmx_depriv)
 
 /**
  * vmread_error_trampoline - Trampoline from inline asm to vmread_error()
diff --git a/arch/x86/kvm/vmx/vmx.c b/arch/x86/kvm/vmx/vmx.c
index 1b2e9d8c5cc9..3a183623b9bd 100644
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@ -38,6 +38,9 @@
 #include <asm/fpu/api.h>
 #include <asm/fpu/xstate.h>
 #include <asm/idtentry.h>
+#if IS_ENABLED(CONFIG_KVM_INTEL_DEPRIV_HOST)
+#include <asm/insn.h>
+#endif
 #include <asm/io.h>
 #include <asm/irq_remapping.h>
 #include <asm/kexec.h>
@@ -5790,7 +5793,7 @@ void dump_vmcs(struct kvm_vcpu *vcpu)
 	unsigned long cr4;
 	int efer_slot;
 
-	if (!dump_invalid_vmcs) {
+	if (false && !dump_invalid_vmcs) {
 		pr_warn_ratelimited("set kvm_intel.dump_invalid_vmcs=1 to dump internal KVM state.\n");
 		return;
 	}
@@ -5917,8 +5920,9 @@ void dump_vmcs(struct kvm_vcpu *vcpu)
 	       vmcs_read32(VM_EXIT_INTR_INFO),
 	       vmcs_read32(VM_EXIT_INTR_ERROR_CODE),
 	       vmcs_read32(VM_EXIT_INSTRUCTION_LEN));
-	pr_err("        reason=%08x qualification=%016lx\n",
-	       vmcs_read32(VM_EXIT_REASON), vmcs_readl(EXIT_QUALIFICATION));
+	pr_err("        reason=%08x qualification=%016lx VM instr error=%08x\n",
+	       vmcs_read32(VM_EXIT_REASON), vmcs_readl(EXIT_QUALIFICATION),
+	       vmcs_read32(VM_INSTRUCTION_ERROR));
 	pr_err("IDTVectoring: info=%08x errcode=%08x\n",
 	       vmcs_read32(IDT_VECTORING_INFO_FIELD),
 	       vmcs_read32(IDT_VECTORING_ERROR_CODE));
@@ -8038,6 +8042,1035 @@ static void vmx_cleanup_l1d_flush(void)
 	l1tf_vmx_mitigation = VMENTER_L1D_FLUSH_AUTO;
 }
 
+#if IS_ENABLED(CONFIG_KVM_INTEL_DEPRIV_HOST)
+static struct vmcs_config depriv_vmcs_conf;
+static DEFINE_PER_CPU(struct vmcs *, depriv_vmcs);
+static DEFINE_PER_CPU(void *, depriv_msr_bitmap);
+static DEFINE_PER_CPU(void *, depriv_stack_base);
+
+static void __init vmx_depriv_cpu_controls(void)
+{
+	u32 eb;
+
+	vmcs_write32(PIN_BASED_VM_EXEC_CONTROL,
+		     depriv_vmcs_conf.pin_based_exec_ctrl);
+	vmcs_write32(CPU_BASED_VM_EXEC_CONTROL,
+		     depriv_vmcs_conf.cpu_based_exec_ctrl);
+
+	if (cpu_has_secondary_exec_ctrls()) {
+		vmcs_write32(SECONDARY_VM_EXEC_CONTROL,
+		     depriv_vmcs_conf.cpu_based_2nd_exec_ctrl);
+	}
+
+	vmcs_write32(VM_EXIT_CONTROLS,
+		     depriv_vmcs_conf.vmexit_ctrl |
+		     VM_EXIT_HOST_ADDR_SPACE_SIZE);
+	vmcs_write32(VM_ENTRY_CONTROLS,
+		     depriv_vmcs_conf.vmentry_ctrl);
+
+	eb = (1u << DB_VECTOR) | (1u << UD_VECTOR) |
+	     (1u << DF_VECTOR) |
+	     (1u << AC_VECTOR) | (1u << MC_VECTOR);
+	vmcs_write32(EXCEPTION_BITMAP, eb);
+	vmcs_write32(PAGE_FAULT_ERROR_CODE_MASK, 0);
+	vmcs_write32(PAGE_FAULT_ERROR_CODE_MATCH, 0);
+	vmcs_write32(CR3_TARGET_COUNT, 0);
+
+	vmcs_write32(VM_EXIT_MSR_STORE_COUNT, 0);
+	vmcs_write32(VM_EXIT_MSR_LOAD_COUNT, 0);
+	vmcs_write64(VM_EXIT_MSR_LOAD_ADDR, 0);
+	vmcs_write32(VM_ENTRY_MSR_LOAD_COUNT, 0);
+	vmcs_write64(VM_ENTRY_MSR_LOAD_ADDR, 0);
+}
+
+static void __init vmx_depriv_cpu_crs(void)
+{
+	unsigned long cr0, cr3, cr4;
+	u64 pat, efer;
+
+	cr0 = read_cr0();
+	vmcs_writel(HOST_CR0, cr0);
+	vmcs_writel(CR0_READ_SHADOW, cr0);
+	vmcs_writel(GUEST_CR0, cr0);
+
+	cr3 = __read_cr3();
+	vmcs_writel(HOST_CR3, cr3);
+	vmcs_writel(GUEST_CR3, cr3);
+
+	cr4 = __read_cr4();
+	vmcs_writel(HOST_CR4, cr4);
+	vmcs_writel(CR4_READ_SHADOW, cr4);
+	vmcs_writel(GUEST_CR4, cr4 | KVM_PMODE_VM_CR4_ALWAYS_ON);
+
+	vmcs_writel(CR0_GUEST_HOST_MASK, ~X86_CR0_TS);
+	vmcs_writel(CR4_GUEST_HOST_MASK, ~KVM_CR4_GUEST_OWNED_BITS);
+
+	pat = read_msr(MSR_IA32_CR_PAT);
+	vmcs_write64(HOST_IA32_PAT, pat);
+	vmcs_write64(GUEST_IA32_PAT, pat);
+
+	efer = read_msr(MSR_EFER);
+	vmcs_write64(HOST_IA32_EFER, efer);
+	vmcs_write64(GUEST_IA32_EFER, efer);
+
+	if (efer & EFER_LMA && efer & EFER_SCE) {
+		pr_debug("STAR=%016lx", read_msr(MSR_STAR));
+		pr_debug("LSTAR=%016lx", read_msr(MSR_LSTAR));
+		pr_debug("CSTAR=%016lx", read_msr(MSR_CSTAR));
+		pr_debug("syscall mask=%016lx", read_msr(MSR_SYSCALL_MASK));
+	}
+
+	if (cpu_has_load_perf_global_ctrl()) {
+		u64 perf_global_ctrl;
+		rdmsrl_safe(MSR_CORE_PERF_GLOBAL_CTRL, &perf_global_ctrl);
+		vmcs_write64(GUEST_IA32_PERF_GLOBAL_CTRL, perf_global_ctrl);
+		vmcs_write64(HOST_IA32_PERF_GLOBAL_CTRL, perf_global_ctrl);
+	}
+
+	if (false) // true: test code path handling vmlaunch caused VM-entry fail
+		vmcs_write32(CR3_TARGET_COUNT, 0x100000);
+}
+
+static inline bool __init is_desc_16byte(struct desc_struct *dentry)
+{
+	// s = 0 : system descriptor
+	return dentry->p && !dentry->s;
+}
+
+static inline u32 __init get_desc_limit_in_byte(struct desc_struct *dentry)
+{
+	u32 limit = get_desc_limit(dentry);
+	if (dentry->g)
+		limit = (limit << PAGE_SHIFT) | (PAGE_SIZE - 1);
+	return limit;
+}
+
+static inline void __init dump_desc_entry(struct desc_struct *dentry)
+{
+	int cpu = smp_processor_id();
+	bool is_16byte = is_desc_16byte(dentry);
+	u16 *entry = (u16 *)dentry;
+	u32 limit = get_desc_limit_in_byte(dentry);
+	unsigned long base = get_desc_base(dentry);
+
+	if (is_16byte) {
+		pr_info("depriv: cpu %d %04x %04x %04x %04x %04x %04x %04x %04x\n",
+			cpu, entry[0], entry[1], entry[2], entry[3],
+			entry[4], entry[5], entry[6], entry[7]);
+		base += (u64)(*((u32 *)(dentry + 1))) << 32;
+	} else {
+		pr_info("depriv: cpu %d %04x %04x %04x %04x\n",
+			cpu, entry[0], entry[1], entry[2], entry[3]);
+	}
+
+	pr_info("depriv: cpu %d type %x, S %x, DPL %x, P %x, AVL %x, "
+		"L %x, D %x, G %x, limit %#x, base %#lx\n",
+		cpu, dentry->type, dentry->s, dentry->dpl, dentry->p,
+		dentry->avl, dentry->l, dentry->d, dentry->g, limit, base);
+}
+
+static inline __init struct desc_struct *get_gdt_entry(unsigned long addr)
+{
+	struct desc_struct *dentry = (struct desc_struct *)addr;
+	if (false)
+		dump_desc_entry(dentry);
+	return dentry;
+}
+
+static inline u32 __init get_desc_ar(struct desc_struct *dentry,
+				     bool is_null, bool is_segment)
+{
+	int cpu = smp_processor_id();
+	u32 unusable = is_null ? 1 : 0; // 0 = usable; 1 = unusable
+	/*
+	 * 26.3.1.2 Checks on Guest Segment Registers, AR bytes:
+	 */
+	bool s = (unusable ? dentry->s :
+			     (is_segment ? 1 : 0));
+	u32 ar = dentry->type |
+		 (s ? 1 : 0) << 4 |
+		 dentry->dpl << 5 |
+		 dentry->p << 7 |
+		 dentry->avl << 12 |
+		 dentry->l << 13 |
+		 dentry->d << 14 |
+		 dentry->g << 15 |
+		 unusable << 16;
+	pr_debug("depriv: cpu %d entry ar %#x\n", cpu, ar);
+	return ar;
+}
+
+#define DEPRIV_SELECTOR(name, sel) {						\
+	pr_debug("depriv: cpu %d " #name " %#x\n", cpu, sel);			\
+	dentry = get_gdt_entry(gdt_base + sel);					\
+	base = get_desc_base(dentry);						\
+	if (is_desc_16byte(dentry))						\
+		base += (u64)(*((u32 *)(dentry + 1))) << 32;			\
+	vmcs_write16(GUEST_##name##_SELECTOR, sel);				\
+	vmcs_writel(GUEST_##name##_BASE, base);					\
+	vmcs_write32(GUEST_##name##_LIMIT, get_desc_limit_in_byte(dentry));	\
+	vmcs_write32(GUEST_##name##_AR_BYTES,					\
+		     get_desc_ar(dentry, sel == 0, is_segment));		\
+}
+
+#define DEPRIV_SEGMENT(SEG, fix_null_selector) {				\
+	u16 seg;								\
+	savesegment(SEG, seg);							\
+	if (fix_null_selector && seg == 0) seg = __KERNEL_DS;			\
+	vmcs_write16(HOST_##SEG##_SELECTOR, seg);				\
+	DEPRIV_SELECTOR(SEG, seg);						\
+}
+
+static void __init vmx_depriv_cpu_segments(unsigned long gdt_base)
+{
+	int cpu = smp_processor_id();
+	struct desc_struct *dentry;
+	unsigned long base;
+	bool is_segment = true;
+
+	DEPRIV_SEGMENT(CS, false);
+	DEPRIV_SEGMENT(DS, false);
+	DEPRIV_SEGMENT(ES, false);
+	DEPRIV_SEGMENT(SS, false);
+	DEPRIV_SEGMENT(FS, false);
+	DEPRIV_SEGMENT(GS, false);
+
+	base = read_msr(MSR_FS_BASE);
+	pr_debug("depriv: cpu %d FS base MSR %#lx\n", cpu, base);
+	vmcs_writel(HOST_FS_BASE, base);
+	vmcs_writel(GUEST_FS_BASE, base);
+
+	base = read_msr(MSR_GS_BASE);
+	pr_debug("depriv: cpu %d GS base MSR %#lx\n", cpu, base);
+	vmcs_writel(HOST_GS_BASE, base);
+	vmcs_writel(GUEST_GS_BASE, base);
+}
+
+static void __init vmx_depriv_cpu_ldtr(unsigned long gdt_base)
+{
+	int cpu = smp_processor_id();
+	struct desc_struct *dentry;
+	unsigned long base;
+	u16 ldtr;
+	bool is_segment = false;
+
+	store_ldt(ldtr);
+	DEPRIV_SELECTOR(LDTR, ldtr);
+}
+
+static void __init vmx_depriv_cpu_tr(unsigned long gdt_base)
+{
+	int cpu = smp_processor_id();
+	struct desc_struct *dentry;
+	unsigned long base, tss_base;
+	u16 tr;
+	u32 ar;
+	bool is_segment = false;
+
+	store_tr(tr);
+	if (tr != GDT_ENTRY_TSS*8)
+		pr_err("depriv: cpu %d tr selector mismatch %#x : %#x\n",
+		       cpu, tr, GDT_ENTRY_TSS*8);
+	vmcs_write16(HOST_TR_SELECTOR, tr);
+	DEPRIV_SELECTOR(TR, tr);
+	vmcs_writel(HOST_TR_BASE, base);
+	tss_base = (unsigned long)&get_cpu_entry_area(cpu)->tss.x86_tss;
+	if (base != tss_base)
+		pr_err("depriv: cpu %d tr base mismatch %#lx : %#lx\n",
+		       cpu, base, tss_base);
+
+	ar = vmcs_read32(GUEST_TR_AR_BYTES);
+	if ((ar & VMX_AR_TYPE_MASK) != VMX_AR_TYPE_BUSY_64_TSS) {
+		pr_err("%s: tss fixup for long mode\n", __func__);
+		vmcs_write32(GUEST_TR_AR_BYTES,
+			     (ar & ~VMX_AR_TYPE_MASK) |
+			     VMX_AR_TYPE_BUSY_64_TSS);
+	}
+}
+
+static void __init vmx_depriv_cpu_desc_tables(void)
+{
+	int cpu = smp_processor_id();
+	struct desc_ptr gdt, idt;
+	unsigned long gdt_base;
+
+	store_gdt(&gdt);
+	gdt_base = gdt.address;
+	if (gdt_base != (unsigned long)get_current_gdt_ro())
+		pr_err("depriv: cpu %d gdt base mismatch %#lx : %#lx\n",
+		       cpu, gdt_base, (unsigned long)get_current_gdt_ro());
+	vmcs_writel(HOST_GDTR_BASE, gdt_base);
+	vmcs_writel(GUEST_GDTR_BASE, gdt_base);
+	/* there is no host gdt limit */
+	vmcs_write32(GUEST_GDTR_LIMIT, gdt.size);
+
+	store_idt(&idt);
+	/* host should never handle interrupts */
+	vmcs_writel(HOST_IDTR_BASE, idt.address);
+	vmcs_writel(GUEST_IDTR_BASE, idt.address);
+	/* there is no host idt limit */
+	vmcs_write32(GUEST_IDTR_LIMIT, idt.size);
+
+	vmx_depriv_cpu_segments(gdt_base);
+	vmx_depriv_cpu_ldtr(gdt_base);
+	vmx_depriv_cpu_tr(gdt_base);
+}
+
+static void __init vmx_depriv_cpu_sysenter_msrs(void)
+{
+	u32 low32, high32;
+	unsigned long msr;
+
+	msr = read_msr(MSR_IA32_SYSENTER_ESP);
+	vmcs_writel(HOST_IA32_SYSENTER_ESP, msr);
+	vmcs_writel(GUEST_SYSENTER_ESP, msr);
+
+	rdmsr(MSR_IA32_SYSENTER_CS, low32, high32);
+	vmcs_write32(HOST_IA32_SYSENTER_CS, low32);
+	vmcs_write32(GUEST_SYSENTER_CS, low32);
+
+	msr = read_msr(MSR_IA32_SYSENTER_EIP);
+	vmcs_writel(HOST_IA32_SYSENTER_EIP, msr);
+	vmcs_writel(GUEST_SYSENTER_EIP, msr);
+}
+
+static void __init vmx_depriv_cpu_misc(void)
+{
+	unsigned long dr7;
+	u64 dbg_ctrl;
+
+	get_debugreg(dr7, 7);
+	vmcs_writel(GUEST_DR7, dr7);
+
+	dbg_ctrl = read_msr(MSR_IA32_DEBUGCTLMSR);
+	vmcs_write64(GUEST_IA32_DEBUGCTL, dbg_ctrl);
+}
+
+static void vmx_repriv_cpu_release_resources(void)
+{
+	int cpu = smp_processor_id();
+	void *host_stack_base = per_cpu(depriv_stack_base, cpu);
+	struct vmcs *vmcs = per_cpu(depriv_vmcs, cpu);
+	unsigned long *msr_bitmap = per_cpu(depriv_msr_bitmap, cpu);
+	unsigned long rsp;
+
+	asm volatile("mov %%rsp,%0" : "=m"(rsp));
+	pr_info("depriv: reprivilege cpu %d rsp %#lx\n", cpu, rsp);
+
+	if (host_stack_base) {
+		per_cpu(depriv_stack_base, cpu) = NULL;
+		free_pages((unsigned long)host_stack_base, 0);
+	}
+
+	if (vmcs) {
+		per_cpu(depriv_vmcs, cpu) = NULL;
+		vmcs_clear(vmcs);
+		free_vmcs(vmcs);
+	}
+
+	if (msr_bitmap) {
+		per_cpu(depriv_msr_bitmap, cpu) = NULL;
+		free_pages((unsigned long)msr_bitmap, 0);
+	}
+}
+
+void vmx_depriv_vmexit(void);
+int vmx_depriv(void);
+void vmx_depriv_guest_rip(void);
+void vmx_depriv_test_vmcall(void);
+
+/*
+ * needed to iret to root mode kernel or user space when the VM exit happened
+ */
+#define HOST_STACK_RESERVED_BYTES (16 * 8)
+
+static void __init vmx_depriv_cpu(void *info)
+{
+	int cpu = smp_processor_id();
+	int node = cpu_to_node(cpu);
+	struct vmcs *vmcs = NULL;
+	struct page *page = NULL;
+	void *msr_bitmap = NULL;
+	void *host_stack_base = NULL;
+	unsigned long host_rsp, guest_rsp, guest_rflags;
+	int vmx_depriv_result;
+
+	if (!(depriv_vmcs_conf.cpu_based_exec_ctrl & CPU_BASED_USE_MSR_BITMAPS)) {
+		pr_err("depriv: MSR bitmap not available on cpu %d\n", cpu);
+		goto error;
+	}
+
+	page = __alloc_pages_node(node, GFP_KERNEL, 0);
+	if (!page) {
+		pr_err("depriv: unable to allocate MSR bitmap for cpu %d\n", cpu);
+		goto error;
+	}
+
+	msr_bitmap  = page_address(page);
+	// no VM-exits on MSR accesses
+	memset(msr_bitmap, 0, PAGE_SIZE);
+	per_cpu(depriv_msr_bitmap, cpu) = msr_bitmap;
+
+	vmcs = alloc_vmcs_cpu(false, cpu, GFP_KERNEL);
+	if (!vmcs) {
+		pr_err("depriv: unable to allocate VMCS for cpu %d\n", cpu);
+		goto error;
+	}
+	vmcs_clear(vmcs);
+	vmcs_load(vmcs);
+	vmcs_write64(VMCS_LINK_POINTER, ~0ull);
+	vmcs_write64(MSR_BITMAP, __pa(msr_bitmap));
+	indirect_branch_prediction_barrier();
+	per_cpu(depriv_vmcs, cpu) = vmcs;
+
+	// one stack page for root mode VM exit handler
+	page = __alloc_pages_node(node, GFP_KERNEL, 0);
+	if (!page) {
+		pr_err("depriv: unable to allocate stack for cpu %d\n", cpu);
+		goto error;
+	}
+
+	host_stack_base = page_address(page);
+	memset(host_stack_base, 0, PAGE_SIZE);
+	per_cpu(depriv_stack_base, cpu) = host_stack_base;
+
+	vmx_depriv_cpu_controls();
+	vmx_depriv_cpu_crs();
+	vmx_depriv_cpu_desc_tables();
+	vmx_depriv_cpu_sysenter_msrs();
+	vmx_depriv_cpu_misc();
+
+	vmcs_writel(HOST_RIP, (unsigned long)vmx_depriv_vmexit);
+	// reserve extra HOST_STACK_RESERVED_BYTES bytes for reprivileging host
+	host_rsp = (unsigned long)host_stack_base + PAGE_SIZE -
+		   HOST_STACK_RESERVED_BYTES;
+	vmcs_writel(HOST_RSP, host_rsp);
+
+	/* switching to non-root mode */
+	vmcs_writel(GUEST_RIP, (unsigned long)vmx_depriv_guest_rip);
+	asm volatile("mov %%rsp,%0" : "=m"(guest_rsp));
+	// reserve extra 8 bytes for RIP pushed to stack when calling vmx_depriv
+	guest_rsp -= 8;
+	vmcs_writel(GUEST_RSP, guest_rsp);
+	*(unsigned long *)host_rsp = guest_rsp;
+
+	asm volatile("xor %%rax,%%rax\n\t"
+		     "pushf\n\t"
+		     "pop %%rax\n\t"
+		     "mov %%rax,%0"
+		     : "=m"(guest_rflags) :: "%rax");
+	vmcs_writel(GUEST_RFLAGS, guest_rflags & ~X86_EFLAGS_IF);
+
+	pr_info("depriv: deprivileging cpu %d: guest rip=%#lx guest rsp=%#lx\n",
+		cpu, vmcs_readl(GUEST_RIP), guest_rsp);
+
+	if (false) // true: test code path handling vmresume caused VM-entry fail
+		vmcs_write32(GUEST_TR_AR_BYTES, 0x009b);
+
+	/*
+	 * Should we save/restore general purpose registers around vmx_depriv?
+	 * Yes, but only restore them when there was a successful vmentry.
+	 */
+	vmx_depriv_result = vmx_depriv();
+	if (!vmx_depriv_result) {
+		// continue in non-root mode...
+		asm volatile("mov %%rsp,%0" : "=m"(guest_rsp));
+		asm volatile("xor %%rax,%%rax\n\t"
+			     "pushf\n\t"
+			     "pop %%rax\n\t"
+			     "mov %%rax,%0"
+			     : "=m"(guest_rflags) :: "%rax");
+		pr_info("depriv: cpu %d deprivileged: rsp=%#lx  rflags=%#lx\n",
+			cpu, guest_rsp, guest_rflags);
+
+		vmx_depriv_test_vmcall();
+		return;
+	}
+
+	// still in root mode
+	if (vmx_depriv_result == 1)
+		pr_err("depriv: launch failed on cpu %d\n", cpu);
+	else if (vmx_depriv_result == 2)
+		pr_err("depriv: resume failed on cpu %d\n", cpu);
+
+error:
+	vmx_repriv_cpu_release_resources();
+}
+
+#ifdef CONFIG_PAGE_TABLE_ISOLATION
+/*
+ * the following macros are from arch/x86/entry/calling.h
+ */
+#define PTI_USER_PGTABLE_BIT		PAGE_SHIFT
+#define PTI_USER_PGTABLE_MASK		(1 << PTI_USER_PGTABLE_BIT)
+#define PTI_USER_PCID_BIT		X86_CR3_PTI_PCID_USER_BIT
+#define PTI_USER_PCID_MASK		(1 << PTI_USER_PCID_BIT)
+#define PTI_USER_PGTABLE_AND_PCID_MASK  (PTI_USER_PCID_MASK | PTI_USER_PGTABLE_MASK)
+#else
+#define PTI_USER_PGTABLE_AND_PCID_MASK  (0)
+#endif
+
+static void vmx_repriv_cpu_crs(void)
+{
+	int cpu = smp_processor_id();
+	unsigned long host_cr0 = read_cr0();
+	unsigned long host_cr3 = __read_cr3();
+	unsigned long host_cr4 = __read_cr4();
+	unsigned long guest_cr0 = vmcs_readl(GUEST_CR0);
+	unsigned long guest_cr3 = vmcs_readl(GUEST_CR3);
+	unsigned long guest_cr4 = vmcs_readl(GUEST_CR4);
+
+	if (host_cr0 != guest_cr0) {
+		pr_info("depriv: repriv cpu %d cr0 %#lx : %#lx : %#lx\n",
+			cpu, host_cr0, vmcs_readl(HOST_CR0), guest_cr0);
+		write_cr0(guest_cr0);
+		//vmcs_writel(HOST_CR0, guest_cr0);
+	}
+
+	if (host_cr3 != guest_cr3) {
+		pr_info("depriv: repriv cpu %d cr3 %#lx : %#lx\n",
+			cpu, host_cr3, guest_cr3);
+
+		host_cr3 = boot_cpu_has(X86_FEATURE_PTI) ?
+			guest_cr3 & ~PTI_USER_PGTABLE_AND_PCID_MASK : guest_cr3;
+		pr_info("depriv: repriv cpu %d cr3 = %#lx\n", cpu, host_cr3);
+		vmcs_writel(HOST_CR3, host_cr3);
+		//write_cr3(host_cr3);
+	}
+
+	if (host_cr4 != guest_cr4) {
+		pr_info("depriv: repriv cpu %d cr4 %#lx : %#lx : %#lx\n",
+			cpu, host_cr4, vmcs_readl(HOST_CR4), guest_cr4);
+		//__write_cr4(guest_cr4);
+		//vmcs_writel(HOST_CR4, guest_cr4);
+	}
+}
+
+static inline void vmx_repriv_cpu_sysenter_msrs(void)
+{
+	wrmsrl(MSR_IA32_SYSENTER_ESP, vmcs_readl(GUEST_SYSENTER_ESP));
+	wrmsr(MSR_IA32_SYSENTER_CS, vmcs_read32(GUEST_SYSENTER_CS), 0);
+	wrmsrl(MSR_IA32_SYSENTER_EIP, vmcs_readl(GUEST_SYSENTER_EIP));
+}
+
+static inline void vmx_repriv_cpu_misc(void)
+{
+	set_debugreg(vmcs_readl(GUEST_DR7), 7);
+	wrmsrl(MSR_IA32_DEBUGCTLMSR, vmcs_read64(GUEST_IA32_DEBUGCTL));
+}
+
+static void dump_fs(void)
+{
+	int cpu = smp_processor_id();
+
+	pr_info("depriv: cpu %d host FS base=%#lx\n",
+		cpu, read_msr(MSR_FS_BASE));
+	pr_info("depriv: cpu %d guest FS sel=0x%04x, attr=0x%05x, "
+		"limit=0x%08x, base=0x%016lx\n",
+		cpu, vmcs_read16(GUEST_FS_SELECTOR),
+		vmcs_read32(GUEST_FS_AR_BYTES),
+		vmcs_read32(GUEST_FS_LIMIT),
+		vmcs_readl(GUEST_FS_BASE));
+}
+
+#define depriv_loadsegment	loadsegment
+
+#define REPRIV_SEGMENT(tag, TAG) do {						\
+	u32 ar = vmcs_read32(GUEST_##TAG##S_AR_BYTES);				\
+	if ((ar >> 16) & 0x1) {							\
+		pr_info("depriv: cpu %d " #TAG "S unusable\n", cpu);		\
+		break;								\
+	}									\
+	sel = vmcs_read16(GUEST_##TAG##S_SELECTOR);				\
+	depriv_loadsegment(tag##s, sel);					\
+	pr_debug("depriv: cpu %d " #TAG "S %#x\n", cpu, sel);			\
+} while (0)
+
+#define DEPRIV_FSGS_BASE(TAG) do {						\
+	host_base = read_msr(MSR_##TAG##S_BASE);				\
+	guest_base = vmcs_readl(GUEST_##TAG##S_BASE);				\
+	pr_debug("depriv: cpu %d " #TAG "S base %#lx : %#lx\n",			\
+		 cpu, host_base, guest_base);					\
+	if (host_base != guest_base) {						\
+		pr_info("depriv: cpu %d " #TAG "S base mismatch %#lx : %#lx\n",	\
+			cpu, host_base, guest_base);				\
+		wrmsrl(MSR_##TAG##S_BASE, guest_base);				\
+	}									\
+} while (0)
+
+static inline void vmx_repriv_cpu_segments(void)
+{
+	int cpu = smp_processor_id();
+	u16 sel;
+	unsigned long host_base, guest_base;
+
+	dump_fs();
+
+	REPRIV_SEGMENT(d, D);
+	REPRIV_SEGMENT(e, E);
+	REPRIV_SEGMENT(f, F);
+
+#undef depriv_loadsegment
+#define depriv_loadsegment(gs, sel)	load_gs_index(sel)
+	REPRIV_SEGMENT(g, G);
+#undef depriv_loadsegment
+
+	DEPRIV_FSGS_BASE(F);
+	DEPRIV_FSGS_BASE(G);
+
+	/*
+	 * XXX: FS handling is complicated
+	 *
+	 * make sure you understand load_seg_legacy() in arch/x86/kernel/process_64.c
+	 * before making any change here.
+	 */
+	dump_fs();
+}
+
+static inline void vmx_repriv_cpu_ldtr(void)
+{
+	int cpu = smp_processor_id();
+	u16 guest_ldtr = vmcs_read16(GUEST_LDTR_SELECTOR), host_ldtr;
+
+	store_ldt(host_ldtr);
+	if (host_ldtr != guest_ldtr) {
+		pr_info("depriv: cpu %d LDTR mismatch %#x : %#x\n",
+			cpu, host_ldtr, guest_ldtr);
+		load_ldt(guest_ldtr);
+	}
+}
+
+static inline void vmx_repriv_cpu_tr(void)
+{
+	int cpu = smp_processor_id();
+	u16 guest_tr = vmcs_read16(GUEST_TR_SELECTOR), host_tr;
+
+	store_tr(host_tr);
+	if (host_tr != guest_tr) {
+		pr_info("depriv: cpu %d TR mismatch %#x : %#x\n",
+			cpu, host_tr, guest_tr);
+		if (guest_tr == 0)
+			return;
+		load_tr(guest_tr);
+		//vmcs_write16(HOST_TR_SELECTOR, guest_tr);
+	}
+}
+
+#define REPRIV_DESC_TABLE(tag, TAG) do {					\
+	store_##tag##dt(&host_dt);						\
+	guest_dt_base = vmcs_readl(GUEST_##TAG##DTR_BASE);			\
+	if (host_dt.address != guest_dt_base)					\
+		pr_err("depriv: cpu %d " #tag "dt base mismatch %#lx : %#lx\n",	\
+		       cpu, host_dt.address, guest_dt_base);			\
+	/*vmcs_writel(HOST_##TAG##DTR_BASE, guest_dt_base);*/			\
+	guest_dt_limit = vmcs_read32(GUEST_##TAG##DTR_LIMIT);			\
+	if (host_dt.size != guest_dt_limit) {					\
+		pr_debug("depriv: cpu %d " #tag "dt limit mismatch %#x : %#x\n",\
+			 cpu, host_dt.size , guest_dt_limit);			\
+		host_dt.size = guest_dt_limit;					\
+		load_##tag##dt(&host_dt);					\
+	}									\
+} while (0)
+
+static inline void vmx_repriv_cpu_desc_tables(void)
+{
+	int cpu = smp_processor_id();
+	struct desc_ptr host_dt;
+	unsigned long guest_dt_base;
+	u32 guest_dt_limit;
+
+	REPRIV_DESC_TABLE(g, G);
+	REPRIV_DESC_TABLE(i, I);
+
+	vmx_repriv_cpu_segments();
+	vmx_repriv_cpu_ldtr();
+	vmx_repriv_cpu_tr();
+}
+
+static void vmx_repriv_cpu(void *info)
+{
+	/* trigger a vmcall vmexit to reprivilege */
+#if 0
+	asm volatile("push %%rbp; vmcall; pop %%rbp" : : :
+		     "rax", "rbx", "rcx", "rdx",
+		     "rsi", "rdi", "r8", "r9", "r10", "r11", "r12",
+		     "r13", "r14", "r15");
+#endif
+
+	vmx_repriv_cpu_release_resources();
+}
+
+static void vmx_repriv_host(void)
+{
+	on_each_cpu(vmx_repriv_cpu, NULL, 0);
+	pr_info("depriv: reprivileged host\n");
+}
+
+/*
+ * sync guest state to host w/o changing guest state
+ */
+static void vmx_repriv_cpu_state(void)
+{
+	vmx_repriv_cpu_crs();
+	vmx_repriv_cpu_misc();
+	vmx_repriv_cpu_sysenter_msrs();
+	vmx_repriv_cpu_desc_tables();
+}
+
+#define CONTINUE_IN_NON_ROOT_MODE do {						\
+	exit_insn_len = vmcs_read32(VM_EXIT_INSTRUCTION_LEN);			\
+	vmcs_writel(GUEST_RIP, guest_rip + exit_insn_len);			\
+	return true;								\
+} while (0)
+
+/*
+ * push current guest RAX, RFLAGS and RIP to guest stask top
+ * XXX: look it's okay to trigger stack page fault in the macro?
+ */
+#define CONTINUE_IN_ROOT_MODE__OLD(_rip_offset) do {				\
+	guest_rsp -= 24;							\
+	*(unsigned long *)(guest_rsp + 0) = guest_regs[__VCPU_REGS_RAX];	\
+	*(unsigned long *)(guest_rsp + 8) = vmcs_readl(GUEST_RFLAGS);		\
+	*(unsigned long *)(guest_rsp + 16) = guest_rip + (_rip_offset);		\
+	*(unsigned long *)host_rsp = guest_rsp;					\
+	vmx_repriv_cpu_state();							\
+	dump_vmcs();								\
+	pr_info("depriv: cpu %d exit reason %d skip %d bytes instruction "	\
+		"and continue in root mode\n",					\
+		cpu, exit_reason, _rip_offset);					\
+	return false;								\
+} while (0)
+
+#define CONTINUE_IN_ROOT_MODE(insn_len) do {					\
+	*(unsigned long *)(host_rsp + 0x0) = vmcs_readl(GUEST_RIP) + (insn_len);\
+	*(unsigned long *)(host_rsp + 0x8) = vmcs_read16(GUEST_CS_SELECTOR);	\
+	*(unsigned long *)(host_rsp + 0x10) = vmcs_readl(GUEST_RFLAGS);		\
+	*(unsigned long *)(host_rsp + 0x18) = vmcs_readl(GUEST_RSP);		\
+	*(unsigned long *)(host_rsp + 0x20) = vmcs_read16(GUEST_SS_SELECTOR);	\
+	vmx_repriv_cpu_state();							\
+	dump_vmcs();								\
+	*(unsigned long *)(host_rsp + 0x28) = vmcs_readl(HOST_CR3);		\
+	pr_info("depriv: cpu %d exit reason %d skip %d bytes instruction "	\
+		"and continue in root mode\n",					\
+		cpu, exit_reason, insn_len);					\
+	return false;								\
+} while (0)
+
+void vmx_depriv_special_insn(void);
+void vmx_depriv_execute_special_insn_in_root_mode(unsigned long *guest_regs);
+
+static void dump_guest_insn(unsigned long guest_rip, char *insn)
+{
+	int cpu = smp_processor_id(), i;
+
+	for (i = 0; i < vmcs_read32(VM_EXIT_INSTRUCTION_LEN); i++)
+		sprintf(insn + 3 * i, " %02x", *(u8 *)(guest_rip + i));
+	insn[3 * i] = '\0';
+
+	pr_info("depriv: cpu %d guest rip %#lx: %s\n", cpu, guest_rip, insn);
+}
+
+bool vmx_depriv_vmexit_handler(unsigned long *guest_regs)
+{
+	int cpu = smp_processor_id(), i;
+	u32 exit_reason = vmcs_read32(VM_EXIT_REASON);
+	unsigned long host_rsp = vmcs_readl(HOST_RSP);
+	unsigned long guest_rip = vmcs_readl(GUEST_RIP);
+	unsigned long guest_rsp = vmcs_readl(GUEST_RSP);
+	char insn[64];
+	u32 exit_insn_len;
+	static int exit_cnt = 0;
+
+	pr_info("depriv: cpu %d exit reason=%#x count=%d\n", cpu, exit_reason, ++exit_cnt);
+
+	pr_debug("depriv: cpu %d rip=%#lx rsp=%#lx\n", cpu, guest_rip, guest_rsp);
+	pr_debug("depriv: cpu %d host rsp=%#lx\n", cpu, host_rsp);
+
+	pr_debug("depriv: guest GPRs:\n");
+	for (i = __VCPU_REGS_RAX; i <= __VCPU_REGS_R15; i++) {
+		pr_debug("\t\t%#lx\n", guest_regs[i]);
+	}
+
+	if (exit_reason & VMX_EXIT_REASONS_FAILED_VMENTRY) {
+		dump_vmcs();
+		vmx_check_guest_state();
+		// no need to sync guest state to host if we never enter guest
+		CONTINUE_IN_ROOT_MODE(0);
+	}
+
+	switch (exit_reason) {
+	case EXIT_REASON_EXCEPTION_NMI: {
+		/*
+		 * u32 idt_vectoring_info = vmcs_read32(IDT_VECTORING_INFO_FIELD);
+		 * pr_info("depriv: cpu %d vectoring info=%#x\n", idt_vectoring_info);
+		 */
+		u32 exit_intr_info = vmcs_read32(VM_EXIT_INTR_INFO);
+
+		if (is_invalid_opcode(exit_intr_info)) {
+			dump_guest_insn(guest_rip, insn);
+
+			if (!memcmp((void *)guest_rip, vmx_depriv_special_insn, exit_insn_len)) {
+				pr_info("depriv: cpu %d executing:%s\n", cpu, insn);
+				vmx_repriv_cpu_state();
+				vmx_depriv_execute_special_insn_in_root_mode(guest_regs);
+				CONTINUE_IN_NON_ROOT_MODE;
+			}
+			pr_info("depriv: cpu %d UD: %s\n", cpu, insn);
+		} else if (is_machine_check(exit_intr_info))
+			pr_info("depriv: cpu %d to handle machine check in root mode\n", cpu);
+		else if (is_machine_check(exit_intr_info) || is_nmi(exit_intr_info))
+			pr_info("depriv: cpu %d to handle NMI in root mode\n", cpu);
+		else
+			pr_info("depriv: cpu %d intr info=%#x\n", cpu, exit_intr_info);
+
+		CONTINUE_IN_ROOT_MODE(0);
+		break;
+	}
+
+	case EXIT_REASON_CR_ACCESS: {
+		unsigned long exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
+		int cr = exit_qualification & 0xf;
+		int reg = (exit_qualification >> 8) & 0xf;
+
+		dump_guest_insn(guest_rip, insn);
+
+		if (cr != 3)
+			CONTINUE_IN_ROOT_MODE(0);
+
+		switch ((exit_qualification >> 4) & 3) {
+		case 0: { /* mov to cr */
+			unsigned long cr3 = guest_regs[reg];
+
+			cr3 &= ~X86_CR3_PCID_NOFLUSH;
+			vmcs_writel(GUEST_CR3, cr3);
+
+			if (boot_cpu_has(X86_FEATURE_PTI))
+				cr3 &= ~PTI_USER_PGTABLE_AND_PCID_MASK;
+
+			//vmcs_writel(HOST_CR3, cr3);
+			pr_info("depriv: depriv cpu %d write cr3 %#lx : %#lx (%#lx)\n",
+				cpu, vmcs_readl(HOST_CR3), cr3, guest_regs[reg]);
+			//write_cr3(cr3);
+			break;
+		}
+		case 1: /*mov from cr*/
+			guest_regs[reg] = vmcs_readl(GUEST_CR3);
+			pr_info("depriv: depriv cpu %d read cr3 %#lx : %#lx\n",
+				cpu, vmcs_readl(HOST_CR3), guest_regs[reg]);
+			break;
+		}
+
+		if (true && exit_cnt < 200) {
+			pr_info("depriv: cpu %d accessed cr3 and continue in non-root mode\n", cpu);
+			CONTINUE_IN_NON_ROOT_MODE;
+		} else {
+			pr_info("depriv: cpu %d accessed cr3 and continue in root mode\n", cpu);
+			exit_insn_len = vmcs_read32(VM_EXIT_INSTRUCTION_LEN);
+			CONTINUE_IN_ROOT_MODE(exit_insn_len);
+		}
+	}
+
+	case EXIT_REASON_CPUID: {
+		pr_info("depriv: cpu %d cpuid[%#x]\n", cpu, (u32)guest_regs[__VCPU_REGS_RAX]);
+		native_cpuid((unsigned int *)&guest_regs[__VCPU_REGS_RAX],
+			     (unsigned int *)&guest_regs[__VCPU_REGS_RBX],
+			     (unsigned int *)&guest_regs[__VCPU_REGS_RCX],
+			     (unsigned int *)&guest_regs[__VCPU_REGS_RDX]);
+
+		if (exit_cnt < 300) {
+			pr_debug("depriv: cpu %d executed cpuid and continue in non-root mode\n", cpu);
+			CONTINUE_IN_NON_ROOT_MODE;
+		} else {
+			pr_info("depriv: cpu %d executed cpuid and continue in root mode\n", cpu);
+			exit_insn_len = vmcs_read32(VM_EXIT_INSTRUCTION_LEN);
+			CONTINUE_IN_ROOT_MODE(exit_insn_len);
+		}
+	}
+
+	case EXIT_REASON_VMCALL:
+		if (true) {
+			pr_info("depriv: cpu %d vmcall: continue in non-root mode\n", cpu);
+			CONTINUE_IN_NON_ROOT_MODE;
+		}
+
+		exit_insn_len = vmcs_read32(VM_EXIT_INSTRUCTION_LEN);
+		CONTINUE_IN_ROOT_MODE(exit_insn_len);
+		break;
+
+	case EXIT_REASON_MSR_READ: {
+		u32 ecx = (u32)guest_regs[__VCPU_REGS_RCX];
+		unsigned long long val;
+
+		if (rdmsrl_safe(ecx, &val)) {
+			pr_info("depriv: cpu %d msr[%#x]: %#llx failed\n", cpu, ecx, val);
+			guest_regs[__VCPU_REGS_RAX] = (u32)val;
+			guest_regs[__VCPU_REGS_RDX] = (u32)(val >> 32);
+			CONTINUE_IN_NON_ROOT_MODE;
+		}
+
+		pr_info("depriv: cpu %d msr[%#x]: %#llx\n", cpu, ecx, val);
+
+		guest_regs[__VCPU_REGS_RAX] = (u32)val;
+		guest_regs[__VCPU_REGS_RDX] = (u32)(val >> 32);
+
+		pr_info("depriv: cpu %d executed rdmsr and continue in non-root mode\n", cpu);
+		CONTINUE_IN_NON_ROOT_MODE;
+	}
+
+	case EXIT_REASON_MSR_WRITE: {
+		u32 ecx = (u32)guest_regs[__VCPU_REGS_RCX];
+		unsigned long val = (unsigned long)(u32)guest_regs[__VCPU_REGS_RAX] |
+				    ((unsigned long)(u32)guest_regs[__VCPU_REGS_RDX] << 32);
+
+		if (wrmsrl_safe(ecx, val)) {
+			pr_info("depriv: cpu %d msr[%#x] = %#lx failed\n", cpu, ecx, val);
+			CONTINUE_IN_ROOT_MODE(0);
+		}
+
+		switch (ecx) {
+		case MSR_IA32_SPEC_CTRL:
+			pr_debug("depriv: cpu %d speculation control msr = %#lx\n", cpu, val);
+			break;
+		case MSR_FS_BASE:
+			pr_debug("depriv: cpu %d FS base = %#lx\n", cpu, val);
+			vmcs_writel(GUEST_FS_BASE, val);
+			vmcs_writel(HOST_FS_BASE, val);
+			break;
+		case MSR_GS_BASE:
+			pr_info("depriv: cpu %d GS base = %#lx\n", cpu, val);
+			vmcs_writel(GUEST_GS_BASE, val);
+			vmcs_writel(HOST_GS_BASE, val);
+			break;
+		case MSR_KERNEL_GS_BASE:
+			pr_info("depriv: cpu %d kernel GS base = %#lx\n", cpu, val);
+			break;
+		case MSR_IA32_TSCDEADLINE:
+			pr_debug("depriv: cpu %d TSC deadline = %#lx\n", cpu, val);
+			break;
+		case 0x80b: // EOI virtualization msr
+			pr_debug("depriv: cpu %d EOI msr = %#lx\n", cpu, val);
+			break;
+		default:
+			pr_info("depriv: cpu %d msr[%#x] = %#lx\n", cpu, ecx, val);
+			break;
+		}
+
+		pr_debug("depriv: cpu %d executed wrmsr and continue in non-root mode\n", cpu);
+		CONTINUE_IN_NON_ROOT_MODE;
+	}
+
+	default: /* continue in root mode */
+		pr_info("depriv: cpu %d exit reason=%#x\n", cpu, exit_reason);
+		CONTINUE_IN_ROOT_MODE(0);
+	}
+}
+
+static int __init setup_depriv_vmcs_config(void)
+{
+	u32 min, opt, min2, opt2;
+	u32 _pin_based_exec_control = 0;
+	u32 _cpu_based_exec_control = 0;
+	u32 _cpu_based_2nd_exec_control = 0;
+	u32 _vmexit_control = 0;
+	u32 _vmentry_control = 0;
+
+	memset(&depriv_vmcs_conf, 0, sizeof(depriv_vmcs_conf));
+	min = 0;
+	opt = CPU_BASED_USE_MSR_BITMAPS |
+	      CPU_BASED_ACTIVATE_SECONDARY_CONTROLS;
+	if (adjust_vmx_controls(min, opt, MSR_IA32_VMX_PROCBASED_CTLS,
+				&_cpu_based_exec_control) < 0)
+		return -EIO;
+
+	if (_cpu_based_exec_control & CPU_BASED_ACTIVATE_SECONDARY_CONTROLS) {
+		min2 = 0;
+		opt2 = SECONDARY_EXEC_RDTSCP |
+		       SECONDARY_EXEC_ENABLE_INVPCID;
+		if (adjust_vmx_controls(min2, opt2,
+					MSR_IA32_VMX_PROCBASED_CTLS2,
+					&_cpu_based_2nd_exec_control) < 0)
+			return -EIO;
+	}
+
+	if (_cpu_based_exec_control & CPU_BASED_CR3_LOAD_EXITING) {
+		// if EPT is available, it's OK to disable this control
+		_cpu_based_exec_control &= ~CPU_BASED_CR3_LOAD_EXITING;
+		pr_info("depriv: disabled cr3 load exiting\n");
+		//pr_info("depriv: keep cr3 load exiting\n");
+	}
+
+	if (_cpu_based_exec_control & CPU_BASED_CR3_STORE_EXITING) {
+		// if EPT is available, it's OK to disable this control
+		//_cpu_based_exec_control &= ~CPU_BASED_CR3_STORE_EXITING;
+		//pr_info("depriv: disabled cr3 restore exiting\n");
+		pr_info("depriv: keep cr3 restore exiting\n");
+	}
+
+	if (_cpu_based_exec_control & CPU_BASED_INVLPG_EXITING) {
+		pr_info("depriv: invlpg causes VM exits\n");
+	}
+
+	min = //VM_EXIT_SAVE_DEBUG_CONTROLS |
+	      VM_EXIT_HOST_ADDR_SPACE_SIZE;
+	opt = 0;//VM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL;
+	      //VM_EXIT_LOAD_IA32_PAT |
+	      //VM_EXIT_LOAD_IA32_EFER;
+	if (adjust_vmx_controls(min, opt, MSR_IA32_VMX_EXIT_CTLS,
+				&_vmexit_control) < 0)
+		return -EIO;
+
+	min = 0;
+	opt = 0;
+	if (adjust_vmx_controls(min, opt, MSR_IA32_VMX_PINBASED_CTLS,
+				&_pin_based_exec_control) < 0)
+		return -EIO;
+
+	if (cpu_has_broken_vmx_preemption_timer())
+		_pin_based_exec_control &= ~PIN_BASED_VMX_PREEMPTION_TIMER;
+	if (!(_cpu_based_2nd_exec_control &
+		SECONDARY_EXEC_VIRTUAL_INTR_DELIVERY))
+		_pin_based_exec_control &= ~PIN_BASED_POSTED_INTR;
+
+	min = //VM_ENTRY_LOAD_DEBUG_CONTROLS |
+	      VM_ENTRY_IA32E_MODE;
+	opt = 0;//VM_ENTRY_LOAD_IA32_PERF_GLOBAL_CTRL;
+	      //VM_ENTRY_LOAD_IA32_PAT |
+	      //VM_ENTRY_LOAD_IA32_EFER;
+	if (adjust_vmx_controls(min, opt, MSR_IA32_VMX_ENTRY_CTLS,
+				&_vmentry_control) < 0)
+		return -EIO;
+
+	depriv_vmcs_conf.pin_based_exec_ctrl = _pin_based_exec_control;
+	depriv_vmcs_conf.cpu_based_exec_ctrl = _cpu_based_exec_control;
+	depriv_vmcs_conf.cpu_based_2nd_exec_ctrl = _cpu_based_2nd_exec_control;
+	depriv_vmcs_conf.vmexit_ctrl         = _vmexit_control;
+	depriv_vmcs_conf.vmentry_ctrl        = _vmentry_control;
+
+	pr_info("depriv: pin based controls: %#x\n",
+		depriv_vmcs_conf.pin_based_exec_ctrl);
+	pr_info("depriv: processor based controls: %#x\n",
+		depriv_vmcs_conf.cpu_based_exec_ctrl);
+	pr_info("depriv: processor based 2nd controls: %#x\n",
+		depriv_vmcs_conf.cpu_based_2nd_exec_ctrl);
+	pr_info("depriv: vm exit controls: %#x\n",
+		depriv_vmcs_conf.vmexit_ctrl);
+	pr_info("depriv: vm entry controls: %#x\n",
+		depriv_vmcs_conf.vmentry_ctrl);
+
+	return 0;
+}
+
+static void __init vmx_depriv_host(void)
+{
+	if (setup_depriv_vmcs_config()) {
+		pr_err("depriv: error setting up deprivilege VMCS config\n");
+		return;
+	}
+
+	on_each_cpu(vmx_depriv_cpu, NULL, 0);
+}
+#endif
+
 static void vmx_exit(void)
 {
 #ifdef CONFIG_KEXEC_CORE
@@ -8045,6 +9078,13 @@ static void vmx_exit(void)
 	synchronize_rcu();
 #endif
 
+#if IS_ENABLED(CONFIG_KVM_INTEL_DEPRIV_HOST)
+	/*
+	 * reprivilege host before kvm_exit disables VMX
+	 */
+	vmx_repriv_host();
+#endif
+
 	kvm_exit();
 
 #if IS_ENABLED(CONFIG_HYPERV)
@@ -8152,6 +9192,481 @@ static int __init vmx_init(void)
 	if (!enable_ept)
 		allow_smaller_maxphyaddr = true;
 
+#if IS_ENABLED(CONFIG_KVM_INTEL_DEPRIV_HOST)
+	vmx_depriv_host();
+#endif
+
 	return 0;
 }
 module_init(vmx_init);
+
+#if IS_ENABLED(CONFIG_KVM_INTEL_DEPRIV_HOST)
+#define CHECK(_c) do {							\
+	if (!(_c)) {							\
+		pr_err("VM-entry failure (%d): %s\n", __LINE__, #_c);	\
+		root_caused = true;					\
+	}								\
+} while (0)
+
+#define CHECK_VMX_CTLS(ctls, val)					\
+	CHECK((~(ctls) & ((u32)(val))) == 0 &&				\
+	      ((ctls) & ~((u32)((val) >> 32))) == 0)
+
+static inline bool is_canonical_address(u64 la, u64 cr4)
+{
+	return get_canonical(la, cr4 & X86_CR4_LA57 ? 57 : 48) == la;
+}
+
+#define CHECK_HOST_SEG(seg)						\
+	CHECK((vmcs_read16(HOST_##seg##_SELECTOR) & 0x7) == 0)
+
+#define CHECK_IS_ADDR_CANONICAL(a, gh)					\
+	CHECK(is_canonical_address(a, vmcs_readl(gh##_CR4)))
+
+#define CHECK_IS_HOST_ADDR_CANONICAL(a)					\
+	CHECK_IS_ADDR_CANONICAL(a, HOST)
+
+#define CHECK_IS_GUEST_ADDR_CANONICAL(a)				\
+	CHECK_IS_ADDR_CANONICAL(a, GUEST)
+
+#define CHECK_IS_VMCS_FIELD_ADDR_CANONICAL(vmcs_field, gh)		\
+	CHECK_IS_ADDR_CANONICAL(vmcs_readl(vmcs_field), gh)
+
+#define CHECK_IS_HOST_VMCS_FIELD_ADDR_CANONICAL(addr)			\
+	CHECK_IS_VMCS_FIELD_ADDR_CANONICAL(HOST_##addr, HOST)
+
+#define CHECK_IS_GUEST_VMCS_FIELD_ADDR_CANONICAL(addr)			\
+	CHECK_IS_VMCS_FIELD_ADDR_CANONICAL(GUEST_##addr, GUEST)
+
+#define CHECK_IS_HOST_TABLE_BASE_CANONICAL(tab)				\
+	CHECK_IS_HOST_VMCS_FIELD_ADDR_CANONICAL(tab##_BASE)
+
+#define CHECK_IS_GUEST_TABLE_BASE_CANONICAL(tab)			\
+	CHECK_IS_GUEST_VMCS_FIELD_ADDR_CANONICAL(tab##_BASE)
+
+#define PAGE_M (PAGE_SIZE - 1)
+
+void vmx_check_guest_state(void)
+{
+	bool root_caused = false;
+	u32 vmentry_ctl = vmcs_read32(VM_ENTRY_CONTROLS);
+	u32 vmexit_ctl = vmcs_read32(VM_EXIT_CONTROLS);
+	u32 cpu_based_exec_ctrl = vmcs_read32(CPU_BASED_VM_EXEC_CONTROL);
+	u32 pin_based_exec_ctrl = vmcs_read32(PIN_BASED_VM_EXEC_CONTROL);
+	u32 secondary_exec_control = cpu_has_secondary_exec_ctrls() ?
+		vmcs_read32(SECONDARY_VM_EXEC_CONTROL) : 0;
+	u32 exit_reason = vmcs_read32(VM_EXIT_REASON);
+	u64 exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
+	u64 rflags = vmcs_readl(GUEST_RFLAGS);
+	bool vm86_active = rflags & X86_EFLAGS_VM;
+	bool longmode = vmentry_ctl & VM_ENTRY_IA32E_MODE;
+
+	u32 vmentry_intr_info = vmcs_read32(VM_ENTRY_INTR_INFO_FIELD);
+
+	bool unrestricted = cpu_has_secondary_exec_ctrls() ?
+		secondary_exec_control & SECONDARY_EXEC_UNRESTRICTED_GUEST :
+		false;
+
+	u64 cr0_must_be_zeros = ~read_msr(MSR_IA32_VMX_CR0_FIXED1);
+	u64 host_cr0_must_be_ones = read_msr(MSR_IA32_VMX_CR0_FIXED0);
+	u64 guest_cr0_must_be_ones = host_cr0_must_be_ones &
+		~(unrestricted ? X86_CR0_PG | X86_CR0_PE : 0);
+	u64 cr4_must_be_zeros = ~read_msr(MSR_IA32_VMX_CR4_FIXED1);
+	u64 cr4_must_be_ones = read_msr(MSR_IA32_VMX_CR4_FIXED0);
+
+	u64 pin_based_ctrls = read_msr(MSR_IA32_VMX_TRUE_PINBASED_CTLS);
+	u64 proc_based_ctrls = read_msr(MSR_IA32_VMX_TRUE_PROCBASED_CTLS);
+	u64 exit_ctrls = read_msr(MSR_IA32_VMX_TRUE_EXIT_CTLS);
+	u64 entry_ctrls = read_msr(MSR_IA32_VMX_TRUE_ENTRY_CTLS);
+
+	u64 debug_ctrl = vmcs_read64(GUEST_IA32_DEBUGCTL);
+	u64 link_ptr = vmcs_read64(VMCS_LINK_POINTER);
+	u32 activity_state = vmcs_read32(GUEST_ACTIVITY_STATE);
+	u32 interruptibility = vmcs_read32(GUEST_INTERRUPTIBILITY_INFO);
+	u64 pending_dbg_exceptions = vmcs_readl(GUEST_PENDING_DBG_EXCEPTIONS);
+
+	u8 s;
+
+	// 26.4: Loading MSRs
+	if (exit_reason == EXIT_REASON_MSR_LOAD_FAIL) {
+		pr_err("VM-entry failure: bad item (%#llx) in MSR load area\n",
+		       exit_qualification - 1);
+		root_caused = true;
+	}
+
+	// 26.2.1.1: VM-Execution Control Fields
+	CHECK_VMX_CTLS(pin_based_exec_ctrl, pin_based_ctrls);
+	CHECK_VMX_CTLS(cpu_based_exec_ctrl, proc_based_ctrls);
+
+	if (cpu_based_exec_ctrl & CPU_BASED_ACTIVATE_SECONDARY_CONTROLS) {
+		u64 secondary_ctrls = read_msr(MSR_IA32_VMX_PROCBASED_CTLS2);
+		CHECK_VMX_CTLS(secondary_exec_control, secondary_ctrls);
+	}
+
+	CHECK(vmcs_read32(CR3_TARGET_COUNT) == 0);
+
+	CHECK(!(cpu_based_exec_ctrl & CPU_BASED_USE_IO_BITMAPS));
+	CHECK(!(cpu_based_exec_ctrl & CPU_BASED_USE_MSR_BITMAPS));
+	CHECK(!(cpu_based_exec_ctrl & CPU_BASED_TPR_SHADOW));
+	CHECK(pin_based_exec_ctrl & PIN_BASED_NMI_EXITING ||
+	      !(pin_based_exec_ctrl & PIN_BASED_VIRTUAL_NMIS));
+	CHECK(pin_based_exec_ctrl & PIN_BASED_VIRTUAL_NMIS ||
+	      !(cpu_based_exec_ctrl & CPU_BASED_NMI_WINDOW_EXITING));
+	CHECK(!(pin_based_exec_ctrl & PIN_BASED_POSTED_INTR));
+
+	// 26.2.1.2: VM-Exit Control Fields
+	CHECK_VMX_CTLS(vmexit_ctl, exit_ctrls);
+	CHECK(pin_based_exec_ctrl & PIN_BASED_VMX_PREEMPTION_TIMER ||
+	      !(vmexit_ctl & VM_EXIT_SAVE_VMX_PREEMPTION_TIMER));
+
+	// 26.2.1.3: VM-Entry Control Fields
+	CHECK_VMX_CTLS(vmentry_ctl, entry_ctrls);
+	if (vmentry_intr_info & INTR_INFO_VALID_MASK) {
+		u32 type = vmentry_intr_info & INTR_INFO_INTR_TYPE_MASK;
+		u32 vector = vmentry_intr_info & INTR_INFO_VECTOR_MASK;
+		u32 instrLen = vmcs_read32(VM_ENTRY_INSTRUCTION_LEN);
+		bool has_error_code = vmentry_intr_info & INTR_INFO_DELIVER_CODE_MASK;
+
+		CHECK(type != (0x1 << 8));
+		CHECK((u32)(proc_based_ctrls >> 32) & CPU_BASED_MONITOR_TRAP_FLAG ||
+		      type != (0x7 << 8));
+		CHECK(type != (0x2 << 8) || vector == 2);
+		CHECK(type != (0x3 << 8) || vector <= 31);
+		CHECK(type != (0x7 << 8) || vector == 0);
+
+		CHECK(has_error_code == ((vmcs_readl(GUEST_CR0) & X86_CR0_PE) &&
+					 (type == (0x3 << 8) &&
+					  (vector == 8  ||
+					   vector == 10 ||
+					   vector == 11 ||
+					   vector == 12 ||
+					   vector == 13 ||
+					   vector == 14 ||
+					   vector == 17))));
+
+		CHECK(!(vmentry_intr_info &
+			(INTR_INFO_RESVD_BITS_MASK | INTR_INFO_UNBLOCK_NMI)));
+
+		CHECK(!has_error_code ||
+		      (vmcs_read32(VM_ENTRY_EXCEPTION_ERROR_CODE) & 0xffff8000) == 0);
+		CHECK(!(type == (0x4 << 8) ||
+			type == (0x5 << 8) ||
+			type == (0x6 << 8)) ||
+		      instrLen < MAX_INSN_SIZE);
+	}
+
+	CHECK(!(vmentry_ctl & VM_ENTRY_SMM));
+	CHECK(!(vmentry_ctl & VM_ENTRY_DEACT_DUAL_MONITOR));
+
+	// 26.2.2: Checks on Host Control Registers and MSRs
+	CHECK((~vmcs_readl(HOST_CR0) & cr0_must_be_zeros) == cr0_must_be_zeros);
+	CHECK((vmcs_readl(HOST_CR0) & host_cr0_must_be_ones) == host_cr0_must_be_ones);
+
+	CHECK((~vmcs_readl(HOST_CR4) & cr4_must_be_zeros) == cr4_must_be_zeros);
+	CHECK((vmcs_readl(HOST_CR4) & cr4_must_be_ones) == cr4_must_be_ones);
+
+	CHECK_IS_HOST_VMCS_FIELD_ADDR_CANONICAL(IA32_SYSENTER_ESP);
+	CHECK_IS_HOST_VMCS_FIELD_ADDR_CANONICAL(IA32_SYSENTER_EIP);
+
+	CHECK(vmexit_ctl & VM_EXIT_LOAD_IA32_PERF_GLOBAL_CTRL);
+
+	if (vmexit_ctl & VM_EXIT_LOAD_IA32_PAT) {
+		u64 pat = vmcs_read64(HOST_IA32_PAT);
+		unsigned i;
+		for (i = 0; i < 8; i++) {
+			u8 byte = pat & 0xff;
+			CHECK(byte < 8 && byte != 2 && byte != 3);
+			pat >>= 8;
+		}
+	}
+
+	if (vmexit_ctl & VM_EXIT_LOAD_IA32_EFER) {
+		u64 efer = vmcs_read64(HOST_IA32_EFER);
+		CHECK((efer & 0xffffffffffff0200ull) == 0);
+		if (vmexit_ctl & VM_EXIT_HOST_ADDR_SPACE_SIZE)
+			CHECK((efer & (EFER_LMA | EFER_LME)) ==
+			      (EFER_LMA | EFER_LME));
+		else
+			CHECK((efer & (EFER_LMA | EFER_LME)) == 0);
+	}
+
+	// 26.2.3: Checks on Host Segment and Descriptor-Table Registers
+	CHECK_HOST_SEG(ES);
+	CHECK_HOST_SEG(CS);
+	CHECK_HOST_SEG(SS);
+	CHECK_HOST_SEG(DS);
+	CHECK_HOST_SEG(FS);
+	CHECK_HOST_SEG(GS);
+	CHECK_HOST_SEG(TR);
+
+	CHECK(vmcs_read16(HOST_CS_SELECTOR) != 0);
+	CHECK(vmcs_read16(HOST_TR_SELECTOR) != 0);
+
+	CHECK_IS_HOST_TABLE_BASE_CANONICAL(FS);
+	CHECK_IS_HOST_TABLE_BASE_CANONICAL(GS);
+	CHECK_IS_HOST_TABLE_BASE_CANONICAL(GDTR);
+	CHECK_IS_HOST_TABLE_BASE_CANONICAL(IDTR);
+	CHECK_IS_HOST_TABLE_BASE_CANONICAL(TR);
+
+	// 26.2.4: Checks Related to Address-Space Size
+	CHECK(vmexit_ctl & VM_EXIT_HOST_ADDR_SPACE_SIZE);
+
+	CHECK(vmcs_readl(HOST_CR4) & X86_CR4_PAE);
+	CHECK_IS_HOST_VMCS_FIELD_ADDR_CANONICAL(RIP);
+
+	// 26.3.1.1: Checks on Guest Control Registers, Debug Registers, and MSRs
+	CHECK((~vmcs_readl(GUEST_CR0) & cr0_must_be_zeros) ==
+	      cr0_must_be_zeros);
+	CHECK((vmcs_readl(GUEST_CR0) & guest_cr0_must_be_ones) ==
+	      guest_cr0_must_be_ones);
+
+	CHECK((~vmcs_readl(GUEST_CR4) & cr4_must_be_zeros) == cr4_must_be_zeros);
+	CHECK((vmcs_readl(GUEST_CR4) & cr4_must_be_ones) == cr4_must_be_ones);
+
+	if (vmentry_ctl & VM_ENTRY_LOAD_DEBUG_CONTROLS) {
+		u64 debug_ctrl_reserved = 0xffffffffffff003cull;
+#if 0
+		if (!CpuidInfo_IsSet(RTM, CPUID_GetHostCPUID())) {
+			debug_ctrlRsvd |= MSR_DEBUGCTL_RTM;
+		}
+#endif
+		CHECK((debug_ctrl & debug_ctrl_reserved) == 0);
+	}
+
+	CHECK(!longmode || vmcs_readl(GUEST_CR4) & X86_CR4_PAE);
+
+	if (vmentry_ctl & VM_ENTRY_LOAD_DEBUG_CONTROLS)
+		CHECK(!((u32)(vmcs_readl(GUEST_DR7) >> 32)));
+
+	CHECK_IS_GUEST_VMCS_FIELD_ADDR_CANONICAL(SYSENTER_ESP);
+	CHECK_IS_GUEST_VMCS_FIELD_ADDR_CANONICAL(SYSENTER_EIP);
+
+	CHECK(vmentry_ctl & VM_ENTRY_LOAD_IA32_PERF_GLOBAL_CTRL);
+
+	if (vmentry_ctl & VM_ENTRY_LOAD_IA32_PAT) {
+		u64 pat = vmcs_read64(GUEST_IA32_PAT);
+		unsigned i;
+		for (i = 0; i < 8; i++) {
+			u8 byte = pat & 0xff;
+			CHECK(byte < 8 && byte != 2 && byte != 3);
+			pat >>= 8;
+		}
+	}
+
+	if (vmentry_ctl & VM_ENTRY_LOAD_IA32_EFER) {
+		u64 efer = vmcs_read64(GUEST_IA32_EFER);
+		CHECK((efer & 0xffffffffffff0200ull) == 0);
+		if ((vmcs_readl(GUEST_CR0) & X86_CR0_PG) != 0) {
+			if (longmode)
+				CHECK((efer & (EFER_LMA | EFER_LME)) ==
+				      (EFER_LMA | EFER_LME));
+			else
+				CHECK((efer & (EFER_LMA | EFER_LME)) == 0);
+		}
+	}
+
+	if (vmentry_ctl & VM_ENTRY_LOAD_BNDCFGS) {
+		u64 bndcfgs = vmcs_read64(GUEST_BNDCFGS);
+		CHECK_IS_GUEST_ADDR_CANONICAL(bndcfgs & 0xfffffffffffff000ull);
+		CHECK((bndcfgs & 0x00000ffc) == 0);
+	}
+
+	// 26.3.1.2: Checks on Guest Segment Registers
+	for (s = VCPU_SREG_ES; s <= VCPU_SREG_LDTR; s++) {
+		u16 selector = vmcs_read16(kvm_vmx_segment_fields[s].selector);
+		unsigned long base = vmcs_readl(kvm_vmx_segment_fields[s].base);
+		u32 limit = vmcs_read32(kvm_vmx_segment_fields[s].limit);
+		u32 orig_ar = vmcs_read32(kvm_vmx_segment_fields[s].ar_bytes);
+		u16 ar = orig_ar;
+		bool unusable = !((orig_ar >> 16) & 0x1);
+		unsigned rpl = selector & 0x3;
+		unsigned ti = (selector >> 2) & 0x1;
+		unsigned type = ar & 0xf;
+		unsigned dpl = (ar >> 5) & 0x3;
+#if 0
+		pr_err("seg %d: sel=0x%04x, attr=0x%05x, limit=0x%08x, base=0x%016lx\n",
+		       s, selector, orig_ar, limit, base);
+#endif
+		if (s == VCPU_SREG_TR) {
+			CHECK(ti == 0);
+		} else if (s == VCPU_SREG_LDTR) {
+			CHECK(unusable || ti == 0);
+		} else if (s == VCPU_SREG_SS) {
+			CHECK(vm86_active ||
+			      unrestricted ||
+			      rpl == (vmcs_read16(GUEST_CS_SELECTOR) & 0x3));
+		}
+
+		if (s < VCPU_SREG_LDTR && vm86_active) {
+			CHECK(base == (unsigned long)(selector << 4));
+		}
+		if (s == VCPU_SREG_TR || s == VCPU_SREG_FS || s == VCPU_SREG_GS) {
+			CHECK_IS_GUEST_ADDR_CANONICAL(base);
+		} else if (s == VCPU_SREG_LDTR) {
+			CHECK(unusable || is_canonical_address(base, vmcs_readl(GUEST_CR4)));
+		} else if (s == VCPU_SREG_CS) {
+			CHECK(!((u32)(base >> 32)));
+		} else {
+			CHECK(unusable || !((u32)(base >> 32)));
+		}
+
+		if (s < VCPU_SREG_LDTR && vm86_active) {
+			CHECK(limit == 0xffff);
+		}
+
+		if (s < VCPU_SREG_LDTR) {
+			if (vm86_active) {
+				CHECK(ar == 0xF3);
+			} else {
+				// Bits 3:0 (Type).
+				if (s == VCPU_SREG_CS) {
+					CHECK((unrestricted && type == 3) ||
+					      (type & 9) == 9); /* Must be code, accessed. */
+				} else if (s == VCPU_SREG_SS) {
+					/* If not a null selector, must be data, accessed, writable. */
+					CHECK(unusable || type == 3 || type == 7);
+				} else if (!unusable) {
+					/* Must be accessed. Also must be data or writable. */
+					CHECK((type & 1) == 1 &&
+					      ((type & 8) == 0 || (type & 2) == 2));
+				}
+				// Bit 4 (S).
+				if (s == VCPU_SREG_TR) {
+					CHECK(!((ar >> 4) & 0x1));
+				}
+				if (s != VCPU_SREG_TR && !unusable) {
+					CHECK((ar >> 4) & 0x1);
+				}
+				// Bits 6:5 (DPL).
+				if (s == VCPU_SREG_CS) {
+					if (type == 3) {              /* Data segment => real mode. */
+						CHECK(dpl == 0);
+					} else if ((type & 4) == 0) { /* Non-conforming code segment. */
+						CHECK(dpl == ((vmcs_read32(GUEST_SS_AR_BYTES) >> 5) & 0x3));
+					} else {                      /* Conforming code segment. */
+						CHECK(dpl <= ((vmcs_read32(GUEST_SS_AR_BYTES) >> 5) & 0x3));
+					}
+				} else if (s == VCPU_SREG_SS) {
+					if ((vmcs_readl(GUEST_CR0) & X86_CR0_PE) == 0 ||
+					    (vmcs_read32(GUEST_CS_AR_BYTES) & 0xf) == 3) {
+						CHECK(dpl == 0);
+					} else {
+						CHECK(dpl == rpl);
+					}
+				} else if (!unrestricted && !unusable && type <= 11) {
+					/* Not a conforming code segment. */
+					CHECK(dpl >= rpl);
+				}
+				// Bit 7 (P).
+				if (s == VCPU_SREG_CS || !unusable) {
+					CHECK((ar >> 7) & 0x1);
+				}
+				// Bits 11:8, 31:17 (reserved).
+				if (s == VCPU_SREG_CS || !unusable) {
+					CHECK((ar & 0xfffe0f00) == 0);
+				}
+				// Bit 14 (D/B).
+				if (s == VCPU_SREG_CS) {
+					CHECK(!longmode || !((ar >> 13) & 0x1) || ((ar >> 14) & 0x1) == 0);
+				}
+				// Bit 15 (G).
+				if (s == VCPU_SREG_CS || !unusable) {
+					CHECK((limit & PAGE_M) == PAGE_M|| !((ar >> 15) & 0x1));
+					CHECK(limit >> 20 == 0 || ((ar >> 15) & 0x1));
+				}
+			}
+		} else if (s == VCPU_SREG_TR) {
+			/* Must be 16-bit busy TSS if not in long mode or 32-bit busy TSS. */
+			CHECK((!longmode && type == 3) || type == 11);
+			CHECK(!((ar >> 4) & 0x1));
+			CHECK((ar >> 7) & 0x1);
+			CHECK((ar & 0xfffe0f00) == 0);
+			CHECK((limit & PAGE_M) == PAGE_M|| !((ar >> 15) & 0x1));
+			CHECK(limit >> 20 == 0 || ((ar >> 15) & 0x1));
+			CHECK(!unusable);
+		} else if (s == VCPU_SREG_LDTR && !unusable) {
+			/* Must be a LDT selector. */
+			CHECK(type == 2);
+			CHECK(!((ar >> 4) & 0x1));
+			CHECK((ar >> 7) & 0x1);
+			CHECK((ar & 0xfffe0f00) == 0);
+			CHECK((limit & PAGE_M) == PAGE_M|| !((ar >> 15) & 0x1));
+			CHECK(limit >> 20 == 0 || ((ar >> 15) & 0x1));
+		}
+	}
+
+	// 26.3.1.3: Checks on Guest Descriptor-Table Registers
+	CHECK_IS_GUEST_TABLE_BASE_CANONICAL(GDTR);
+	CHECK_IS_GUEST_TABLE_BASE_CANONICAL(IDTR);
+
+	// 26.3.1.4: Checks on Guest RIP and RFLAGS
+	CHECK((longmode && ((vmcs_read32(kvm_vmx_segment_fields[VCPU_SREG_CS].ar_bytes) >> 13) & 0x1)) ||
+	      (u32)(vmcs_readl(GUEST_RIP) >> 32) == 0);
+	CHECK_IS_GUEST_VMCS_FIELD_ADDR_CANONICAL(RIP);
+
+	CHECK((rflags & ((-1ull << 22) | (1 << 15) | (1 << 5) | (1 << 3))) == 0);
+	CHECK((rflags & X86_EFLAGS_FIXED) != 0);
+	CHECK(!longmode || !vm86_active);
+	CHECK(((vmentry_intr_info & (INTR_INFO_VALID_MASK |
+				     INTR_INFO_INTR_TYPE_MASK)) !=
+	       (INTR_INFO_VALID_MASK | INTR_TYPE_EXT_INTR)) ||
+	      (rflags & X86_EFLAGS_IF));
+
+	// 26.3.1.5: Checks on Guest Non-Register State
+	CHECK(activity_state <= GUEST_ACTIVITY_WAIT_SIPI);
+	CHECK(activity_state != GUEST_ACTIVITY_HLT||
+	      ((vmcs_read32(kvm_vmx_segment_fields[VCPU_SREG_SS].ar_bytes) >> 5) & 0x3) == 0);
+	CHECK(!(interruptibility & (GUEST_INTR_STATE_STI | GUEST_INTR_STATE_MOV_SS)) ||
+	      activity_state == GUEST_ACTIVITY_ACTIVE);
+	if (vmentry_intr_info & INTR_INFO_VALID_MASK) {
+		u32 type = vmentry_intr_info & INTR_INFO_INTR_TYPE_MASK;
+		u32 vector = vmentry_intr_info & INTR_INFO_VECTOR_MASK;
+
+		CHECK(activity_state != GUEST_ACTIVITY_HLT ||
+		      (type == INTR_TYPE_EXT_INTR ||
+		       type == INTR_TYPE_NMI_INTR ||
+		       (type == INTR_TYPE_HARD_EXCEPTION &&
+			(vector == 1 || vector == 18)) ||
+		       (type == INTR_TYPE_OTHER_EVENT && vector == 0)));
+		CHECK(activity_state != GUEST_ACTIVITY_SHUTDOWN ||
+		      (type == INTR_TYPE_NMI_INTR ||
+		       (type == INTR_TYPE_HARD_EXCEPTION && vector == 18)));
+		CHECK(activity_state != GUEST_ACTIVITY_WAIT_SIPI);
+	}
+
+	CHECK((interruptibility & 0xFFFFFFE0) == 0);
+	CHECK((interruptibility & (GUEST_INTR_STATE_STI | GUEST_INTR_STATE_MOV_SS)) !=
+	      (GUEST_INTR_STATE_STI | GUEST_INTR_STATE_MOV_SS));
+	CHECK(rflags & X86_EFLAGS_IF || !(interruptibility & GUEST_INTR_STATE_STI));
+	CHECK(((vmentry_intr_info & (INTR_INFO_VALID_MASK | INTR_INFO_INTR_TYPE_MASK)) !=
+	       (INTR_INFO_VALID_MASK | INTR_TYPE_EXT_INTR)) ||
+	      (interruptibility & (GUEST_INTR_STATE_STI | GUEST_INTR_STATE_MOV_SS)) == 0);
+	CHECK(((vmentry_intr_info & (INTR_INFO_VALID_MASK | INTR_INFO_INTR_TYPE_MASK)) !=
+	       (INTR_INFO_VALID_MASK | INTR_TYPE_NMI_INTR)) ||
+	      (interruptibility & GUEST_INTR_STATE_MOV_SS) == 0);
+	CHECK((interruptibility & GUEST_INTR_STATE_SMI) == 0);
+	// Some processors require the following check; others do not.
+	CHECK(((vmentry_intr_info & (INTR_INFO_VALID_MASK | INTR_INFO_INTR_TYPE_MASK)) !=
+	       (INTR_INFO_VALID_MASK | INTR_TYPE_NMI_INTR)) ||
+	      (interruptibility & GUEST_INTR_STATE_STI) == 0);
+	CHECK(((vmentry_intr_info & (INTR_INFO_VALID_MASK | INTR_INFO_INTR_TYPE_MASK)) !=
+	       (INTR_INFO_VALID_MASK | INTR_TYPE_NMI_INTR)) ||
+	      (pin_based_exec_ctrl & PIN_BASED_VIRTUAL_NMIS) == 0 ||
+	      (interruptibility & GUEST_INTR_STATE_NMI) == 0);
+
+	CHECK((u32)(pending_dbg_exceptions >> 32) == 0);
+	CHECK((pending_dbg_exceptions & 0xfffeaff0) == 0);
+	if ((interruptibility & (GUEST_INTR_STATE_STI | GUEST_INTR_STATE_MOV_SS)) != 0 ||
+	    activity_state == GUEST_ACTIVITY_HLT) {
+		CHECK(!((rflags & X86_EFLAGS_TF) && !(debug_ctrl & 0x2)) ||
+		      (pending_dbg_exceptions & 0x00004000));
+		CHECK(!(!(rflags & X86_EFLAGS_TF) || (debug_ctrl & 0x2)) ||
+		      !(pending_dbg_exceptions & 0x00004000));
+	}
+
+	if (!root_caused) {
+		pr_err("unexplained VM-entry failure, reason: %#x qualification: %#llx, link ptr: %#llx\n",
+		       exit_reason, exit_qualification, link_ptr);
+	}
+}
+#endif
diff --git a/arch/x86/kvm/vmx/vmx.h b/arch/x86/kvm/vmx/vmx.h
index f8fc7441baea..81f0c6b7630a 100644
--- a/arch/x86/kvm/vmx/vmx.h
+++ b/arch/x86/kvm/vmx/vmx.h
@@ -577,4 +577,9 @@ static inline int vmx_get_instr_info_reg2(u32 vmx_instr_info)
 	return (vmx_instr_info >> 28) & 0xf;
 }
 
+#if IS_ENABLED(CONFIG_KVM_INTEL_DEPRIV_HOST)
+bool vmx_depriv_vmexit_handler(unsigned long *guset_regs);
+void vmx_check_guest_state(void);
+#endif
+
 #endif /* __KVM_X86_VMX_H */
diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c
index 504158f0e131..0624eeb5eb6b 100644
--- a/virt/kvm/kvm_main.c
+++ b/virt/kvm/kvm_main.c
@@ -5751,6 +5751,10 @@ int kvm_init(void *opaque, unsigned vcpu_size, unsigned vcpu_align,
 	r = kvm_vfio_ops_init();
 	WARN_ON(r);
 
+#if IS_ENABLED(CONFIG_KVM_INTEL_DEPRIV_HOST) // XXX: hacky
+	hardware_enable_all();
+#endif
+
 	return 0;
 
 out_unreg:
@@ -5779,6 +5783,11 @@ EXPORT_SYMBOL_GPL(kvm_init);
 void kvm_exit(void)
 {
 	int cpu;
+#if IS_ENABLED(CONFIG_KVM_INTEL_DEPRIV_HOST) // XXX: hacky
+	raw_spin_lock(&kvm_count_lock);
+	kvm_usage_count--;
+	raw_spin_unlock(&kvm_count_lock);
+#endif
 
 	debugfs_remove_recursive(kvm_debugfs_dir);
 	misc_deregister(&kvm_dev);
-- 
2.34.1

